{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 과제1번"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.datasets import load_breast_cancer, load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed =2020\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_breast_cancer()\n",
    "X = data.data\n",
    "Y = data.target"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 20,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
=======
   "execution_count": 20,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
<<<<<<< HEAD
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                372       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
=======
=======
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 12)                372       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 9         \n",
<<<<<<< HEAD
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
=======
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
      "=================================================================\n",
      "Total params: 1,415\n",
      "Trainable params: 1,415\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(30, input_dim=30, activation='relu'),\n",
    "    Dense(12, activation='relu'),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "]) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 21,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
=======
   "execution_count": 21,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n"
     ]
    }
   ],
   "source": [
    "print(data.data.shape)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 22,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
=======
   "execution_count": 22,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 23,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
=======
   "execution_count": 23,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "MODEL_DIR = './model/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 24,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
=======
   "execution_count": 24,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = MODEL_DIR + \"final{epoch:03d}-{val_loss:.4f}.hdf5\"\n",
    "\n",
    "checkpointer_callback = ModelCheckpoint(filepath=modelpath, monitor='val_loss', \n",
    "                               verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 3,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
=======
   "execution_count": 3,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=100)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 26,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
=======
   "execution_count": 26,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
<<<<<<< HEAD
<<<<<<< HEAD
      "Epoch 00001: val_loss improved from inf to 134.38034, saving model to ./model/final001-134.3803.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 134.38034 to 127.69637, saving model to ./model/final002-127.6964.hdf5\n",
      "\n",
      "Epoch 00003: val_loss improved from 127.69637 to 121.10952, saving model to ./model/final003-121.1095.hdf5\n",
      "\n",
      "Epoch 00004: val_loss improved from 121.10952 to 114.61555, saving model to ./model/final004-114.6155.hdf5\n",
      "\n",
      "Epoch 00005: val_loss improved from 114.61555 to 108.20789, saving model to ./model/final005-108.2079.hdf5\n",
      "\n",
      "Epoch 00006: val_loss improved from 108.20789 to 101.88339, saving model to ./model/final006-101.8834.hdf5\n",
      "\n",
      "Epoch 00007: val_loss improved from 101.88339 to 95.65546, saving model to ./model/final007-95.6555.hdf5\n",
      "\n",
      "Epoch 00008: val_loss improved from 95.65546 to 89.60613, saving model to ./model/final008-89.6061.hdf5\n",
      "\n",
      "Epoch 00009: val_loss improved from 89.60613 to 83.83163, saving model to ./model/final009-83.8316.hdf5\n",
      "\n",
      "Epoch 00010: val_loss improved from 83.83163 to 78.35395, saving model to ./model/final010-78.3540.hdf5\n",
      "\n",
      "Epoch 00011: val_loss improved from 78.35395 to 73.23484, saving model to ./model/final011-73.2348.hdf5\n",
      "\n",
      "Epoch 00012: val_loss improved from 73.23484 to 68.34192, saving model to ./model/final012-68.3419.hdf5\n",
      "\n",
      "Epoch 00013: val_loss improved from 68.34192 to 63.56371, saving model to ./model/final013-63.5637.hdf5\n",
      "\n",
      "Epoch 00014: val_loss improved from 63.56371 to 58.85646, saving model to ./model/final014-58.8565.hdf5\n",
      "\n",
      "Epoch 00015: val_loss improved from 58.85646 to 54.18890, saving model to ./model/final015-54.1889.hdf5\n",
      "\n",
      "Epoch 00016: val_loss improved from 54.18890 to 49.56612, saving model to ./model/final016-49.5661.hdf5\n",
      "\n",
      "Epoch 00017: val_loss improved from 49.56612 to 45.04171, saving model to ./model/final017-45.0417.hdf5\n",
      "\n",
      "Epoch 00018: val_loss improved from 45.04171 to 40.64491, saving model to ./model/final018-40.6449.hdf5\n",
      "\n",
      "Epoch 00019: val_loss improved from 40.64491 to 36.37979, saving model to ./model/final019-36.3798.hdf5\n",
      "\n",
      "Epoch 00020: val_loss improved from 36.37979 to 32.26496, saving model to ./model/final020-32.2650.hdf5\n",
      "\n",
      "Epoch 00021: val_loss improved from 32.26496 to 28.18707, saving model to ./model/final021-28.1871.hdf5\n",
      "\n",
      "Epoch 00022: val_loss improved from 28.18707 to 24.01650, saving model to ./model/final022-24.0165.hdf5\n",
      "\n",
      "Epoch 00023: val_loss improved from 24.01650 to 19.82535, saving model to ./model/final023-19.8253.hdf5\n",
      "\n",
      "Epoch 00024: val_loss improved from 19.82535 to 15.64262, saving model to ./model/final024-15.6426.hdf5\n",
      "\n",
      "Epoch 00025: val_loss improved from 15.64262 to 11.48744, saving model to ./model/final025-11.4874.hdf5\n",
      "\n",
      "Epoch 00026: val_loss improved from 11.48744 to 7.84991, saving model to ./model/final026-7.8499.hdf5\n",
      "\n",
      "Epoch 00027: val_loss improved from 7.84991 to 5.59717, saving model to ./model/final027-5.5972.hdf5\n",
      "\n",
      "Epoch 00028: val_loss improved from 5.59717 to 4.37546, saving model to ./model/final028-4.3755.hdf5\n",
      "\n",
      "Epoch 00029: val_loss improved from 4.37546 to 3.44154, saving model to ./model/final029-3.4415.hdf5\n",
      "\n",
      "Epoch 00030: val_loss improved from 3.44154 to 2.67553, saving model to ./model/final030-2.6755.hdf5\n",
      "\n",
      "Epoch 00031: val_loss improved from 2.67553 to 2.25600, saving model to ./model/final031-2.2560.hdf5\n",
      "\n",
      "Epoch 00032: val_loss improved from 2.25600 to 2.18815, saving model to ./model/final032-2.1881.hdf5\n",
      "\n",
      "Epoch 00033: val_loss improved from 2.18815 to 2.13249, saving model to ./model/final033-2.1325.hdf5\n",
      "\n",
      "Epoch 00034: val_loss improved from 2.13249 to 2.05559, saving model to ./model/final034-2.0556.hdf5\n",
      "\n",
      "Epoch 00035: val_loss improved from 2.05559 to 1.96000, saving model to ./model/final035-1.9600.hdf5\n",
      "\n",
      "Epoch 00036: val_loss improved from 1.96000 to 1.84839, saving model to ./model/final036-1.8484.hdf5\n",
      "\n",
      "Epoch 00037: val_loss improved from 1.84839 to 1.72378, saving model to ./model/final037-1.7238.hdf5\n",
      "\n",
      "Epoch 00038: val_loss improved from 1.72378 to 1.58977, saving model to ./model/final038-1.5898.hdf5\n",
      "\n",
      "Epoch 00039: val_loss improved from 1.58977 to 1.45097, saving model to ./model/final039-1.4510.hdf5\n",
      "\n",
      "Epoch 00040: val_loss improved from 1.45097 to 1.31375, saving model to ./model/final040-1.3137.hdf5\n",
      "\n",
      "Epoch 00041: val_loss improved from 1.31375 to 1.18718, saving model to ./model/final041-1.1872.hdf5\n",
      "\n",
      "Epoch 00042: val_loss improved from 1.18718 to 1.08344, saving model to ./model/final042-1.0834.hdf5\n",
      "\n",
      "Epoch 00043: val_loss improved from 1.08344 to 1.01318, saving model to ./model/final043-1.0132.hdf5\n",
      "\n",
      "Epoch 00044: val_loss improved from 1.01318 to 0.97581, saving model to ./model/final044-0.9758.hdf5\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.97581 to 0.96136, saving model to ./model/final045-0.9614.hdf5\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.96136 to 0.95857, saving model to ./model/final046-0.9586.hdf5\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.95857\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.95857\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.95857\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.95857\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.95857\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.95857\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.95857\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.95857\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.95857\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.95857\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.95857\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.95857\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.95857\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.95857\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.95857\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.95857\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.95857\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.95857\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.95857\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.95857\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.95857\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.95857 to 0.94933, saving model to ./model/final068-0.9493.hdf5\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.94933 to 0.93459, saving model to ./model/final069-0.9346.hdf5\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.93459 to 0.92419, saving model to ./model/final070-0.9242.hdf5\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.92419 to 0.91737, saving model to ./model/final071-0.9174.hdf5\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.91737 to 0.91361, saving model to ./model/final072-0.9136.hdf5\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.91361 to 0.91262, saving model to ./model/final073-0.9126.hdf5\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.91262\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.91262\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.91262\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.91262\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.91262\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.91262\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.91262\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.91262\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.91262\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.91262\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.91262 to 0.90878, saving model to ./model/final084-0.9088.hdf5\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.90878 to 0.89537, saving model to ./model/final085-0.8954.hdf5\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.89537 to 0.88102, saving model to ./model/final086-0.8810.hdf5\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.88102 to 0.86657, saving model to ./model/final087-0.8666.hdf5\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.86657 to 0.85265, saving model to ./model/final088-0.8527.hdf5\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.85265 to 0.84003, saving model to ./model/final089-0.8400.hdf5\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.84003 to 0.82894, saving model to ./model/final090-0.8289.hdf5\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.82894 to 0.81942, saving model to ./model/final091-0.8194.hdf5\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.81942 to 0.81146, saving model to ./model/final092-0.8115.hdf5\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.81146 to 0.80496, saving model to ./model/final093-0.8050.hdf5\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.80496 to 0.79982, saving model to ./model/final094-0.7998.hdf5\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.79982 to 0.79609, saving model to ./model/final095-0.7961.hdf5\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.79609 to 0.79374, saving model to ./model/final096-0.7937.hdf5\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.79374 to 0.79238, saving model to ./model/final097-0.7924.hdf5\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.79238 to 0.79153, saving model to ./model/final098-0.7915.hdf5\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.79153 to 0.79100, saving model to ./model/final099-0.7910.hdf5\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00100: val_loss improved from 0.79100 to 0.79042, saving model to ./model/final100-0.7904.hdf5\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.79042 to 0.78939, saving model to ./model/final101-0.7894.hdf5\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.78939 to 0.78762, saving model to ./model/final102-0.7876.hdf5\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.78762 to 0.78501, saving model to ./model/final103-0.7850.hdf5\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.78501 to 0.78150, saving model to ./model/final104-0.7815.hdf5\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.78150 to 0.77722, saving model to ./model/final105-0.7772.hdf5\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.77722 to 0.77232, saving model to ./model/final106-0.7723.hdf5\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.77232 to 0.76704, saving model to ./model/final107-0.7670.hdf5\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.76704 to 0.76160, saving model to ./model/final108-0.7616.hdf5\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.76160 to 0.75620, saving model to ./model/final109-0.7562.hdf5\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.75620 to 0.75105, saving model to ./model/final110-0.7511.hdf5\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.75105 to 0.74633, saving model to ./model/final111-0.7463.hdf5\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.74633 to 0.74196, saving model to ./model/final112-0.7420.hdf5\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.74196 to 0.73790, saving model to ./model/final113-0.7379.hdf5\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.73790 to 0.73430, saving model to ./model/final114-0.7343.hdf5\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.73430 to 0.73115, saving model to ./model/final115-0.7312.hdf5\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.73115 to 0.72833, saving model to ./model/final116-0.7283.hdf5\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.72833 to 0.72554, saving model to ./model/final117-0.7255.hdf5\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.72554 to 0.72263, saving model to ./model/final118-0.7226.hdf5\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.72263 to 0.71962, saving model to ./model/final119-0.7196.hdf5\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.71962 to 0.71540, saving model to ./model/final120-0.7154.hdf5\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.71540 to 0.71033, saving model to ./model/final121-0.7103.hdf5\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.71033 to 0.70520, saving model to ./model/final122-0.7052.hdf5\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.70520 to 0.69983, saving model to ./model/final123-0.6998.hdf5\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.69983 to 0.69220, saving model to ./model/final124-0.6922.hdf5\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.69220 to 0.68355, saving model to ./model/final125-0.6835.hdf5\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.68355 to 0.67627, saving model to ./model/final126-0.6763.hdf5\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.67627 to 0.66849, saving model to ./model/final127-0.6685.hdf5\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.66849 to 0.66211, saving model to ./model/final128-0.6621.hdf5\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.66211 to 0.65429, saving model to ./model/final129-0.6543.hdf5\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.65429 to 0.64260, saving model to ./model/final130-0.6426.hdf5\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.64260 to 0.62632, saving model to ./model/final131-0.6263.hdf5\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.62632 to 0.60676, saving model to ./model/final132-0.6068.hdf5\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.60676 to 0.58606, saving model to ./model/final133-0.5861.hdf5\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.58606 to 0.56680, saving model to ./model/final134-0.5668.hdf5\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.56680 to 0.55090, saving model to ./model/final135-0.5509.hdf5\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.55090 to 0.53915, saving model to ./model/final136-0.5392.hdf5\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.53915 to 0.53157, saving model to ./model/final137-0.5316.hdf5\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.53157 to 0.52817, saving model to ./model/final138-0.5282.hdf5\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.52817\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.52817\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.52817\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.52817\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.52817\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.52817\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.52817\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.52817 to 0.51963, saving model to ./model/final146-0.5196.hdf5\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.51963 to 0.50608, saving model to ./model/final147-0.5061.hdf5\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.50608 to 0.49494, saving model to ./model/final148-0.4949.hdf5\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.49494 to 0.48739, saving model to ./model/final149-0.4874.hdf5\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.48739 to 0.48341, saving model to ./model/final150-0.4834.hdf5\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.48341 to 0.48254, saving model to ./model/final151-0.4825.hdf5\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.48254\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.48254\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.48254\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.48254\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.48254\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.48254 to 0.47918, saving model to ./model/final157-0.4792.hdf5\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.47918 to 0.47209, saving model to ./model/final158-0.4721.hdf5\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.47209 to 0.46442, saving model to ./model/final159-0.4644.hdf5\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.46442 to 0.45690, saving model to ./model/final160-0.4569.hdf5\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.45690 to 0.45037, saving model to ./model/final161-0.4504.hdf5\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.45037 to 0.44543, saving model to ./model/final162-0.4454.hdf5\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.44543 to 0.44224, saving model to ./model/final163-0.4422.hdf5\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.44224 to 0.43985, saving model to ./model/final164-0.4399.hdf5\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.43985 to 0.43770, saving model to ./model/final165-0.4377.hdf5\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.43770 to 0.43576, saving model to ./model/final166-0.4358.hdf5\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.43576 to 0.43343, saving model to ./model/final167-0.4334.hdf5\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.43343 to 0.43021, saving model to ./model/final168-0.4302.hdf5\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.43021 to 0.42649, saving model to ./model/final169-0.4265.hdf5\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.42649 to 0.42202, saving model to ./model/final170-0.4220.hdf5\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.42202 to 0.41657, saving model to ./model/final171-0.4166.hdf5\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.41657 to 0.41043, saving model to ./model/final172-0.4104.hdf5\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.41043 to 0.40400, saving model to ./model/final173-0.4040.hdf5\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.40400 to 0.39806, saving model to ./model/final174-0.3981.hdf5\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.39806 to 0.39298, saving model to ./model/final175-0.3930.hdf5\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.39298 to 0.38884, saving model to ./model/final176-0.3888.hdf5\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.38884 to 0.38526, saving model to ./model/final177-0.3853.hdf5\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.38526 to 0.38192, saving model to ./model/final178-0.3819.hdf5\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.38192 to 0.37858, saving model to ./model/final179-0.3786.hdf5\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.37858 to 0.37496, saving model to ./model/final180-0.3750.hdf5\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.37496 to 0.37086, saving model to ./model/final181-0.3709.hdf5\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.37086 to 0.36592, saving model to ./model/final182-0.3659.hdf5\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.36592 to 0.36086, saving model to ./model/final183-0.3609.hdf5\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.36086 to 0.35587, saving model to ./model/final184-0.3559.hdf5\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.35587 to 0.35120, saving model to ./model/final185-0.3512.hdf5\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.35120 to 0.34728, saving model to ./model/final186-0.3473.hdf5\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.34728 to 0.34427, saving model to ./model/final187-0.3443.hdf5\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.34427 to 0.34191, saving model to ./model/final188-0.3419.hdf5\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.34191 to 0.34009, saving model to ./model/final189-0.3401.hdf5\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.34009 to 0.33815, saving model to ./model/final190-0.3381.hdf5\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.33815 to 0.33568, saving model to ./model/final191-0.3357.hdf5\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.33568 to 0.33242, saving model to ./model/final192-0.3324.hdf5\n",
      "\n",
      "Epoch 00193: val_loss improved from 0.33242 to 0.32840, saving model to ./model/final193-0.3284.hdf5\n"
=======
=======
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
      "Epoch 00001: val_loss improved from inf to 19.62851, saving model to ./model/final001-19.6285.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 19.62851 to 17.29044, saving model to ./model/final002-17.2904.hdf5\n",
      "\n",
      "Epoch 00003: val_loss improved from 17.29044 to 14.97553, saving model to ./model/final003-14.9755.hdf5\n",
      "\n",
      "Epoch 00004: val_loss improved from 14.97553 to 12.68668, saving model to ./model/final004-12.6867.hdf5\n",
      "\n",
      "Epoch 00005: val_loss improved from 12.68668 to 10.44045, saving model to ./model/final005-10.4405.hdf5\n",
      "\n",
      "Epoch 00006: val_loss improved from 10.44045 to 8.24197, saving model to ./model/final006-8.2420.hdf5\n",
      "\n",
      "Epoch 00007: val_loss improved from 8.24197 to 6.15014, saving model to ./model/final007-6.1501.hdf5\n",
      "\n",
      "Epoch 00008: val_loss improved from 6.15014 to 4.20904, saving model to ./model/final008-4.2090.hdf5\n",
      "\n",
      "Epoch 00009: val_loss improved from 4.20904 to 2.54544, saving model to ./model/final009-2.5454.hdf5\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.54544 to 1.32435, saving model to ./model/final010-1.3243.hdf5\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.32435 to 0.64941, saving model to ./model/final011-0.6494.hdf5\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.64941 to 0.45346, saving model to ./model/final012-0.4535.hdf5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.45346\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.45346\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.45346\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.45346\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.45346\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.45346\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.45346\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.45346\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.45346\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.45346\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.45346\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.45346 to 0.38874, saving model to ./model/final024-0.3887.hdf5\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.38874 to 0.32823, saving model to ./model/final025-0.3282.hdf5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.32823\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.32823\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.32823\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.32823\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.32823\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.32823\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.32823\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.32823\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.32823\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.32823\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.32823\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.32823\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.32823\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.32823\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.32823\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.32823\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.32823\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.32823\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.32823 to 0.32134, saving model to ./model/final044-0.3213.hdf5\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.32134 to 0.31298, saving model to ./model/final045-0.3130.hdf5\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.31298 to 0.30729, saving model to ./model/final046-0.3073.hdf5\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.30729\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.30729\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.30729\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.30729\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.30729\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.30729\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.30729\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.30729\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.30729 to 0.30495, saving model to ./model/final055-0.3050.hdf5\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.30495 to 0.28128, saving model to ./model/final056-0.2813.hdf5\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.28128 to 0.26307, saving model to ./model/final057-0.2631.hdf5\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.26307 to 0.25062, saving model to ./model/final058-0.2506.hdf5\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.25062 to 0.24265, saving model to ./model/final059-0.2427.hdf5\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.24265 to 0.23718, saving model to ./model/final060-0.2372.hdf5\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.23718 to 0.23296, saving model to ./model/final061-0.2330.hdf5\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.23296 to 0.22943, saving model to ./model/final062-0.2294.hdf5\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.22943 to 0.22707, saving model to ./model/final063-0.2271.hdf5\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.22707 to 0.22699, saving model to ./model/final064-0.2270.hdf5\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.22699\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.22699\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.22699\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.22699\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.22699\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.22699\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.22699\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.22699\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.22699 to 0.21956, saving model to ./model/final073-0.2196.hdf5\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.21956 to 0.21352, saving model to ./model/final074-0.2135.hdf5\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.21352 to 0.20954, saving model to ./model/final075-0.2095.hdf5\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.20954 to 0.20728, saving model to ./model/final076-0.2073.hdf5\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.20728 to 0.20625, saving model to ./model/final077-0.2063.hdf5\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.20625\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.20625\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.20625\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.20625\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.20625\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.20625\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.20625\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.20625\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.20625\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.20625\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.20625 to 0.20449, saving model to ./model/final088-0.2045.hdf5\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.20449 to 0.20244, saving model to ./model/final089-0.2024.hdf5\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.20244 to 0.20137, saving model to ./model/final090-0.2014.hdf5\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.20137 to 0.20122, saving model to ./model/final091-0.2012.hdf5\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.20122\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.20122 to 0.19988, saving model to ./model/final131-0.1999.hdf5\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.19988 to 0.19700, saving model to ./model/final132-0.1970.hdf5\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.19700 to 0.19549, saving model to ./model/final133-0.1955.hdf5\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.19549\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.19549\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.19549\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.19549\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.19549\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.19549\n"
<<<<<<< HEAD
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
=======
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
<<<<<<< HEAD
<<<<<<< HEAD
      "Epoch 00194: val_loss improved from 0.32840 to 0.32400, saving model to ./model/final194-0.3240.hdf5\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.32400 to 0.31970, saving model to ./model/final195-0.3197.hdf5\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.31970 to 0.31592, saving model to ./model/final196-0.3159.hdf5\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.31592 to 0.31310, saving model to ./model/final197-0.3131.hdf5\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.31310 to 0.31115, saving model to ./model/final198-0.3112.hdf5\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.31115 to 0.30980, saving model to ./model/final199-0.3098.hdf5\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.30980 to 0.30869, saving model to ./model/final200-0.3087.hdf5\n",
      "\n",
      "Epoch 00201: val_loss improved from 0.30869 to 0.30738, saving model to ./model/final201-0.3074.hdf5\n",
      "\n",
      "Epoch 00202: val_loss improved from 0.30738 to 0.30584, saving model to ./model/final202-0.3058.hdf5\n",
      "\n",
      "Epoch 00203: val_loss improved from 0.30584 to 0.30387, saving model to ./model/final203-0.3039.hdf5\n",
      "\n",
      "Epoch 00204: val_loss improved from 0.30387 to 0.30151, saving model to ./model/final204-0.3015.hdf5\n",
      "\n",
      "Epoch 00205: val_loss improved from 0.30151 to 0.29895, saving model to ./model/final205-0.2989.hdf5\n",
      "\n",
      "Epoch 00206: val_loss improved from 0.29895 to 0.29638, saving model to ./model/final206-0.2964.hdf5\n",
      "\n",
      "Epoch 00207: val_loss improved from 0.29638 to 0.29405, saving model to ./model/final207-0.2940.hdf5\n",
      "\n",
      "Epoch 00208: val_loss improved from 0.29405 to 0.29203, saving model to ./model/final208-0.2920.hdf5\n",
      "\n",
      "Epoch 00209: val_loss improved from 0.29203 to 0.29046, saving model to ./model/final209-0.2905.hdf5\n",
      "\n",
      "Epoch 00210: val_loss improved from 0.29046 to 0.28925, saving model to ./model/final210-0.2892.hdf5\n",
      "\n",
      "Epoch 00211: val_loss improved from 0.28925 to 0.28812, saving model to ./model/final211-0.2881.hdf5\n",
      "\n",
      "Epoch 00212: val_loss improved from 0.28812 to 0.28735, saving model to ./model/final212-0.2873.hdf5\n",
      "\n",
      "Epoch 00213: val_loss improved from 0.28735 to 0.28621, saving model to ./model/final213-0.2862.hdf5\n",
      "\n",
      "Epoch 00214: val_loss improved from 0.28621 to 0.28469, saving model to ./model/final214-0.2847.hdf5\n",
      "\n",
      "Epoch 00215: val_loss improved from 0.28469 to 0.28308, saving model to ./model/final215-0.2831.hdf5\n",
      "\n",
      "Epoch 00216: val_loss improved from 0.28308 to 0.28146, saving model to ./model/final216-0.2815.hdf5\n",
      "\n",
      "Epoch 00217: val_loss improved from 0.28146 to 0.28014, saving model to ./model/final217-0.2801.hdf5\n",
      "\n",
      "Epoch 00218: val_loss improved from 0.28014 to 0.27919, saving model to ./model/final218-0.2792.hdf5\n",
      "\n",
      "Epoch 00219: val_loss improved from 0.27919 to 0.27846, saving model to ./model/final219-0.2785.hdf5\n",
      "\n",
      "Epoch 00220: val_loss improved from 0.27846 to 0.27841, saving model to ./model/final220-0.2784.hdf5\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.27841\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.27841\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.27841\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.27841\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.27841\n",
      "\n",
      "Epoch 00226: val_loss improved from 0.27841 to 0.27711, saving model to ./model/final226-0.2771.hdf5\n",
      "\n",
      "Epoch 00227: val_loss improved from 0.27711 to 0.27307, saving model to ./model/final227-0.2731.hdf5\n",
      "\n",
      "Epoch 00228: val_loss improved from 0.27307 to 0.26855, saving model to ./model/final228-0.2686.hdf5\n",
      "\n",
      "Epoch 00229: val_loss improved from 0.26855 to 0.26478, saving model to ./model/final229-0.2648.hdf5\n",
      "\n",
      "Epoch 00230: val_loss improved from 0.26478 to 0.26220, saving model to ./model/final230-0.2622.hdf5\n",
      "\n",
      "Epoch 00231: val_loss improved from 0.26220 to 0.26137, saving model to ./model/final231-0.2614.hdf5\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.26137\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.26137\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.26137\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.26137\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.26137\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.26137\n",
      "\n",
      "Epoch 00238: val_loss improved from 0.26137 to 0.26087, saving model to ./model/final238-0.2609.hdf5\n",
      "\n",
      "Epoch 00239: val_loss improved from 0.26087 to 0.25986, saving model to ./model/final239-0.2599.hdf5\n",
      "\n",
      "Epoch 00240: val_loss improved from 0.25986 to 0.25949, saving model to ./model/final240-0.2595.hdf5\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.25949\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.25949\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.25949\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.25949\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.25949\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.25949\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.25949\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.25949\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.25949\n",
      "\n",
      "Epoch 00250: val_loss improved from 0.25949 to 0.25936, saving model to ./model/final250-0.2594.hdf5\n",
      "\n",
      "Epoch 00251: val_loss improved from 0.25936 to 0.25753, saving model to ./model/final251-0.2575.hdf5\n",
      "\n",
      "Epoch 00252: val_loss improved from 0.25753 to 0.25538, saving model to ./model/final252-0.2554.hdf5\n",
      "\n",
      "Epoch 00253: val_loss improved from 0.25538 to 0.25356, saving model to ./model/final253-0.2536.hdf5\n",
      "\n",
      "Epoch 00254: val_loss improved from 0.25356 to 0.25245, saving model to ./model/final254-0.2524.hdf5\n",
      "\n",
      "Epoch 00255: val_loss improved from 0.25245 to 0.25215, saving model to ./model/final255-0.2521.hdf5\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.25215\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.25215\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.25215\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.25215\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.25215\n",
      "\n",
      "Epoch 00261: val_loss improved from 0.25215 to 0.25126, saving model to ./model/final261-0.2513.hdf5\n",
      "\n",
      "Epoch 00262: val_loss improved from 0.25126 to 0.25020, saving model to ./model/final262-0.2502.hdf5\n",
      "\n",
      "Epoch 00263: val_loss improved from 0.25020 to 0.24920, saving model to ./model/final263-0.2492.hdf5\n",
      "\n",
      "Epoch 00264: val_loss improved from 0.24920 to 0.24853, saving model to ./model/final264-0.2485.hdf5\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.24853\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.24853\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.24853\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.24853\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.24853\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.24853\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.24853\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.24853\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.24853\n",
      "\n",
      "Epoch 00274: val_loss improved from 0.24853 to 0.24834, saving model to ./model/final274-0.2483.hdf5\n",
      "\n",
      "Epoch 00275: val_loss improved from 0.24834 to 0.24765, saving model to ./model/final275-0.2476.hdf5\n",
      "\n",
      "Epoch 00276: val_loss improved from 0.24765 to 0.24677, saving model to ./model/final276-0.2468.hdf5\n",
      "\n",
      "Epoch 00277: val_loss improved from 0.24677 to 0.24581, saving model to ./model/final277-0.2458.hdf5\n",
      "\n",
      "Epoch 00278: val_loss improved from 0.24581 to 0.24489, saving model to ./model/final278-0.2449.hdf5\n",
      "\n",
      "Epoch 00279: val_loss improved from 0.24489 to 0.24416, saving model to ./model/final279-0.2442.hdf5\n",
      "\n",
      "Epoch 00280: val_loss improved from 0.24416 to 0.24363, saving model to ./model/final280-0.2436.hdf5\n",
      "\n",
      "Epoch 00281: val_loss improved from 0.24363 to 0.24321, saving model to ./model/final281-0.2432.hdf5\n",
      "\n",
      "Epoch 00282: val_loss improved from 0.24321 to 0.24283, saving model to ./model/final282-0.2428.hdf5\n",
      "\n",
      "Epoch 00283: val_loss improved from 0.24283 to 0.24245, saving model to ./model/final283-0.2425.hdf5\n",
      "\n",
      "Epoch 00284: val_loss improved from 0.24245 to 0.24200, saving model to ./model/final284-0.2420.hdf5\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.24200\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.24200\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.24200\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.24200\n",
      "\n",
      "Epoch 00289: val_loss improved from 0.24200 to 0.24136, saving model to ./model/final289-0.2414.hdf5\n",
      "\n",
      "Epoch 00290: val_loss improved from 0.24136 to 0.24069, saving model to ./model/final290-0.2407.hdf5\n",
      "\n",
      "Epoch 00291: val_loss improved from 0.24069 to 0.24027, saving model to ./model/final291-0.2403.hdf5\n",
      "\n",
      "Epoch 00292: val_loss improved from 0.24027 to 0.23973, saving model to ./model/final292-0.2397.hdf5\n",
      "\n",
      "Epoch 00293: val_loss improved from 0.23973 to 0.23920, saving model to ./model/final293-0.2392.hdf5\n",
      "\n",
      "Epoch 00294: val_loss improved from 0.23920 to 0.23873, saving model to ./model/final294-0.2387.hdf5\n",
      "\n",
      "Epoch 00295: val_loss improved from 0.23873 to 0.23835, saving model to ./model/final295-0.2384.hdf5\n",
      "\n",
      "Epoch 00296: val_loss improved from 0.23835 to 0.23797, saving model to ./model/final296-0.2380.hdf5\n",
      "\n",
      "Epoch 00297: val_loss improved from 0.23797 to 0.23745, saving model to ./model/final297-0.2375.hdf5\n",
      "\n",
      "Epoch 00298: val_loss improved from 0.23745 to 0.23676, saving model to ./model/final298-0.2368.hdf5\n",
      "\n",
      "Epoch 00299: val_loss improved from 0.23676 to 0.23605, saving model to ./model/final299-0.2361.hdf5\n",
      "\n",
      "Epoch 00300: val_loss improved from 0.23605 to 0.23536, saving model to ./model/final300-0.2354.hdf5\n",
      "\n",
      "Epoch 00301: val_loss improved from 0.23536 to 0.23465, saving model to ./model/final301-0.2347.hdf5\n",
      "\n",
      "Epoch 00302: val_loss improved from 0.23465 to 0.23396, saving model to ./model/final302-0.2340.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00303: val_loss improved from 0.23396 to 0.23338, saving model to ./model/final303-0.2334.hdf5\n",
      "\n",
      "Epoch 00304: val_loss improved from 0.23338 to 0.23297, saving model to ./model/final304-0.2330.hdf5\n",
      "\n",
      "Epoch 00305: val_loss improved from 0.23297 to 0.23284, saving model to ./model/final305-0.2328.hdf5\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.23284\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.23284\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.23284\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.23284\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.23284\n",
      "\n",
      "Epoch 00311: val_loss improved from 0.23284 to 0.23226, saving model to ./model/final311-0.2323.hdf5\n",
      "\n",
      "Epoch 00312: val_loss improved from 0.23226 to 0.23126, saving model to ./model/final312-0.2313.hdf5\n",
      "\n",
      "Epoch 00313: val_loss improved from 0.23126 to 0.23045, saving model to ./model/final313-0.2304.hdf5\n",
      "\n",
      "Epoch 00314: val_loss improved from 0.23045 to 0.22999, saving model to ./model/final314-0.2300.hdf5\n",
      "\n",
      "Epoch 00315: val_loss improved from 0.22999 to 0.22992, saving model to ./model/final315-0.2299.hdf5\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.22992\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.22992\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.22992\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.22992\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.22992\n",
      "\n",
      "Epoch 00321: val_loss improved from 0.22992 to 0.22943, saving model to ./model/final321-0.2294.hdf5\n",
      "\n",
      "Epoch 00322: val_loss improved from 0.22943 to 0.22891, saving model to ./model/final322-0.2289.hdf5\n",
      "\n",
      "Epoch 00323: val_loss improved from 0.22891 to 0.22857, saving model to ./model/final323-0.2286.hdf5\n",
      "\n",
      "Epoch 00324: val_loss improved from 0.22857 to 0.22844, saving model to ./model/final324-0.2284.hdf5\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.22844\n",
      "\n",
      "Epoch 00326: val_loss improved from 0.22844 to 0.22824, saving model to ./model/final326-0.2282.hdf5\n",
      "\n",
      "Epoch 00327: val_loss improved from 0.22824 to 0.22774, saving model to ./model/final327-0.2277.hdf5\n",
      "\n",
      "Epoch 00328: val_loss improved from 0.22774 to 0.22750, saving model to ./model/final328-0.2275.hdf5\n",
      "\n",
      "Epoch 00329: val_loss improved from 0.22750 to 0.22719, saving model to ./model/final329-0.2272.hdf5\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.22719\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.22719\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.22719\n",
      "\n",
      "Epoch 00333: val_loss improved from 0.22719 to 0.22653, saving model to ./model/final333-0.2265.hdf5\n",
      "\n",
      "Epoch 00334: val_loss improved from 0.22653 to 0.22599, saving model to ./model/final334-0.2260.hdf5\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.22599\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.22599\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.22599\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.22599\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.22599\n",
      "\n",
      "Epoch 00340: val_loss improved from 0.22599 to 0.22577, saving model to ./model/final340-0.2258.hdf5\n",
      "\n",
      "Epoch 00341: val_loss improved from 0.22577 to 0.22490, saving model to ./model/final341-0.2249.hdf5\n",
      "\n",
      "Epoch 00342: val_loss improved from 0.22490 to 0.22429, saving model to ./model/final342-0.2243.hdf5\n",
      "\n",
      "Epoch 00343: val_loss improved from 0.22429 to 0.22409, saving model to ./model/final343-0.2241.hdf5\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.22409\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.22409\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.22409\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.22409\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.22409\n",
      "\n",
      "Epoch 00349: val_loss improved from 0.22409 to 0.22355, saving model to ./model/final349-0.2236.hdf5\n",
      "\n",
      "Epoch 00350: val_loss improved from 0.22355 to 0.22288, saving model to ./model/final350-0.2229.hdf5\n",
      "\n",
      "Epoch 00351: val_loss improved from 0.22288 to 0.22242, saving model to ./model/final351-0.2224.hdf5\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.22242\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.22242\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.22242\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.22242\n",
      "\n",
      "Epoch 00356: val_loss improved from 0.22242 to 0.22233, saving model to ./model/final356-0.2223.hdf5\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.22233\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.22233\n",
      "\n",
      "Epoch 00359: val_loss improved from 0.22233 to 0.22221, saving model to ./model/final359-0.2222.hdf5\n",
      "\n",
      "Epoch 00360: val_loss improved from 0.22221 to 0.22124, saving model to ./model/final360-0.2212.hdf5\n",
      "\n",
      "Epoch 00361: val_loss improved from 0.22124 to 0.22009, saving model to ./model/final361-0.2201.hdf5\n",
      "\n",
      "Epoch 00362: val_loss improved from 0.22009 to 0.21988, saving model to ./model/final362-0.2199.hdf5\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.21988\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.21988\n",
      "\n",
      "Epoch 00365: val_loss improved from 0.21988 to 0.21972, saving model to ./model/final365-0.2197.hdf5\n",
      "\n",
      "Epoch 00366: val_loss improved from 0.21972 to 0.21963, saving model to ./model/final366-0.2196.hdf5\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.21963\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.21963\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.21963\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.21963\n",
      "\n",
      "Epoch 00371: val_loss improved from 0.21963 to 0.21940, saving model to ./model/final371-0.2194.hdf5\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.21940\n",
      "\n",
      "Epoch 00373: val_loss improved from 0.21940 to 0.21932, saving model to ./model/final373-0.2193.hdf5\n",
      "\n",
      "Epoch 00374: val_loss improved from 0.21932 to 0.21889, saving model to ./model/final374-0.2189.hdf5\n",
      "\n",
      "Epoch 00375: val_loss improved from 0.21889 to 0.21834, saving model to ./model/final375-0.2183.hdf5\n",
      "\n",
      "Epoch 00376: val_loss improved from 0.21834 to 0.21793, saving model to ./model/final376-0.2179.hdf5\n",
      "\n",
      "Epoch 00377: val_loss improved from 0.21793 to 0.21777, saving model to ./model/final377-0.2178.hdf5\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.21777\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.21777\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.21777\n",
      "\n",
      "Epoch 00381: val_loss improved from 0.21777 to 0.21777, saving model to ./model/final381-0.2178.hdf5\n",
      "\n",
      "Epoch 00382: val_loss improved from 0.21777 to 0.21745, saving model to ./model/final382-0.2174.hdf5\n",
      "\n",
      "Epoch 00383: val_loss improved from 0.21745 to 0.21711, saving model to ./model/final383-0.2171.hdf5\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.21711\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.21711\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.21711\n",
      "\n",
      "Epoch 00387: val_loss improved from 0.21711 to 0.21697, saving model to ./model/final387-0.2170.hdf5\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.21697\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.21697\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.21697\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.21697\n",
      "\n",
      "Epoch 00392: val_loss improved from 0.21697 to 0.21582, saving model to ./model/final392-0.2158.hdf5\n",
      "\n",
      "Epoch 00393: val_loss improved from 0.21582 to 0.21451, saving model to ./model/final393-0.2145.hdf5\n",
      "\n",
      "Epoch 00394: val_loss improved from 0.21451 to 0.21437, saving model to ./model/final394-0.2144.hdf5\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.21437\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.21437\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.21437\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.21437\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.21437\n",
      "\n",
      "Epoch 00400: val_loss improved from 0.21437 to 0.21354, saving model to ./model/final400-0.2135.hdf5\n",
      "\n",
      "Epoch 00401: val_loss improved from 0.21354 to 0.21306, saving model to ./model/final401-0.2131.hdf5\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.21306\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.21306\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.21306\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.21306\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.21306\n",
      "\n",
      "Epoch 00407: val_loss improved from 0.21306 to 0.21279, saving model to ./model/final407-0.2128.hdf5\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.21279\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.21279\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.21279\n",
      "\n",
      "Epoch 00411: val_loss improved from 0.21279 to 0.21272, saving model to ./model/final411-0.2127.hdf5\n",
      "\n",
      "Epoch 00412: val_loss improved from 0.21272 to 0.21118, saving model to ./model/final412-0.2112.hdf5\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.21118\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.21118\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.21118\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.21118\n",
      "\n",
      "Epoch 00417: val_loss improved from 0.21118 to 0.21069, saving model to ./model/final417-0.2107.hdf5\n",
      "\n",
      "Epoch 00418: val_loss improved from 0.21069 to 0.21053, saving model to ./model/final418-0.2105.hdf5\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.21053\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.21053\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.21053\n",
      "\n",
      "Epoch 00422: val_loss improved from 0.21053 to 0.21048, saving model to ./model/final422-0.2105.hdf5\n",
      "\n",
      "Epoch 00423: val_loss improved from 0.21048 to 0.21025, saving model to ./model/final423-0.2102.hdf5\n"
=======
      "Epoch 00140: val_loss did not improve from 0.19549\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.19549\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.19549 to 0.19530, saving model to ./model/final142-0.1953.hdf5\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.19530\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.19530\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.19530\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.19530\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.19530\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.19530 to 0.19506, saving model to ./model/final148-0.1951.hdf5\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.19506 to 0.19411, saving model to ./model/final149-0.1941.hdf5\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.19411 to 0.19399, saving model to ./model/final150-0.1940.hdf5\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.19399 to 0.19383, saving model to ./model/final151-0.1938.hdf5\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.19383 to 0.19313, saving model to ./model/final152-0.1931.hdf5\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.19313 to 0.19307, saving model to ./model/final153-0.1931.hdf5\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.19307\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.19307\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.19307\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.19307\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.19307\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.19307 to 0.19216, saving model to ./model/final159-0.1922.hdf5\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.19216 to 0.19092, saving model to ./model/final160-0.1909.hdf5\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.19092 to 0.19004, saving model to ./model/final161-0.1900.hdf5\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.19004 to 0.18994, saving model to ./model/final162-0.1899.hdf5\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.18994\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.18994\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.18994\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.18994\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.18994 to 0.18943, saving model to ./model/final167-0.1894.hdf5\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.18943 to 0.18852, saving model to ./model/final168-0.1885.hdf5\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.18852 to 0.18769, saving model to ./model/final169-0.1877.hdf5\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.18769 to 0.18708, saving model to ./model/final170-0.1871.hdf5\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.18708 to 0.18688, saving model to ./model/final171-0.1869.hdf5\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.18688\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.18688\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.18688\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.18688\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.18688 to 0.18608, saving model to ./model/final176-0.1861.hdf5\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.18608 to 0.18541, saving model to ./model/final177-0.1854.hdf5\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.18541 to 0.18479, saving model to ./model/final178-0.1848.hdf5\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.18479 to 0.18435, saving model to ./model/final179-0.1843.hdf5\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.18435\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.18435\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.18435\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.18435\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.18435 to 0.18385, saving model to ./model/final184-0.1839.hdf5\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.18385 to 0.18290, saving model to ./model/final185-0.1829.hdf5\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.18290\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.18290\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.18290\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.18290 to 0.18228, saving model to ./model/final189-0.1823.hdf5\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.18228 to 0.18184, saving model to ./model/final190-0.1818.hdf5\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.18184 to 0.18165, saving model to ./model/final191-0.1816.hdf5\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.18165 to 0.18121, saving model to ./model/final192-0.1812.hdf5\n",
      "\n",
      "Epoch 00193: val_loss improved from 0.18121 to 0.18095, saving model to ./model/final193-0.1810.hdf5\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.18095\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.18095\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.18095\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.18095\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.18095\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.18095\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.18095 to 0.17913, saving model to ./model/final200-0.1791.hdf5\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.17913\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.17913\n",
      "\n",
      "Epoch 00203: val_loss improved from 0.17913 to 0.17833, saving model to ./model/final203-0.1783.hdf5\n",
      "\n",
      "Epoch 00204: val_loss improved from 0.17833 to 0.17718, saving model to ./model/final204-0.1772.hdf5\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.17718\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.17718\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.17718\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.17718\n",
      "\n",
      "Epoch 00209: val_loss improved from 0.17718 to 0.17704, saving model to ./model/final209-0.1770.hdf5\n",
      "\n",
      "Epoch 00210: val_loss improved from 0.17704 to 0.17636, saving model to ./model/final210-0.1764.hdf5\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.17636\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.17636\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.17636\n",
      "\n",
      "Epoch 00214: val_loss improved from 0.17636 to 0.17540, saving model to ./model/final214-0.1754.hdf5\n",
      "\n",
      "Epoch 00215: val_loss improved from 0.17540 to 0.17434, saving model to ./model/final215-0.1743.hdf5\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.17434\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.17434\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.17434\n",
      "\n",
      "Epoch 00219: val_loss improved from 0.17434 to 0.17431, saving model to ./model/final219-0.1743.hdf5\n",
      "\n",
      "Epoch 00220: val_loss improved from 0.17431 to 0.17303, saving model to ./model/final220-0.1730.hdf5\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.17303\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.17303\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.17303\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.17303\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.17303\n",
      "\n",
      "Epoch 00226: val_loss improved from 0.17303 to 0.17256, saving model to ./model/final226-0.1726.hdf5\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.17256\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.17256\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.17256\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.17256\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.17256\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.17256\n",
      "\n",
      "Epoch 00233: val_loss improved from 0.17256 to 0.17121, saving model to ./model/final233-0.1712.hdf5\n",
      "\n",
      "Epoch 00234: val_loss improved from 0.17121 to 0.17093, saving model to ./model/final234-0.1709.hdf5\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.17093\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.17093\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.17093\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.17093\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.17093\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.17093\n",
      "\n",
      "Epoch 00241: val_loss improved from 0.17093 to 0.17081, saving model to ./model/final241-0.1708.hdf5\n",
      "\n",
      "Epoch 00242: val_loss improved from 0.17081 to 0.17021, saving model to ./model/final242-0.1702.hdf5\n",
      "\n",
      "Epoch 00243: val_loss improved from 0.17021 to 0.16993, saving model to ./model/final243-0.1699.hdf5\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.16993\n",
      "\n",
      "Epoch 00245: val_loss improved from 0.16993 to 0.16952, saving model to ./model/final245-0.1695.hdf5\n",
      "\n",
      "Epoch 00246: val_loss improved from 0.16952 to 0.16860, saving model to ./model/final246-0.1686.hdf5\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.16860\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.16860\n",
      "\n",
      "Epoch 00249: val_loss improved from 0.16860 to 0.16846, saving model to ./model/final249-0.1685.hdf5\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.16846\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.16846\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.16846\n",
      "\n",
      "Epoch 00253: val_loss improved from 0.16846 to 0.16530, saving model to ./model/final253-0.1653.hdf5\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.16530\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.16530\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.16530\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.16530\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.16530\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.16530\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.16530\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.16530\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.16530\n",
      "\n",
      "Epoch 00263: val_loss improved from 0.16530 to 0.16458, saving model to ./model/final263-0.1646.hdf5\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.16458\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.16458\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.16458\n",
      "\n",
      "Epoch 00267: val_loss improved from 0.16458 to 0.16280, saving model to ./model/final267-0.1628.hdf5\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.16280\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.16280\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.16280\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.16280\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.16280\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.16280\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.16280\n",
      "\n"
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "\n",
      "Epoch 00424: val_loss did not improve from 0.21025\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.21025\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.21025\n",
      "\n",
      "Epoch 00427: val_loss improved from 0.21025 to 0.20966, saving model to ./model/final427-0.2097.hdf5\n",
      "\n",
      "Epoch 00428: val_loss improved from 0.20966 to 0.20901, saving model to ./model/final428-0.2090.hdf5\n",
      "\n",
      "Epoch 00429: val_loss improved from 0.20901 to 0.20869, saving model to ./model/final429-0.2087.hdf5\n",
      "\n",
      "Epoch 00430: val_loss improved from 0.20869 to 0.20862, saving model to ./model/final430-0.2086.hdf5\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.20862\n",
      "\n",
      "Epoch 00432: val_loss improved from 0.20862 to 0.20783, saving model to ./model/final432-0.2078.hdf5\n",
      "\n",
      "Epoch 00433: val_loss improved from 0.20783 to 0.20720, saving model to ./model/final433-0.2072.hdf5\n",
      "\n",
      "Epoch 00434: val_loss improved from 0.20720 to 0.20680, saving model to ./model/final434-0.2068.hdf5\n",
      "\n",
      "Epoch 00435: val_loss improved from 0.20680 to 0.20657, saving model to ./model/final435-0.2066.hdf5\n",
      "\n",
      "Epoch 00436: val_loss improved from 0.20657 to 0.20535, saving model to ./model/final436-0.2053.hdf5\n",
      "\n",
      "Epoch 00437: val_loss improved from 0.20535 to 0.20440, saving model to ./model/final437-0.2044.hdf5\n",
      "\n",
      "Epoch 00438: val_loss improved from 0.20440 to 0.20394, saving model to ./model/final438-0.2039.hdf5\n",
      "\n",
      "Epoch 00439: val_loss improved from 0.20394 to 0.20384, saving model to ./model/final439-0.2038.hdf5\n",
      "\n",
      "Epoch 00440: val_loss improved from 0.20384 to 0.20294, saving model to ./model/final440-0.2029.hdf5\n",
      "\n",
      "Epoch 00441: val_loss improved from 0.20294 to 0.20227, saving model to ./model/final441-0.2023.hdf5\n",
      "\n",
      "Epoch 00442: val_loss improved from 0.20227 to 0.20172, saving model to ./model/final442-0.2017.hdf5\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.20172\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.20172\n",
      "\n",
      "Epoch 00445: val_loss improved from 0.20172 to 0.20090, saving model to ./model/final445-0.2009.hdf5\n",
      "\n",
      "Epoch 00446: val_loss improved from 0.20090 to 0.20036, saving model to ./model/final446-0.2004.hdf5\n",
      "\n",
      "Epoch 00447: val_loss improved from 0.20036 to 0.20018, saving model to ./model/final447-0.2002.hdf5\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.20018\n",
      "\n",
      "Epoch 00449: val_loss improved from 0.20018 to 0.19949, saving model to ./model/final449-0.1995.hdf5\n",
      "\n",
      "Epoch 00450: val_loss improved from 0.19949 to 0.19834, saving model to ./model/final450-0.1983.hdf5\n",
      "\n",
      "Epoch 00451: val_loss improved from 0.19834 to 0.19798, saving model to ./model/final451-0.1980.hdf5\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.19798\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.19798\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.19798\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.19798\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.19798\n",
      "\n",
      "Epoch 00457: val_loss improved from 0.19798 to 0.19762, saving model to ./model/final457-0.1976.hdf5\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.19762\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.19762\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.19762\n",
      "\n",
      "Epoch 00461: val_loss improved from 0.19762 to 0.19749, saving model to ./model/final461-0.1975.hdf5\n",
      "\n",
      "Epoch 00462: val_loss improved from 0.19749 to 0.19708, saving model to ./model/final462-0.1971.hdf5\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.19708\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.19708\n",
      "\n",
      "Epoch 00465: val_loss improved from 0.19708 to 0.19702, saving model to ./model/final465-0.1970.hdf5\n",
      "\n",
      "Epoch 00466: val_loss improved from 0.19702 to 0.19582, saving model to ./model/final466-0.1958.hdf5\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.19582\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.19582\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.19582\n",
      "\n",
      "Epoch 00470: val_loss improved from 0.19582 to 0.19559, saving model to ./model/final470-0.1956.hdf5\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.19559\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.19559\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.19559\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.19559\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.19559\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.19559\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.19559\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.19559\n",
      "\n",
      "Epoch 00479: val_loss improved from 0.19559 to 0.19464, saving model to ./model/final479-0.1946.hdf5\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.19464\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.19464\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.19464\n",
      "\n",
      "Epoch 00483: val_loss improved from 0.19464 to 0.19419, saving model to ./model/final483-0.1942.hdf5\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.19419\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.19419\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.19419\n",
      "\n",
      "Epoch 00487: val_loss improved from 0.19419 to 0.19370, saving model to ./model/final487-0.1937.hdf5\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.19370\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.19370\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.19370\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.19370\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.19370\n",
      "\n",
      "Epoch 00493: val_loss improved from 0.19370 to 0.19366, saving model to ./model/final493-0.1937.hdf5\n",
      "\n",
      "Epoch 00494: val_loss improved from 0.19366 to 0.19355, saving model to ./model/final494-0.1936.hdf5\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.19355\n",
      "\n",
      "Epoch 00496: val_loss improved from 0.19355 to 0.19311, saving model to ./model/final496-0.1931.hdf5\n",
      "\n",
      "Epoch 00497: val_loss improved from 0.19311 to 0.19210, saving model to ./model/final497-0.1921.hdf5\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.19210\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.19210\n",
      "\n",
      "Epoch 00500: val_loss improved from 0.19210 to 0.19175, saving model to ./model/final500-0.1918.hdf5\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 0.19175\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 0.19175\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 0.19175\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 0.19175\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 0.19175\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 0.19175\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 0.19175\n",
      "\n",
      "Epoch 00508: val_loss improved from 0.19175 to 0.19126, saving model to ./model/final508-0.1913.hdf5\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 0.19126\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 0.19126\n",
      "\n",
      "Epoch 00511: val_loss improved from 0.19126 to 0.19095, saving model to ./model/final511-0.1909.hdf5\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 0.19095\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 0.19095\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 0.19095\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 0.19095\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 0.19095\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 0.19095\n",
      "\n",
      "Epoch 00518: val_loss improved from 0.19095 to 0.19056, saving model to ./model/final518-0.1906.hdf5\n",
      "\n",
      "Epoch 00519: val_loss improved from 0.19056 to 0.18992, saving model to ./model/final519-0.1899.hdf5\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 0.18992\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 0.18992\n",
      "\n",
      "Epoch 00522: val_loss improved from 0.18992 to 0.18983, saving model to ./model/final522-0.1898.hdf5\n",
      "\n",
      "Epoch 00523: val_loss improved from 0.18983 to 0.18981, saving model to ./model/final523-0.1898.hdf5\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 0.18981\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 0.18981\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 0.18981\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 0.18981\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 0.18981\n",
      "\n",
      "Epoch 00529: val_loss improved from 0.18981 to 0.18894, saving model to ./model/final529-0.1889.hdf5\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 0.18894\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 0.18894\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 0.18894\n",
      "\n",
      "Epoch 00533: val_loss improved from 0.18894 to 0.18869, saving model to ./model/final533-0.1887.hdf5\n",
      "\n",
      "Epoch 00534: val_loss improved from 0.18869 to 0.18862, saving model to ./model/final534-0.1886.hdf5\n",
      "\n",
      "Epoch 00535: val_loss improved from 0.18862 to 0.18828, saving model to ./model/final535-0.1883.hdf5\n",
      "\n",
      "Epoch 00536: val_loss improved from 0.18828 to 0.18784, saving model to ./model/final536-0.1878.hdf5\n",
      "\n",
      "Epoch 00537: val_loss improved from 0.18784 to 0.18755, saving model to ./model/final537-0.1875.hdf5\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 0.18755\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 0.18755\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 0.18755\n",
      "\n",
      "Epoch 00541: val_loss improved from 0.18755 to 0.18698, saving model to ./model/final541-0.1870.hdf5\n",
      "\n",
      "Epoch 00542: val_loss improved from 0.18698 to 0.18515, saving model to ./model/final542-0.1852.hdf5\n",
      "\n",
      "Epoch 00543: val_loss improved from 0.18515 to 0.18515, saving model to ./model/final543-0.1852.hdf5\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 0.18515\n",
      "\n",
      "Epoch 00545: val_loss improved from 0.18515 to 0.18504, saving model to ./model/final545-0.1850.hdf5\n",
      "\n",
      "Epoch 00546: val_loss improved from 0.18504 to 0.18365, saving model to ./model/final546-0.1837.hdf5\n",
      "\n",
      "Epoch 00547: val_loss improved from 0.18365 to 0.18268, saving model to ./model/final547-0.1827.hdf5\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 0.18268\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 0.18268\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00550: val_loss did not improve from 0.18268\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 0.18268\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 0.18268\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 0.18268\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 0.18268\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 0.18268\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 0.18268\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 0.18268\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 0.18268\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 0.18268\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 0.18268\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 0.18268\n",
      "\n",
      "Epoch 00562: val_loss improved from 0.18268 to 0.18156, saving model to ./model/final562-0.1816.hdf5\n",
      "\n",
      "Epoch 00563: val_loss improved from 0.18156 to 0.18090, saving model to ./model/final563-0.1809.hdf5\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 0.18090\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 0.18090\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 0.18090\n",
      "\n",
      "Epoch 00567: val_loss improved from 0.18090 to 0.18087, saving model to ./model/final567-0.1809.hdf5\n",
      "\n",
      "Epoch 00568: val_loss improved from 0.18087 to 0.17977, saving model to ./model/final568-0.1798.hdf5\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 0.17977\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 0.17977\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 0.17977\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 0.17977\n",
      "\n",
      "Epoch 00573: val_loss improved from 0.17977 to 0.17896, saving model to ./model/final573-0.1790.hdf5\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 0.17896\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 0.17896\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 0.17896\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 0.17896\n",
      "\n",
      "Epoch 00578: val_loss improved from 0.17896 to 0.17870, saving model to ./model/final578-0.1787.hdf5\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 0.17870\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 0.17870\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 0.17870\n",
      "\n",
      "Epoch 00582: val_loss improved from 0.17870 to 0.17774, saving model to ./model/final582-0.1777.hdf5\n",
      "\n",
      "Epoch 00583: val_loss improved from 0.17774 to 0.17689, saving model to ./model/final583-0.1769.hdf5\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 0.17689\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 0.17689\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 0.17689\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 0.17689\n",
      "\n",
      "Epoch 00588: val_loss improved from 0.17689 to 0.17523, saving model to ./model/final588-0.1752.hdf5\n",
      "\n",
      "Epoch 00589: val_loss improved from 0.17523 to 0.17464, saving model to ./model/final589-0.1746.hdf5\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 0.17464\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 0.17464\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 0.17464\n",
      "\n",
      "Epoch 00593: val_loss improved from 0.17464 to 0.17464, saving model to ./model/final593-0.1746.hdf5\n",
      "\n",
      "Epoch 00594: val_loss improved from 0.17464 to 0.17400, saving model to ./model/final594-0.1740.hdf5\n",
      "\n",
      "Epoch 00595: val_loss improved from 0.17400 to 0.17359, saving model to ./model/final595-0.1736.hdf5\n",
      "\n",
      "Epoch 00596: val_loss improved from 0.17359 to 0.17302, saving model to ./model/final596-0.1730.hdf5\n",
      "\n",
      "Epoch 00597: val_loss improved from 0.17302 to 0.17254, saving model to ./model/final597-0.1725.hdf5\n",
      "\n",
      "Epoch 00598: val_loss improved from 0.17254 to 0.17170, saving model to ./model/final598-0.1717.hdf5\n",
      "\n",
      "Epoch 00599: val_loss improved from 0.17170 to 0.17077, saving model to ./model/final599-0.1708.hdf5\n",
      "\n",
      "Epoch 00600: val_loss improved from 0.17077 to 0.17020, saving model to ./model/final600-0.1702.hdf5\n",
      "\n",
      "Epoch 00601: val_loss improved from 0.17020 to 0.16992, saving model to ./model/final601-0.1699.hdf5\n",
      "\n",
      "Epoch 00602: val_loss improved from 0.16992 to 0.16940, saving model to ./model/final602-0.1694.hdf5\n",
      "\n",
      "Epoch 00603: val_loss improved from 0.16940 to 0.16873, saving model to ./model/final603-0.1687.hdf5\n",
      "\n",
      "Epoch 00604: val_loss improved from 0.16873 to 0.16853, saving model to ./model/final604-0.1685.hdf5\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 0.16853\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 0.16853\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 0.16853\n",
      "\n",
      "Epoch 00608: val_loss improved from 0.16853 to 0.16837, saving model to ./model/final608-0.1684.hdf5\n",
      "\n",
      "Epoch 00609: val_loss improved from 0.16837 to 0.16822, saving model to ./model/final609-0.1682.hdf5\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 0.16822\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 0.16822\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 0.16822\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 0.16822\n",
      "\n",
      "Epoch 00614: val_loss improved from 0.16822 to 0.16818, saving model to ./model/final614-0.1682.hdf5\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 0.16818\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 0.16818\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 0.16818\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 0.16818\n",
      "\n",
      "Epoch 00619: val_loss improved from 0.16818 to 0.16708, saving model to ./model/final619-0.1671.hdf5\n",
      "\n",
      "Epoch 00620: val_loss improved from 0.16708 to 0.16703, saving model to ./model/final620-0.1670.hdf5\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 0.16703\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 0.16703\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 0.16703\n",
      "\n",
      "Epoch 00624: val_loss improved from 0.16703 to 0.16625, saving model to ./model/final624-0.1662.hdf5\n",
      "\n",
      "Epoch 00625: val_loss improved from 0.16625 to 0.16557, saving model to ./model/final625-0.1656.hdf5\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 0.16557\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 0.16557\n",
      "\n",
      "Epoch 00628: val_loss improved from 0.16557 to 0.16531, saving model to ./model/final628-0.1653.hdf5\n",
      "\n",
      "Epoch 00629: val_loss improved from 0.16531 to 0.16374, saving model to ./model/final629-0.1637.hdf5\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 0.16374\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 0.16374\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 0.16374\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 0.16374\n",
      "\n",
      "Epoch 00634: val_loss improved from 0.16374 to 0.16361, saving model to ./model/final634-0.1636.hdf5\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 0.16361\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 0.16361\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 0.16361\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 0.16361\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 0.16361\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 0.16361\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 0.16361\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 0.16361\n",
      "\n",
      "Epoch 00643: val_loss improved from 0.16361 to 0.16326, saving model to ./model/final643-0.1633.hdf5\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 0.16326\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 0.16326\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 0.16326\n",
      "\n",
      "Epoch 00647: val_loss improved from 0.16326 to 0.16193, saving model to ./model/final647-0.1619.hdf5\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 0.16193\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 0.16193\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 0.16193\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 0.16193\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 0.16193\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 0.16193\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 0.16193\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 0.16193\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 0.16193\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 0.16193\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 0.16193\n",
      "\n",
      "Epoch 00659: val_loss improved from 0.16193 to 0.16160, saving model to ./model/final659-0.1616.hdf5\n",
      "\n",
      "Epoch 00660: val_loss improved from 0.16160 to 0.16084, saving model to ./model/final660-0.1608.hdf5\n",
      "\n",
      "Epoch 00661: val_loss improved from 0.16084 to 0.16068, saving model to ./model/final661-0.1607.hdf5\n",
      "\n",
      "Epoch 00662: val_loss improved from 0.16068 to 0.16040, saving model to ./model/final662-0.1604.hdf5\n",
      "\n",
      "Epoch 00663: val_loss improved from 0.16040 to 0.15982, saving model to ./model/final663-0.1598.hdf5\n",
      "\n",
      "Epoch 00664: val_loss improved from 0.15982 to 0.15938, saving model to ./model/final664-0.1594.hdf5\n",
      "\n",
      "Epoch 00665: val_loss improved from 0.15938 to 0.15928, saving model to ./model/final665-0.1593.hdf5\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 0.15928\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 0.15928\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 0.15928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00669: val_loss did not improve from 0.15928\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 0.15928\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 0.15928\n",
      "\n",
      "Epoch 00672: val_loss improved from 0.15928 to 0.15911, saving model to ./model/final672-0.1591.hdf5\n",
      "\n",
      "Epoch 00673: val_loss improved from 0.15911 to 0.15900, saving model to ./model/final673-0.1590.hdf5\n",
      "\n",
      "Epoch 00674: val_loss improved from 0.15900 to 0.15891, saving model to ./model/final674-0.1589.hdf5\n",
      "\n",
      "Epoch 00675: val_loss improved from 0.15891 to 0.15878, saving model to ./model/final675-0.1588.hdf5\n",
      "\n",
      "Epoch 00676: val_loss improved from 0.15878 to 0.15876, saving model to ./model/final676-0.1588.hdf5\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 0.15876\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 0.15876\n",
      "\n",
      "Epoch 00679: val_loss improved from 0.15876 to 0.15858, saving model to ./model/final679-0.1586.hdf5\n",
      "\n",
      "Epoch 00680: val_loss improved from 0.15858 to 0.15831, saving model to ./model/final680-0.1583.hdf5\n",
      "\n",
      "Epoch 00681: val_loss improved from 0.15831 to 0.15813, saving model to ./model/final681-0.1581.hdf5\n",
      "\n",
      "Epoch 00682: val_loss improved from 0.15813 to 0.15799, saving model to ./model/final682-0.1580.hdf5\n",
      "\n",
      "Epoch 00683: val_loss improved from 0.15799 to 0.15779, saving model to ./model/final683-0.1578.hdf5\n",
      "\n",
      "Epoch 00684: val_loss improved from 0.15779 to 0.15752, saving model to ./model/final684-0.1575.hdf5\n",
      "\n",
      "Epoch 00685: val_loss improved from 0.15752 to 0.15723, saving model to ./model/final685-0.1572.hdf5\n",
      "\n",
      "Epoch 00686: val_loss improved from 0.15723 to 0.15720, saving model to ./model/final686-0.1572.hdf5\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 0.15720\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 0.15720\n",
      "\n",
      "Epoch 00689: val_loss improved from 0.15720 to 0.15682, saving model to ./model/final689-0.1568.hdf5\n",
      "\n",
      "Epoch 00690: val_loss improved from 0.15682 to 0.15654, saving model to ./model/final690-0.1565.hdf5\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 0.15654\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 0.15654\n",
      "\n",
      "Epoch 00693: val_loss improved from 0.15654 to 0.15607, saving model to ./model/final693-0.1561.hdf5\n",
      "\n",
      "Epoch 00694: val_loss improved from 0.15607 to 0.15597, saving model to ./model/final694-0.1560.hdf5\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 0.15597\n",
      "\n",
      "Epoch 00696: val_loss improved from 0.15597 to 0.15594, saving model to ./model/final696-0.1559.hdf5\n",
      "\n",
      "Epoch 00697: val_loss improved from 0.15594 to 0.15531, saving model to ./model/final697-0.1553.hdf5\n",
      "\n",
      "Epoch 00698: val_loss improved from 0.15531 to 0.15527, saving model to ./model/final698-0.1553.hdf5\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 0.15527\n",
      "\n",
      "Epoch 00700: val_loss improved from 0.15527 to 0.15519, saving model to ./model/final700-0.1552.hdf5\n",
      "\n",
      "Epoch 00701: val_loss improved from 0.15519 to 0.15476, saving model to ./model/final701-0.1548.hdf5\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 0.15476\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 0.15476\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 0.15476\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 0.15476\n",
      "\n",
      "Epoch 00706: val_loss improved from 0.15476 to 0.15410, saving model to ./model/final706-0.1541.hdf5\n",
      "\n",
      "Epoch 00707: val_loss improved from 0.15410 to 0.15378, saving model to ./model/final707-0.1538.hdf5\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 0.15378\n",
      "\n",
      "Epoch 00709: val_loss improved from 0.15378 to 0.15364, saving model to ./model/final709-0.1536.hdf5\n",
      "\n",
      "Epoch 00710: val_loss improved from 0.15364 to 0.15305, saving model to ./model/final710-0.1530.hdf5\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 0.15305\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 0.15305\n",
      "\n",
      "Epoch 00713: val_loss improved from 0.15305 to 0.15294, saving model to ./model/final713-0.1529.hdf5\n",
      "\n",
      "Epoch 00714: val_loss improved from 0.15294 to 0.15196, saving model to ./model/final714-0.1520.hdf5\n",
      "\n",
      "Epoch 00715: val_loss improved from 0.15196 to 0.15071, saving model to ./model/final715-0.1507.hdf5\n",
      "\n",
      "Epoch 00716: val_loss improved from 0.15071 to 0.15034, saving model to ./model/final716-0.1503.hdf5\n",
      "\n",
      "Epoch 00717: val_loss improved from 0.15034 to 0.15027, saving model to ./model/final717-0.1503.hdf5\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 0.15027\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 0.15027\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 0.15027\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 0.15027\n",
      "\n",
      "Epoch 00722: val_loss improved from 0.15027 to 0.14886, saving model to ./model/final722-0.1489.hdf5\n",
      "\n",
      "Epoch 00723: val_loss improved from 0.14886 to 0.14770, saving model to ./model/final723-0.1477.hdf5\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 0.14770\n",
      "\n",
      "Epoch 00725: val_loss improved from 0.14770 to 0.14663, saving model to ./model/final725-0.1466.hdf5\n",
      "\n",
      "Epoch 00726: val_loss improved from 0.14663 to 0.14600, saving model to ./model/final726-0.1460.hdf5\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00728: val_loss improved from 0.14600 to 0.14560, saving model to ./model/final728-0.1456.hdf5\n",
      "\n",
      "Epoch 00729: val_loss improved from 0.14560 to 0.14516, saving model to ./model/final729-0.1452.hdf5\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 0.14516\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 0.14516\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 0.14516\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 0.14516\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 0.14516\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 0.14516\n",
      "\n",
      "Epoch 00736: val_loss improved from 0.14516 to 0.14427, saving model to ./model/final736-0.1443.hdf5\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 0.14427\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 0.14427\n",
      "\n",
      "Epoch 00739: val_loss improved from 0.14427 to 0.14410, saving model to ./model/final739-0.1441.hdf5\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 0.14410\n",
      "\n",
      "Epoch 00787: val_loss improved from 0.14410 to 0.14356, saving model to ./model/final787-0.1436.hdf5\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 0.14356\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 0.14356\n",
      "\n",
      "Epoch 00790: val_loss improved from 0.14356 to 0.14326, saving model to ./model/final790-0.1433.hdf5\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 0.14326\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 0.14326\n",
      "\n",
      "Epoch 00793: val_loss improved from 0.14326 to 0.14315, saving model to ./model/final793-0.1431.hdf5\n"
=======
      "Epoch 00275: val_loss did not improve from 0.16280\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.16280\n",
      "\n",
      "Epoch 00277: val_loss improved from 0.16280 to 0.16204, saving model to ./model/final277-0.1620.hdf5\n",
      "\n",
      "Epoch 00278: val_loss improved from 0.16204 to 0.16158, saving model to ./model/final278-0.1616.hdf5\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.16158\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.16158\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.16158\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.16158\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.16158\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.16158\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.16158\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.16158\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.16158\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.16158\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.16158\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.16158\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.16158\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.16158\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.16158\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.16158\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.16158\n",
      "\n",
      "Epoch 00296: val_loss improved from 0.16158 to 0.16156, saving model to ./model/final296-0.1616.hdf5\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.16156\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.16156\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.16156\n",
      "\n",
      "Epoch 00300: val_loss improved from 0.16156 to 0.16135, saving model to ./model/final300-0.1613.hdf5\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.16135\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.16135\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.16135\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.16135\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.16135\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.16135\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.16135\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.16135\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.16135\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.16135\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.16135\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.16135\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.16135\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.16135\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.16135\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.16135\n",
      "\n",
      "Epoch 00317: val_loss improved from 0.16135 to 0.16034, saving model to ./model/final317-0.1603.hdf5\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00336: val_loss improved from 0.16034 to 0.16007, saving model to ./model/final336-0.1601.hdf5\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.16007\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.16007\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.16007\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.16007\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.16007\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.16007\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.16007\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.16007\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.16007\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.16007\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.16007\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.16007\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.16007\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.16007\n",
      "\n",
      "Epoch 00351: val_loss improved from 0.16007 to 0.15854, saving model to ./model/final351-0.1585.hdf5\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.15854\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.15854\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.15854\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.15854\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.15854\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.15854\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.15854\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.15854\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.15854\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.15854\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.15854\n",
      "\n",
      "Epoch 00363: val_loss improved from 0.15854 to 0.15787, saving model to ./model/final363-0.1579.hdf5\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.15787\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.15787\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.15787\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.15787\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.15787\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.15787\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.15787\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.15787\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.15787\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.15787\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.15787\n",
      "\n",
      "Epoch 00375: val_loss improved from 0.15787 to 0.15560, saving model to ./model/final375-0.1556.hdf5\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.15560\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.15560\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.15560\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.15560\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.15560\n",
      "\n",
      "Epoch 00381: val_loss improved from 0.15560 to 0.15407, saving model to ./model/final381-0.1541.hdf5\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00406: val_loss improved from 0.15407 to 0.15406, saving model to ./model/final406-0.1541.hdf5\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.15406\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.15406\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.15406\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.15406\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.15406\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.15406\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.15406\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.15406\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.15406\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.15406\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.15406\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.15406\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.15406\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.15406\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.15406\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.15406\n",
      "\n",
      "Epoch 00423: val_loss improved from 0.15406 to 0.15366, saving model to ./model/final423-0.1537.hdf5\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.15366\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.15366\n",
      "\n",
      "Epoch 00426: val_loss improved from 0.15366 to 0.15340, saving model to ./model/final426-0.1534.hdf5\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.15340\n",
      "\n",
      "Epoch 00428: val_loss improved from 0.15340 to 0.15318, saving model to ./model/final428-0.1532.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00429: val_loss improved from 0.15318 to 0.14960, saving model to ./model/final429-0.1496.hdf5\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.14960\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.14960\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.14960\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.14960\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.14960\n",
      "\n",
      "Epoch 00435: val_loss improved from 0.14960 to 0.14950, saving model to ./model/final435-0.1495.hdf5\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.14950\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.14950\n",
      "\n",
      "Epoch 00438: val_loss improved from 0.14950 to 0.14862, saving model to ./model/final438-0.1486.hdf5\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.14862\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.14862\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.14862\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.14862\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.14862\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.14862\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.14862\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.14862\n",
      "\n",
      "Epoch 00447: val_loss improved from 0.14862 to 0.14628, saving model to ./model/final447-0.1463.hdf5\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.14628\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.14628\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.14628\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.14628\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.14628\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.14628\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.14628\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.14628\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.14628\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.14628\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.14628\n",
      "\n",
      "Epoch 00459: val_loss improved from 0.14628 to 0.14600, saving model to ./model/final459-0.1460.hdf5\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00477: val_loss improved from 0.14600 to 0.14486, saving model to ./model/final477-0.1449.hdf5\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00507: val_loss improved from 0.14486 to 0.14326, saving model to ./model/final507-0.1433.hdf5\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 0.14326\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 0.14326\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 0.14326\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 0.14326\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 0.14326\n",
      "\n",
      "Epoch 00513: val_loss improved from 0.14326 to 0.14095, saving model to ./model/final513-0.1410.hdf5\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 0.14095\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 0.14095\n",
      "\n",
      "Epoch 00516: val_loss improved from 0.14095 to 0.13953, saving model to ./model/final516-0.1395.hdf5\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 0.13953\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 0.13953\n",
      "\n",
      "Epoch 00519: val_loss improved from 0.13953 to 0.13757, saving model to ./model/final519-0.1376.hdf5\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 0.13757\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 0.13757\n",
      "\n",
      "Epoch 00522: val_loss improved from 0.13757 to 0.13685, saving model to ./model/final522-0.1369.hdf5\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00547: val_loss improved from 0.13685 to 0.13645, saving model to ./model/final547-0.1365.hdf5\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 0.13645\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 0.13645\n",
      "\n",
      "Epoch 00550: val_loss improved from 0.13645 to 0.13331, saving model to ./model/final550-0.1333.hdf5\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00584: val_loss improved from 0.13331 to 0.13078, saving model to ./model/final584-0.1308.hdf5\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 0.13078\n"
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
=======
      "Epoch 00140: val_loss did not improve from 0.19549\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.19549\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.19549 to 0.19530, saving model to ./model/final142-0.1953.hdf5\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.19530\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.19530\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.19530\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.19530\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.19530\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.19530 to 0.19506, saving model to ./model/final148-0.1951.hdf5\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.19506 to 0.19411, saving model to ./model/final149-0.1941.hdf5\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.19411 to 0.19399, saving model to ./model/final150-0.1940.hdf5\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.19399 to 0.19383, saving model to ./model/final151-0.1938.hdf5\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.19383 to 0.19313, saving model to ./model/final152-0.1931.hdf5\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.19313 to 0.19307, saving model to ./model/final153-0.1931.hdf5\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.19307\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.19307\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.19307\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.19307\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.19307\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.19307 to 0.19216, saving model to ./model/final159-0.1922.hdf5\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.19216 to 0.19092, saving model to ./model/final160-0.1909.hdf5\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.19092 to 0.19004, saving model to ./model/final161-0.1900.hdf5\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.19004 to 0.18994, saving model to ./model/final162-0.1899.hdf5\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.18994\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.18994\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.18994\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.18994\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.18994 to 0.18943, saving model to ./model/final167-0.1894.hdf5\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.18943 to 0.18852, saving model to ./model/final168-0.1885.hdf5\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.18852 to 0.18769, saving model to ./model/final169-0.1877.hdf5\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.18769 to 0.18708, saving model to ./model/final170-0.1871.hdf5\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.18708 to 0.18688, saving model to ./model/final171-0.1869.hdf5\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.18688\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.18688\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.18688\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.18688\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.18688 to 0.18608, saving model to ./model/final176-0.1861.hdf5\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.18608 to 0.18541, saving model to ./model/final177-0.1854.hdf5\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.18541 to 0.18479, saving model to ./model/final178-0.1848.hdf5\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.18479 to 0.18435, saving model to ./model/final179-0.1843.hdf5\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.18435\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.18435\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.18435\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.18435\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.18435 to 0.18385, saving model to ./model/final184-0.1839.hdf5\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.18385 to 0.18290, saving model to ./model/final185-0.1829.hdf5\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.18290\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.18290\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.18290\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.18290 to 0.18228, saving model to ./model/final189-0.1823.hdf5\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.18228 to 0.18184, saving model to ./model/final190-0.1818.hdf5\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.18184 to 0.18165, saving model to ./model/final191-0.1816.hdf5\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.18165 to 0.18121, saving model to ./model/final192-0.1812.hdf5\n",
      "\n",
      "Epoch 00193: val_loss improved from 0.18121 to 0.18095, saving model to ./model/final193-0.1810.hdf5\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.18095\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.18095\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.18095\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.18095\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.18095\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.18095\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.18095 to 0.17913, saving model to ./model/final200-0.1791.hdf5\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.17913\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.17913\n",
      "\n",
      "Epoch 00203: val_loss improved from 0.17913 to 0.17833, saving model to ./model/final203-0.1783.hdf5\n",
      "\n",
      "Epoch 00204: val_loss improved from 0.17833 to 0.17718, saving model to ./model/final204-0.1772.hdf5\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.17718\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.17718\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.17718\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.17718\n",
      "\n",
      "Epoch 00209: val_loss improved from 0.17718 to 0.17704, saving model to ./model/final209-0.1770.hdf5\n",
      "\n",
      "Epoch 00210: val_loss improved from 0.17704 to 0.17636, saving model to ./model/final210-0.1764.hdf5\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.17636\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.17636\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.17636\n",
      "\n",
      "Epoch 00214: val_loss improved from 0.17636 to 0.17540, saving model to ./model/final214-0.1754.hdf5\n",
      "\n",
      "Epoch 00215: val_loss improved from 0.17540 to 0.17434, saving model to ./model/final215-0.1743.hdf5\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.17434\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.17434\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.17434\n",
      "\n",
      "Epoch 00219: val_loss improved from 0.17434 to 0.17431, saving model to ./model/final219-0.1743.hdf5\n",
      "\n",
      "Epoch 00220: val_loss improved from 0.17431 to 0.17303, saving model to ./model/final220-0.1730.hdf5\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.17303\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.17303\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.17303\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.17303\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.17303\n",
      "\n",
      "Epoch 00226: val_loss improved from 0.17303 to 0.17256, saving model to ./model/final226-0.1726.hdf5\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.17256\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.17256\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.17256\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.17256\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.17256\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.17256\n",
      "\n",
      "Epoch 00233: val_loss improved from 0.17256 to 0.17121, saving model to ./model/final233-0.1712.hdf5\n",
      "\n",
      "Epoch 00234: val_loss improved from 0.17121 to 0.17093, saving model to ./model/final234-0.1709.hdf5\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.17093\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.17093\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.17093\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.17093\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.17093\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.17093\n",
      "\n",
      "Epoch 00241: val_loss improved from 0.17093 to 0.17081, saving model to ./model/final241-0.1708.hdf5\n",
      "\n",
      "Epoch 00242: val_loss improved from 0.17081 to 0.17021, saving model to ./model/final242-0.1702.hdf5\n",
      "\n",
      "Epoch 00243: val_loss improved from 0.17021 to 0.16993, saving model to ./model/final243-0.1699.hdf5\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.16993\n",
      "\n",
      "Epoch 00245: val_loss improved from 0.16993 to 0.16952, saving model to ./model/final245-0.1695.hdf5\n",
      "\n",
      "Epoch 00246: val_loss improved from 0.16952 to 0.16860, saving model to ./model/final246-0.1686.hdf5\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.16860\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.16860\n",
      "\n",
      "Epoch 00249: val_loss improved from 0.16860 to 0.16846, saving model to ./model/final249-0.1685.hdf5\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.16846\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.16846\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.16846\n",
      "\n",
      "Epoch 00253: val_loss improved from 0.16846 to 0.16530, saving model to ./model/final253-0.1653.hdf5\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.16530\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.16530\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.16530\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.16530\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.16530\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.16530\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.16530\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.16530\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.16530\n",
      "\n",
      "Epoch 00263: val_loss improved from 0.16530 to 0.16458, saving model to ./model/final263-0.1646.hdf5\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.16458\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.16458\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.16458\n",
      "\n",
      "Epoch 00267: val_loss improved from 0.16458 to 0.16280, saving model to ./model/final267-0.1628.hdf5\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.16280\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.16280\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.16280\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.16280\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.16280\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.16280\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.16280\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00275: val_loss did not improve from 0.16280\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.16280\n",
      "\n",
      "Epoch 00277: val_loss improved from 0.16280 to 0.16204, saving model to ./model/final277-0.1620.hdf5\n",
      "\n",
      "Epoch 00278: val_loss improved from 0.16204 to 0.16158, saving model to ./model/final278-0.1616.hdf5\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.16158\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.16158\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.16158\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.16158\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.16158\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.16158\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.16158\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.16158\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.16158\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.16158\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.16158\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.16158\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.16158\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.16158\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.16158\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.16158\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.16158\n",
      "\n",
      "Epoch 00296: val_loss improved from 0.16158 to 0.16156, saving model to ./model/final296-0.1616.hdf5\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.16156\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.16156\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.16156\n",
      "\n",
      "Epoch 00300: val_loss improved from 0.16156 to 0.16135, saving model to ./model/final300-0.1613.hdf5\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.16135\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.16135\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.16135\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.16135\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.16135\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.16135\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.16135\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.16135\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.16135\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.16135\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.16135\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.16135\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.16135\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.16135\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.16135\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.16135\n",
      "\n",
      "Epoch 00317: val_loss improved from 0.16135 to 0.16034, saving model to ./model/final317-0.1603.hdf5\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.16034\n",
      "\n",
      "Epoch 00336: val_loss improved from 0.16034 to 0.16007, saving model to ./model/final336-0.1601.hdf5\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.16007\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.16007\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.16007\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.16007\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.16007\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.16007\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.16007\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.16007\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.16007\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.16007\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.16007\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.16007\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.16007\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.16007\n",
      "\n",
      "Epoch 00351: val_loss improved from 0.16007 to 0.15854, saving model to ./model/final351-0.1585.hdf5\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.15854\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.15854\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.15854\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.15854\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.15854\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.15854\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.15854\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.15854\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.15854\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.15854\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.15854\n",
      "\n",
      "Epoch 00363: val_loss improved from 0.15854 to 0.15787, saving model to ./model/final363-0.1579.hdf5\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.15787\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.15787\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.15787\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.15787\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.15787\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.15787\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.15787\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.15787\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.15787\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.15787\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.15787\n",
      "\n",
      "Epoch 00375: val_loss improved from 0.15787 to 0.15560, saving model to ./model/final375-0.1556.hdf5\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.15560\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.15560\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.15560\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.15560\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.15560\n",
      "\n",
      "Epoch 00381: val_loss improved from 0.15560 to 0.15407, saving model to ./model/final381-0.1541.hdf5\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.15407\n",
      "\n",
      "Epoch 00406: val_loss improved from 0.15407 to 0.15406, saving model to ./model/final406-0.1541.hdf5\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.15406\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.15406\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.15406\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.15406\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.15406\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.15406\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.15406\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.15406\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.15406\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.15406\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.15406\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.15406\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.15406\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.15406\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.15406\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.15406\n",
      "\n",
      "Epoch 00423: val_loss improved from 0.15406 to 0.15366, saving model to ./model/final423-0.1537.hdf5\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.15366\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.15366\n",
      "\n",
      "Epoch 00426: val_loss improved from 0.15366 to 0.15340, saving model to ./model/final426-0.1534.hdf5\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.15340\n",
      "\n",
      "Epoch 00428: val_loss improved from 0.15340 to 0.15318, saving model to ./model/final428-0.1532.hdf5\n"
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
<<<<<<< HEAD
<<<<<<< HEAD
      "Epoch 00794: val_loss did not improve from 0.14315\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 0.14315\n",
      "\n",
      "Epoch 00796: val_loss improved from 0.14315 to 0.14307, saving model to ./model/final796-0.1431.hdf5\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 0.14307\n",
      "\n",
      "Epoch 00798: val_loss improved from 0.14307 to 0.14271, saving model to ./model/final798-0.1427.hdf5\n",
      "\n",
      "Epoch 00799: val_loss improved from 0.14271 to 0.14245, saving model to ./model/final799-0.1424.hdf5\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 0.14245\n",
      "\n",
      "Epoch 00801: val_loss improved from 0.14245 to 0.14219, saving model to ./model/final801-0.1422.hdf5\n",
      "\n",
      "Epoch 00802: val_loss improved from 0.14219 to 0.14207, saving model to ./model/final802-0.1421.hdf5\n",
      "\n",
      "Epoch 00803: val_loss did not improve from 0.14207\n",
      "\n",
      "Epoch 00804: val_loss improved from 0.14207 to 0.14193, saving model to ./model/final804-0.1419.hdf5\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 0.14193\n",
      "\n",
      "Epoch 00806: val_loss did not improve from 0.14193\n",
      "\n",
      "Epoch 00807: val_loss improved from 0.14193 to 0.14181, saving model to ./model/final807-0.1418.hdf5\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 0.14181\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 0.14181\n",
      "\n",
      "Epoch 00810: val_loss improved from 0.14181 to 0.14104, saving model to ./model/final810-0.1410.hdf5\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 0.14104\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 0.14104\n",
      "\n",
      "Epoch 00813: val_loss improved from 0.14104 to 0.14051, saving model to ./model/final813-0.1405.hdf5\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 0.14051\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 0.14051\n",
      "\n",
      "Epoch 00816: val_loss improved from 0.14051 to 0.14024, saving model to ./model/final816-0.1402.hdf5\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 0.14024\n",
      "\n",
      "Epoch 00818: val_loss did not improve from 0.14024\n",
      "\n",
      "Epoch 00819: val_loss improved from 0.14024 to 0.14011, saving model to ./model/final819-0.1401.hdf5\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 0.14011\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 0.14011\n",
      "\n",
      "Epoch 00822: val_loss improved from 0.14011 to 0.13993, saving model to ./model/final822-0.1399.hdf5\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 0.13993\n",
      "\n",
      "Epoch 00824: val_loss improved from 0.13993 to 0.13970, saving model to ./model/final824-0.1397.hdf5\n",
      "\n",
      "Epoch 00825: val_loss improved from 0.13970 to 0.13903, saving model to ./model/final825-0.1390.hdf5\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 0.13903\n",
      "\n",
      "Epoch 00827: val_loss improved from 0.13903 to 0.13882, saving model to ./model/final827-0.1388.hdf5\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 0.13882\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 0.13882\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 0.13882\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 0.13882\n",
      "\n",
      "Epoch 00832: val_loss improved from 0.13882 to 0.13820, saving model to ./model/final832-0.1382.hdf5\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 0.13820\n",
      "\n",
      "Epoch 00834: val_loss improved from 0.13820 to 0.13819, saving model to ./model/final834-0.1382.hdf5\n",
      "\n",
      "Epoch 00835: val_loss improved from 0.13819 to 0.13766, saving model to ./model/final835-0.1377.hdf5\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 0.13766\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 0.13766\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 0.13766\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 0.13766\n",
      "\n",
      "Epoch 00840: val_loss improved from 0.13766 to 0.13730, saving model to ./model/final840-0.1373.hdf5\n",
      "\n",
      "Epoch 00841: val_loss improved from 0.13730 to 0.13727, saving model to ./model/final841-0.1373.hdf5\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 0.13727\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 0.13727\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 0.13727\n",
      "\n",
      "Epoch 00845: val_loss improved from 0.13727 to 0.13697, saving model to ./model/final845-0.1370.hdf5\n",
      "\n",
      "Epoch 00846: val_loss improved from 0.13697 to 0.13655, saving model to ./model/final846-0.1366.hdf5\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 0.13655\n",
      "\n",
      "Epoch 00848: val_loss improved from 0.13655 to 0.13654, saving model to ./model/final848-0.1365.hdf5\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 0.13654\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 0.13654\n",
      "\n",
      "Epoch 00851: val_loss improved from 0.13654 to 0.13599, saving model to ./model/final851-0.1360.hdf5\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 0.13599\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 0.13599\n",
      "\n",
      "Epoch 00854: val_loss improved from 0.13599 to 0.13556, saving model to ./model/final854-0.1356.hdf5\n",
      "\n",
      "Epoch 00855: val_loss did not improve from 0.13556\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 0.13556\n",
      "\n",
      "Epoch 00857: val_loss did not improve from 0.13556\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 0.13556\n",
      "\n",
      "Epoch 00859: val_loss improved from 0.13556 to 0.13512, saving model to ./model/final859-0.1351.hdf5\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 0.13512\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 0.13512\n",
      "\n",
      "Epoch 00862: val_loss improved from 0.13512 to 0.13496, saving model to ./model/final862-0.1350.hdf5\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 0.13496\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 0.13496\n",
      "\n",
      "Epoch 00865: val_loss improved from 0.13496 to 0.13454, saving model to ./model/final865-0.1345.hdf5\n",
      "\n",
      "Epoch 00866: val_loss did not improve from 0.13454\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 0.13454\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 0.13454\n",
      "\n",
      "Epoch 00869: val_loss improved from 0.13454 to 0.13440, saving model to ./model/final869-0.1344.hdf5\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 0.13440\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 0.13440\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 0.13440\n",
      "\n",
      "Epoch 00873: val_loss improved from 0.13440 to 0.13416, saving model to ./model/final873-0.1342.hdf5\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 0.13416\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 0.13416\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 0.13416\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 0.13416\n",
      "\n",
      "Epoch 00878: val_loss improved from 0.13416 to 0.13414, saving model to ./model/final878-0.1341.hdf5\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 0.13414\n",
      "\n",
      "Epoch 00880: val_loss improved from 0.13414 to 0.13407, saving model to ./model/final880-0.1341.hdf5\n",
      "\n",
      "Epoch 00881: val_loss improved from 0.13407 to 0.13392, saving model to ./model/final881-0.1339.hdf5\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 0.13392\n",
      "\n",
      "Epoch 00883: val_loss did not improve from 0.13392\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 0.13392\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 0.13392\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 0.13392\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 0.13392\n",
      "\n",
      "Epoch 00888: val_loss improved from 0.13392 to 0.13363, saving model to ./model/final888-0.1336.hdf5\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 0.13363\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 0.13363\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 0.13363\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 0.13363\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 0.13363\n",
      "\n",
      "Epoch 00894: val_loss improved from 0.13363 to 0.13347, saving model to ./model/final894-0.1335.hdf5\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 0.13347\n",
      "\n",
      "Epoch 00896: val_loss improved from 0.13347 to 0.13303, saving model to ./model/final896-0.1330.hdf5\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 0.13303\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 0.13303\n",
      "\n",
      "Epoch 00899: val_loss did not improve from 0.13303\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 0.13303\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 0.13303\n",
      "\n",
      "Epoch 00902: val_loss improved from 0.13303 to 0.13301, saving model to ./model/final902-0.1330.hdf5\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 0.13301\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 0.13301\n",
      "\n",
      "Epoch 00905: val_loss did not improve from 0.13301\n",
      "\n",
      "Epoch 00906: val_loss improved from 0.13301 to 0.13262, saving model to ./model/final906-0.1326.hdf5\n",
      "\n",
      "Epoch 00907: val_loss improved from 0.13262 to 0.13259, saving model to ./model/final907-0.1326.hdf5\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 0.13259\n",
      "\n",
      "Epoch 00909: val_loss improved from 0.13259 to 0.13210, saving model to ./model/final909-0.1321.hdf5\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 0.13210\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 0.13210\n",
      "\n",
      "Epoch 00912: val_loss did not improve from 0.13210\n",
      "\n",
      "Epoch 00913: val_loss did not improve from 0.13210\n",
      "\n",
      "Epoch 00914: val_loss improved from 0.13210 to 0.13098, saving model to ./model/final914-0.1310.hdf5\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 0.13098\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 0.13098\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 0.13098\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 0.13098\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 0.13098\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 0.13098\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 0.13098\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 0.13098\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 0.13098\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 0.13098\n",
      "\n",
      "Epoch 00925: val_loss improved from 0.13098 to 0.13035, saving model to ./model/final925-0.1303.hdf5\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 0.13035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00927: val_loss improved from 0.13035 to 0.12628, saving model to ./model/final927-0.1263.hdf5\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 0.12628\n",
      "\n",
      "Epoch 00929: val_loss improved from 0.12628 to 0.12506, saving model to ./model/final929-0.1251.hdf5\n",
      "\n",
      "Epoch 00930: val_loss did not improve from 0.12506\n",
      "\n",
      "Epoch 00931: val_loss did not improve from 0.12506\n",
      "\n",
      "Epoch 00932: val_loss did not improve from 0.12506\n",
      "\n",
      "Epoch 00933: val_loss did not improve from 0.12506\n",
      "\n",
      "Epoch 00934: val_loss did not improve from 0.12506\n",
      "\n",
      "Epoch 00935: val_loss did not improve from 0.12506\n",
      "\n",
      "Epoch 00936: val_loss improved from 0.12506 to 0.12271, saving model to ./model/final936-0.1227.hdf5\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 0.12271\n",
      "\n",
      "Epoch 00938: val_loss did not improve from 0.12271\n",
      "\n",
      "Epoch 00939: val_loss did not improve from 0.12271\n",
      "\n",
      "Epoch 00940: val_loss did not improve from 0.12271\n",
      "\n",
      "Epoch 00941: val_loss did not improve from 0.12271\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 0.12271\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 0.12271\n",
      "\n",
      "Epoch 00944: val_loss did not improve from 0.12271\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 0.12271\n",
      "\n",
      "Epoch 00946: val_loss did not improve from 0.12271\n",
      "\n",
      "Epoch 00947: val_loss did not improve from 0.12271\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 0.12271\n",
      "\n",
      "Epoch 00949: val_loss did not improve from 0.12271\n",
      "\n",
      "Epoch 00950: val_loss did not improve from 0.12271\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 0.12271\n",
      "\n",
      "Epoch 00952: val_loss did not improve from 0.12271\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 0.12271\n",
      "\n",
      "Epoch 00954: val_loss improved from 0.12271 to 0.12208, saving model to ./model/final954-0.1221.hdf5\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 0.12208\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 0.12208\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 0.12208\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 0.12208\n",
      "\n",
      "Epoch 00959: val_loss did not improve from 0.12208\n",
      "\n",
      "Epoch 00960: val_loss did not improve from 0.12208\n",
      "\n",
      "Epoch 00961: val_loss did not improve from 0.12208\n",
      "\n",
      "Epoch 00962: val_loss did not improve from 0.12208\n",
      "\n",
      "Epoch 00963: val_loss did not improve from 0.12208\n",
      "\n",
      "Epoch 00964: val_loss did not improve from 0.12208\n",
      "\n",
      "Epoch 00965: val_loss did not improve from 0.12208\n",
      "\n",
      "Epoch 00966: val_loss did not improve from 0.12208\n",
      "\n",
      "Epoch 00967: val_loss improved from 0.12208 to 0.12122, saving model to ./model/final967-0.1212.hdf5\n",
      "\n",
      "Epoch 00968: val_loss did not improve from 0.12122\n",
      "\n",
      "Epoch 00969: val_loss improved from 0.12122 to 0.12008, saving model to ./model/final969-0.1201.hdf5\n",
      "\n",
      "Epoch 00970: val_loss did not improve from 0.12008\n",
      "\n",
      "Epoch 00971: val_loss did not improve from 0.12008\n",
      "\n",
      "Epoch 00972: val_loss did not improve from 0.12008\n",
      "\n",
      "Epoch 00973: val_loss did not improve from 0.12008\n",
      "\n",
      "Epoch 00974: val_loss did not improve from 0.12008\n",
      "\n",
      "Epoch 00975: val_loss did not improve from 0.12008\n",
      "\n",
      "Epoch 00976: val_loss did not improve from 0.12008\n",
      "\n",
      "Epoch 00977: val_loss did not improve from 0.12008\n",
      "\n",
      "Epoch 00978: val_loss did not improve from 0.12008\n",
      "\n",
      "Epoch 00979: val_loss did not improve from 0.12008\n",
      "\n",
      "Epoch 00980: val_loss did not improve from 0.12008\n",
      "\n",
      "Epoch 00981: val_loss did not improve from 0.12008\n",
      "\n",
      "Epoch 00982: val_loss did not improve from 0.12008\n",
      "\n",
      "Epoch 00983: val_loss did not improve from 0.12008\n",
      "\n",
      "Epoch 00984: val_loss did not improve from 0.12008\n",
      "\n",
      "Epoch 00985: val_loss did not improve from 0.12008\n",
      "\n",
      "Epoch 00986: val_loss did not improve from 0.12008\n",
      "\n",
      "Epoch 00987: val_loss did not improve from 0.12008\n",
      "\n",
      "Epoch 00988: val_loss did not improve from 0.12008\n",
      "\n",
      "Epoch 00989: val_loss did not improve from 0.12008\n",
      "\n",
      "Epoch 00990: val_loss improved from 0.12008 to 0.11946, saving model to ./model/final990-0.1195.hdf5\n",
      "\n",
      "Epoch 00991: val_loss did not improve from 0.11946\n",
      "\n",
      "Epoch 00992: val_loss improved from 0.11946 to 0.11723, saving model to ./model/final992-0.1172.hdf5\n",
      "\n",
      "Epoch 00993: val_loss did not improve from 0.11723\n",
      "\n",
      "Epoch 00994: val_loss improved from 0.11723 to 0.11585, saving model to ./model/final994-0.1158.hdf5\n",
      "\n",
      "Epoch 00995: val_loss did not improve from 0.11585\n",
      "\n",
      "Epoch 00996: val_loss improved from 0.11585 to 0.11453, saving model to ./model/final996-0.1145.hdf5\n",
      "\n",
      "Epoch 00997: val_loss did not improve from 0.11453\n",
      "\n",
      "Epoch 00998: val_loss did not improve from 0.11453\n",
      "\n",
      "Epoch 00999: val_loss did not improve from 0.11453\n",
      "\n",
      "Epoch 01000: val_loss did not improve from 0.11453\n"
=======
=======
      "Epoch 00429: val_loss improved from 0.15318 to 0.14960, saving model to ./model/final429-0.1496.hdf5\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.14960\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.14960\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.14960\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.14960\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.14960\n",
      "\n",
      "Epoch 00435: val_loss improved from 0.14960 to 0.14950, saving model to ./model/final435-0.1495.hdf5\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.14950\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.14950\n",
      "\n",
      "Epoch 00438: val_loss improved from 0.14950 to 0.14862, saving model to ./model/final438-0.1486.hdf5\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.14862\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.14862\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.14862\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.14862\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.14862\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.14862\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.14862\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.14862\n",
      "\n",
      "Epoch 00447: val_loss improved from 0.14862 to 0.14628, saving model to ./model/final447-0.1463.hdf5\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.14628\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.14628\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.14628\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.14628\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.14628\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.14628\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.14628\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.14628\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.14628\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.14628\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.14628\n",
      "\n",
      "Epoch 00459: val_loss improved from 0.14628 to 0.14600, saving model to ./model/final459-0.1460.hdf5\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.14600\n",
      "\n",
      "Epoch 00477: val_loss improved from 0.14600 to 0.14486, saving model to ./model/final477-0.1449.hdf5\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 0.14486\n",
      "\n",
      "Epoch 00507: val_loss improved from 0.14486 to 0.14326, saving model to ./model/final507-0.1433.hdf5\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 0.14326\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 0.14326\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 0.14326\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 0.14326\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 0.14326\n",
      "\n",
      "Epoch 00513: val_loss improved from 0.14326 to 0.14095, saving model to ./model/final513-0.1410.hdf5\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 0.14095\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 0.14095\n",
      "\n",
      "Epoch 00516: val_loss improved from 0.14095 to 0.13953, saving model to ./model/final516-0.1395.hdf5\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 0.13953\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 0.13953\n",
      "\n",
      "Epoch 00519: val_loss improved from 0.13953 to 0.13757, saving model to ./model/final519-0.1376.hdf5\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 0.13757\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 0.13757\n",
      "\n",
      "Epoch 00522: val_loss improved from 0.13757 to 0.13685, saving model to ./model/final522-0.1369.hdf5\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 0.13685\n",
      "\n",
      "Epoch 00547: val_loss improved from 0.13685 to 0.13645, saving model to ./model/final547-0.1365.hdf5\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 0.13645\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 0.13645\n",
      "\n",
      "Epoch 00550: val_loss improved from 0.13645 to 0.13331, saving model to ./model/final550-0.1333.hdf5\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 0.13331\n",
      "\n",
      "Epoch 00584: val_loss improved from 0.13331 to 0.13078, saving model to ./model/final584-0.1308.hdf5\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 0.13078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
      "Epoch 00589: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 0.13078\n",
      "\n",
      "Epoch 00620: val_loss improved from 0.13078 to 0.12944, saving model to ./model/final620-0.1294.hdf5\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 0.12944\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 0.12944\n",
      "\n",
      "Epoch 00623: val_loss improved from 0.12944 to 0.12874, saving model to ./model/final623-0.1287.hdf5\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 0.12874\n",
      "\n",
      "Epoch 00682: val_loss improved from 0.12874 to 0.12699, saving model to ./model/final682-0.1270.hdf5\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 0.12699\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 0.12699\n",
      "\n",
      "Epoch 00685: val_loss improved from 0.12699 to 0.12692, saving model to ./model/final685-0.1269.hdf5\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 0.12692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00743: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 0.12692\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 0.12692\n"
<<<<<<< HEAD
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
=======
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
     ]
    }
   ],
   "source": [
    "history = model.fit(X, Y, validation_split=0.33, epochs=1000, batch_size=500,\n",
    "                    verbose=0, callbacks=[early_stopping_callback, checkpointer_callback])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 27,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
=======
   "execution_count": 27,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 381 samples, validate on 188 samples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
<<<<<<< HEAD
       "<tensorflow.python.keras.callbacks.History at 0x2877ac58f88>"
      ]
     },
     "execution_count": 11,
=======
=======
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
       "<tensorflow.python.keras.callbacks.History at 0x21795325308>"
      ]
     },
     "execution_count": 27,
<<<<<<< HEAD
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
=======
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y, validation_split=0.33, epochs=0, batch_size=600, \n",
    "          verbose=1, callbacks=[early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 28,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
=======
   "execution_count": 28,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569/569 - 0s - loss: 0.1148 - accuracy: 0.9525\n",
      "\n",
      " Accuracy: 0.9525\n"
     ]
    }
   ],
   "source": [
    "del model\n",
    "model = load_model('model/final1000-0.1467.hdf5')\n",
    "print(\"\\n Accuracy: %.4f\" % (model.evaluate(X, Y, verbose=2)[1]))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 13,
=======
   "execution_count": 29,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
=======
   "execution_count": 29,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vloss=history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": 30,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
=======
   "execution_count": 30,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
   "metadata": {},
   "outputs": [],
   "source": [
    "y_acc=history.history['accuracy']"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 15,
=======
   "execution_count": 33,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
=======
   "execution_count": 33,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
<<<<<<< HEAD
<<<<<<< HEAD
      "image/png": "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\n",
=======
      "image/png": "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\n",
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
=======
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvoAAAFlCAYAAACax0zeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3db5Bs6X0X9u/vzljG/7C80sYRu1J2SQlH8t0A0kQYnLhMFMeSk/KGKlwlJdiggtqR7sgFyYtY5kVIKkUlhFTKoZiVRuUohgKsEkIOG5ewoJIQvQBjzRrZe+VFyiIb6UaGXXuJCVCFcu8+edHdTN9zT3efmemZnj7z+VSd6u7Tp885/czMvd/nOc/znGqtBQAAGJcbmz4BAABg/QR9AAAYIUEfAABGSNAHAIAREvQBAGCEBH0AABih3U0d+LWvfW177LHHNnX4s/nSl5KXXkoefjh5wxs2fTYAAJBnn33211prD3fXbyzoP/bYYzk+Pt7U4c9md1pcL7+cvPjiZs8FAACSVNU/6Fuv685p7O8nOzuTRwAAuMJqU3fG3dvba1vXog8AAFdMVT3bWtvrrteiDwAAIyToAwDACAn6AAAwQoI+AACMkKAPAAAjJOgDAMAICfoAADBCgj4AAIyQoA8AACMk6AMAwAgJ+gAAMEKCPgAAjJCgDwAAIyToAwDACAn6AAAwQoL+WR0cJLu7k0cAALhiBP2zOjpK7t2bPAIAwBUj6J/V/n6yszN5BACAK6Zaaxs58N7eXjs+Pt7IsQEAYCyq6tnW2l53vRZ9AAAYIUEfAABGSNAHAIARWhn0q+ojVfViVd1e8H5V1Z+pqheq6her6i3rP00AAOA0hrTo/0SSdyx5/51J3jhdnkrywfOfFgAAcB4rg35r7dNJXl6yyZNJ/nyb+Nkkr66q163rBAEAgNNbRx/9R5J8ee71nem6B1TVU1V1XFXHL7300hoODQAA9FlH0K+edb2T87fWPtxa22ut7T388MNrODQAANBnHUH/TpLXz71+NMlX1rBfAADgjNYR9J9J8kPT2Xe+I8lvtNZ+dQ37BQAAzmh31QZV9ZNJvjvJa6vqTpI/keRrkqS19qEkn0zyfUleSPLPk7znok4WAAAYZmXQb629e8X7LcnB2s4IAAA4N3fGBQCAERL0AQBghAR9AAC21sFBsrs7eZw9f+KJ5etWfbZq+TLb5saN+/d31dSki/3l29vba8fHxxs5NgDAdXJwkBwdJW96U/L888n+/mT90dHk+eHheo/19NOTIPzt335yvE9/Orl9O3nooeQ3fuPkuH3nNts2OdnP7PVVdOvWesvwtKrq2dba3gPrBX0AgM3qht1uIJ8F5/e97yRQzn/m9u2T95P71/fZ2Zk83rt3sm7R508TtGt6G9VuvNzZuf9YMzdvXu0AP9TOTnL37uaOvyjo67oDAFw5i7pbzHe1WPex5rtvzJ7Pumf0dftYtr/5bh6zz8+ve81rTt57zWsmQf7evUnonX88OposySQ8zwL/E09M1s+2nX9/fl9dN29OQun+/kklYqa1k+N19zs0jLf2YMhP+kN+sv6QXzX5jqvWLfvszs7J461bJ99ptty6dfLe7Hm3LK+M1tpGlre+9a1t1G7dmvw+VE2eA8AWmP/v6+bN1nZ2Th5v3Zo8TyaP5znGbH+z5/PHaG3yfD5e7ezcv362bfdcZ/tctn62zN5/MMqtXh566Gyf29TS913P8/1Pe+zuz7LvuPPnMx+f5n9f6JfkuPXk7QsJ8UOW0Qf9+X+hZv86AbAWfUFxUQjoC5KzMPHQQ/376X6mGxaXresLyauCVl/wqeoPk/PrZ4+r1lm2Y5n9PnaD8SxKdCs/Q5eZvs/PfocXHbfq/srV0L/NRRWv+b9TAX59BP3LpkUfYC36wsB88Oi2q3RDe1/LcF8Y6gs5Q7cb+lnL1V+6FbJllbFl+1h2tWJVRbXvysOiCuWqyueQ/Xbf77sSIpBfbYuCvsG4AFxpu7uT/r3zg936BiEmk/gyP8hwZ2f5gET63bo1eXz66cnj/GDMdc2A8tBDycsvL95fVfIt3zLZZrZt12zw6PxsLrN9Llo/PwtMcrZZZ2a/f+uerQbOyqw7ADxgaGCZ3y558DPz0+nNz9rRN2PI7PVVVzWpOJzlczduPDibyWxd32wn82bT9PVVcJLl0yRe9NSJi84J2KxFQf+BJv7LWkbfdQfYiO4l5mWvl12OXrbdssvxs0vi3b7RfYPMlvXr7jvWokvtiwYa9h2/+53mu5xcxqC8i+xucZouGcsGaXZ/PrOf5c2by9cN7ce/agzAot/lq+AqnhOg6w4wQrNW5GQ7bqhyWovml561ps5//+tq0VWCbleT+bnHAcZmUYv+7iZOBji9VaFujEH3NFob33df9H3u3Tvpk36Z5vs5z/pOz3T7RCf3V1QuI3gL8gD3c8OsLTR/I44bN9Z70xDWa/7GK92bpZx2WdVyO8aguw59N0oZcuOUZdstuvlK1f03UJndaGU2sHH+/UWfW/VdZv27F1l0c5dF573onPuWX//1yeMrr5w8ny2vvDIJ2s89d7Ju/vkrr5y8nm0LwMXSdWcLzQZDzWzToKjugL2x/WffHbB43btVXIZFgz/7Botuw+waywZa9v1edWcRGdvfFACrGYw7In2DxlZtv+pmMssGsM3fUOa8Tnuzj747L572+3c/d5bPLBvct2oQ5LoHHK66KZBBcgBwvcRg3O3R7Yt98+bkkvf8+r51M319ZbsWzUnM+s36KY/1KgYAsFnm0T+vS7x+3+2ak9x/M5iZ1vq3ZXP6BsRu6E8MALgmFgV9g3GHOjqaJOqjows7xGyQbV9wn3XemJkNqJv12V1l1eDD2YC8vkF8OzvDBy8OsehYiwYQLvo+q7bpO+5Zvseyc50//s2bJwMO5wc2AgBsghb9oS6hRX++dX7RPNlDBt6uGvB6HW7dfR2+IwBAouvOlffEE/fPN90N6IIrAAB9BP0rYr61ff7mMd2+9wAAMIQ++lfErIt/m7u5UV/fewAAOA9Bf4X5O5uuw5vetPi9nZ3JQE4AADgvQX+FdU+28/zzJ89ns7LMZmgZOoMOAACssrvpE7jq9vfvv/X8eRwcTCoNfYNtDbAFAGCdDMa9RLPpM4dMkQkAAEMYjHsF7O/rogMAwOXQog8AAFtMi/5FWPeUPAAAsCaC/nmse0oeAABYE0H/PHS6BwDgihL0z+PwcDJ9znRuTD15AAC4KgT9NXqgJ4/kDwDAhgj6S8xy+hNPDMvrD/Tk0YcfAIANMb3mErMbXM2c+kZXBwcnt9V161sAAC6A6TXPYNZCf/Pm6jG3vb10On34AQDgsmjRX5NZ6/+pW/0BAOActOhfsPv65xuECwDAhmnRvwia9wEAuCRa9C/JwUGy+8q/yEEO3UgLAICNEfQHGtob5+goudd2crRzyyBcAAA2RtAfaOiU+A/MpQ8AABsg6A+0KsDPWvwTM2oCALB5BuOuifG3AABsgsG4F0yXHQAArhIt+gAAsMW06AMAwDUi6AMAwAgNCvpV9Y6q+nxVvVBVH+h5/5ur6n+tql+oqs9V1XvWf6rbbeg8/AAAsA4r++hX1U6SLyT5niR3knwmybtba780t80fT/LNrbUfqaqHk3w+yb/aWvvqov1etz76ZuUBAOAinKeP/tuSvNBa++I0uH80yZOdbVqSb6qqSvKNSV5OIs7OMSsPAACXaUjQfyTJl+de35mum/dnk7wpyVeSPJfkj7bWXlnLGY7E4aEbaQEAcHmGBP3qWdft7/O9ST6b5Lck+R1J/mxV/eYHdlT1VFUdV9XxSy+9dOqTBQAAhhkS9O8kef3c60czabmf954kn2gTLyT55ST/RndHrbUPt9b2Wmt7Dz/88FnP+cox0BYAgKtmSND/TJI3VtXjVfWqJO9K8kxnmy8leXuSVNW3Jvm2JF9c54leBYsC/dHRZKDt0dFmzgsAALp2V23QWrtbVe9P8qkkO0k+0lr7XFW9d/r+h5L810l+oqqey6Srz4+01n7tAs97I2aB/umnJ68PDyeh/969pMpAWwAAro6V02telG2cXvPg4CTkz6bJNG0mAACbdJ7pNZk6PExu3pw8/+ZvnrTia80HAOAqEvRP6fnnJ48vv3yy7sYN02YCAHC1CPqn1G2515oPAMBVtHIwLvebtdwfHU0CvpZ8AACuIi36SyyaTtNdbgEAuOoE/SXMjw8AwLYS9JfY359Mm6kPPgAA28Y8+gAAsMXMow8AANeIoA8AACMk6AMAwAgJ+hdg0bScAABwWQT9C2BaTgAANk3QvwCm5QQAYNNMrwkAAFvM9JoAAHCNCPoAADBCgv5FMwUPAAAbIOhftKOjHNz7sew+/T/K+gAAXBpB/6Lt7+co78297JpuEwCASyPoX7TDw+zf2jXdJgAAl8r0mgAAsMVMrwkAANeIoA8AACMk6A+xbIpM02cCAHAF6aM/xO5ucu9esrOT3L07/D0AALhg+uifx/5+Fk6bs+w9AADYEC36AACwxbToAwDANSLoAwDACAn6AAAwQoI+AACMkKAPAAAjJOgDAMAICfoAADBCgj4AAIyQoA8AACMk6AMAwAgJ+sscHCS7u5NHAADYIoL+MkdHyb17k0cAANgigv4y+/vJzs7kEQAAtki11jZy4L29vXZ8fLyRYwMAwFhU1bOttb3uei36AAAwQoL+BTOeFwCATRD0L5jxvAAAbIKgv26dJnzjeQEA2ASDcddtd3fShL+zk9y9u+mzAQBg5AzGvSzTJvyDN/1v+uYDALAxWvQviIZ9AAAugxb9S6ZvPgAAmzQo6FfVO6rq81X1QlV9YME2311Vn62qz1XV/7ne09w+h4eTlvzDw02fCQAA19HKoF9VO0kOk7wzyZuTvLuq3tzZ5tVJnk7y/a21b0/yAxdwrlvHHPoAAGzKkBb9tyV5obX2xdbaV5N8NMmTnW3+4ySfaK19KUlaay+u9zS3kzn0AQDYlCFB/5EkX557fWe6bt5vS/ItVfU3q+rZqvqhdZ3gNtNPHwCATdkdsE31rOtO1bOb5K1J3p7k65L87ar62dbaF+7bUdVTSZ5Kkje84Q2nP9stc3iojz4AAJsxpEX/TpLXz71+NMlXerb5mdbaP2ut/VqSTyf57d0dtdY+3Frba63tPfzww2c9ZwAAYIUhQf8zSd5YVY9X1auSvCvJM51t/mqSf6eqdqvq65P8riTPr/dUt4/BuAAAbMrKoN9au5vk/Uk+lUl4/1hr7XNV9d6qeu90m+eT/EySX0zyc0l+vLV2++JOezsYjAsAwKYM6aOf1tonk3yys+5Dndd/OsmfXt+pbb/9/UnINxgXAIDLVq11x9Vejr29vXZ8fLyRYwMAwFhU1bOttb3u+kF3xgUAALaLoH8ZjMoFAOCSCfqXwahcAAAumaB/GdwiFwCAS2YwLgAAbDGDcQEA4BoR9AEAYIQEfQAAGCFBHwAARkjQBwCAERL0AQBghAR9AAAYIUF/gYODZHd38ggAANtG0F/g6Ci5d2/yCAAA20bQX2B/P9nZmTwCAMC2EfQXODxM7t6dPK6knw8AAFeMoL8O+vkAAHDFCPqrDGmt188HAIArplprGznw3t5eOz4+3sixT2V3d9Jav7Mz6csDAABXSFU921rb667Xor+K1noAALaQFn0AANhiWvQBAOAaEfQBAGCEBH0AABghQR8AAEZI0AcAgBES9AEAYIQEfQAAGCFBHwAARkjQBwCAERL0AQBghAR9AAAYIUEfAABGSNAHAIAREvQBAGCEBP3LcnCQ7O5OHgEA4IIJ+susM5wfHSX37k0eAQDgggn6Pf5lvn/629cXzvf3k52dySMAAFywaq1t5MB7e3vt+Ph4I8deZXd3ku936l7u3vjaSTg/PNz0aQEAwAOq6tnW2l53/bVt0T84SKr6l3v3Jo/779tJ7t4V8gEA2DrXNuiv6o1z44Z8DwDA9rq2QX9ZV/kqXekBANhu+ugDAMAW00cfAACuEUEfAABGSNAHAIAREvQBAGCEBH0AABghQR8AAEZI0AcAgBEaFPSr6h1V9fmqeqGqPrBku3+rqu5V1e9f3ylukYODZHd38ggAABu0MuhX1U6SwyTvTPLmJO+uqjcv2O5PJfnUuk9yaxwdJffuTR4BAGCDhrTovy3JC621L7bWvprko0me7Nnuh5P8lSQvrvH8tsv+frKzM3kEAIANGhL0H0ny5bnXd6br/qWqeiTJ70vyoWU7qqqnquq4qo5feuml057r1Xd4mNy9O3kEAIANGhL0q2dd67z+sSQ/0lq7t2xHrbUPt9b2Wmt7Dz/88NBzBAAATml3wDZ3krx+7vWjSb7S2WYvyUerKklem+T7qupua+1/WctZAgAApzKkRf8zSd5YVY9X1auSvCvJM/MbtNYeb6091lp7LMnHk9y6diF/yIw7ZuUBAOCSVGvdXjg9G1V9Xybdc3aSfKS19ier6r1J0lr7UGfbn0jy0621jy/b597eXjs+Pj7reV89u7uTGXd2dib99M+6DQAAnEJVPdta2+uuHzSPfmvtk62139Za+9dba39yuu5D3ZA/Xf+HVoX8URoy445ZeQAAuCSDWvQvwuha9AEAYAPO1aIPAABsF0H/qjs4SKqSGzcM4gUAYDBB/6o7Opo8tnbyHAAAVhD0r7rZwN0qg3gBABjMYFwAANhiBuNuo/kbbLnZFgAApyDoX7ZZYH/iidXB/ehocoOto6P7nwMAwAqC/mWbBfbbtyePTz+9OOzP32DLzbYAADgFffQv2xNPTEJ+161byeHh5Z8PAABbTR/9q+L550+e37x58lyXHAAA1kjQv2yzLji3biXPPTd5TJJXXjHQFgCAtdF15yrY3Z3019/ZSe7ePVl/cDBp6d/f160HAIBeuu5cZYsG2nZn2jHFJgAAAwn663KeEH54OAn5R0f3f75bATDFJgAAA+m6sy6Lut+s8/O68gAA0KHrzkU77zz3Qz5/eDipBAj5AACsIOivy7pDuP74AACcg647V0W36855uwIBAHAt6Lpz1c267Mzm0z9vVyAAAK41LfpXyawVPzkJ+frjAwCwhBb9bTDfer9oGk199wEAGEDQv0oODyct+TN93XbMpQ8AwACC/lUz65t/61Z/tx199wEAGEAffQAA2GL66AMAwDUi6AMAwAgJ+gAAMEKC/rYxvSYAAAMI+tvG9JoAAAwg6G8b02sCADCA6TUBAGCLmV4TAACuEUEfAABGSNDfRmbeAQBgBUF/G5l5BwCAFQT9bWTmHQAAVtjd9AlwBoeHk8dZi/7sNQAATGnR31ZDuu888URSNXkEAOBaEfS31ZDuO7dvnzwauAsAcK0I+tvq8HAS8o+O+kN8d52BuwAA14qgv81m3Xeefnp5sDdwFwDg2hH0t9l8eJ8P9gcHkwpAVXLr1vKWfwAARqlaaxs58N7eXjs+Pt7IsUfl4GDSol+VvO99ky49u7uToL+zk9y9++BrAABGo6qeba3tdddr0d92s6k1WzsJ/LPW/FmL/+zxlVe06gMAXBNa9Mfgxo1J0J/Xbb3Xqg8AMEpa9Mfsfe+7//V8a/6Mu+kCAFwrgv4YHB5OWvRnyyuvPHi33FXTcSaT9bu7uvcAAIyAoH+dLJuOczaod/b+jRsCPwDAFhP0r5Nl03E+/fT9284G9y5r/a+aLKetFMx/9iyfBwBgpUFBv6reUVWfr6oXquoDPe//J1X1i9Plb1XVb1//qXJuh4fJzZuT5/fuTcJ1N+TP3p9Z1vo/Mz/jz2zbbpifX05TqZjtR2UAAOBUVs66U1U7Sb6Q5HuS3EnymSTvbq390tw2vyfJ8621f1xV70zyX7bWftey/Zp1Z0Nms+8sMvt96AvzSfLEE8nt2xd3fqvM3y9g5uBgcoVif//BsQkAACN3nll33pbkhdbaF1trX03y0SRPzm/QWvtbrbV/PH35s0kePe8Jc0EWzbozu4vuzOHhZN38+1X3h/xbtyYVgO5VgCFu3jwZPDx/nFVau7/bUZJ88IOLxx4AAFxTQ4L+I0m+PPf6znTdIn84yV/re6Oqnqqq46o6fumll4afJeszm6FnFupnAb9vpp7utJ3zbt482f655+7f58xs3/MzAs2W554bdpxkso/5fd+7N7myMOvWM39Vahb2zzOGAABgBIZ03fmBJN/bWvsj09c/mORtrbUf7tn29yZ5Osm/3Vr79WX71XVnS3S78PR1nbmo4/Z1xzlN6/8yN28m3/VdD44XuHVL9x8AYKss6rozJOj/7kz63H/v9PWPJklr7b/pbPdvJvmpJO9srX1h1QkJ+pxJ3xiBWeUjeTC4r8tlVXAAAE7pPH30P5PkjVX1eFW9Ksm7kjzT2fkbknwiyQ8OCflwZs89t7jb0eHh/V18Zu93uxSdxfzMQn2L7kEAwBWzMui31u4meX+STyV5PsnHWmufq6r3VtV7p5v9F0lek+TpqvpsVWmq5+LMxhksugNw9w7B3TsHdysK3fVnsawicOPG5EqECgEAcIlWdt25KLrusFX6bip2kfq6Cs26Ld28ef9gZgDgWjtP1x2ge1Wg7wrBOvVdIZiNTbh9+/6rBFWT58mkQrK7e/9VAzcdA4BrSdCH81pWCZhVBHZ2zna/gUVau39Q8u3bJ3cdnt1ToHsn4lnlYb6CcBEVgL7KBgBw6QR9uGiHh8nduyf3Gxiy9FUKqtZTWejOWrRqoHFfpWAW5rtjD2ZdnOZvYDYf/IdUArrbLPvMoisYi443ZN99V0CGfIf5zz3xxPkqOypLAKxDa20jy1vf+tYGnNGtW63t7Eweb906qSJUtXbz5v3VhqrJNlVDqxlnW3Z2JsvQ7W/dOjnXmzdPvld3m+7refPH65ZF93y65dJ9PTNfTrMyXvR9538Ofd99Z+f+n9fsmLOfSffn2N3X7POLfvZj1vc9r8J3vwrnANCR5Lj15G1BH66LWWCdhcz5detcqlp76KH17GdZxaF7nG6wX9c5DK0g9ZVlX8Vr/nyHHL8bdLuViNn6ZaG4r6IxK5+bN4eF18sIuIsqO7PXs5/Fou8+/z2XrTuPvkrYecpmaIVmUaXwMqjcwJUn6APns+jKQTcM97Xsn+dqwrKwfJZlaAVg2Tn3vXeRV076znn+qsB597/oysd8RaNvm0VXJha91/d7NL/NfJBfdLWkG3bnP9P9vVu07iy/+8sqDt0rS93PLNtn33detW5ZeXSPt45KSF8FC7hSBH3g4vSFu2WvVwXLbpBpbXnYXxauFwXFvq5PiwL0qs/0BeZloXrRdvPfY1mF5LyViXVddTnt0vcz7F656V5xWtRC3y2D+e/00EPLf7593duGrJsPut0rDt3P9nUBW1V5GbpuSCVn0d/SqsA/pII1Xz4X1dK/7isxMHKCPnB1DGnt7Ia+ZfsY2oq8bH9nOc++fS/qYrHofLshthususF4UUWjL0QvGq+xKCxflWVZSO0G2ln5dQP5osC6jvNa9HMf8vnZ8/muU8t+35b9DnZ/j7rHW3WVoC9E93WZ6nt/WXmf5u9nkUU/00V/N+uqDCz6m4UrTtAH2AZ9lZRFXTLmw+4iQ7qzrLoSsyxUz7afD7Hdrl1DKytDKy9DgmtfYJ29f9YW/WXhef5n0ldRWbQsOv9l64a06i8Lw8sqRota0Off79t/3+9U31WXbrevRb/7fcfr+12a/96LKhyn+bvrK8/z7Pe0x1/3/lVWrg1BH+A6WhUYz7q/ZSGt7zN94W5VxaKvUtK90tG9StEN80Naf1eVUTdA9wXYZeewqPKwrELQPUY3eC7r0jP7bt1tlp3X/PH6fobzx+77eQ+puPUtq7pIdb/Hop9p93ucpvKy6OrRqv2u8wrCkP2f5riLKqOMlqAPwHZY1LK7rEX6LOuG7K+vBb8vIJ/nHIYMNj9Ld5Wzhu++Y3bD8qpy7KvEDB1Yv6jitOwq16pjz1diVoXrZZW8vjI9z3iFIb+Pq8ZbrCqPZVeEzlvx73IFYWMEfQDG4bRdXZYN5l3U8rus5X9ZC/BZz2FoBWRVK3XX/D7OM3tVt0W4r/IzdFrTIRWQbmVr0Xfqtr7Pbz+0orOoG9LsdXe/q2bkmr8SMD9AvFvZWVRB6DvvRRXOVb+Li8J/XzkvqkCcphLTV0Fe9DfEWgn6ADBz3sCxjsCyqlV16NWMoRWMRfvsrlt0v4VF32HR+SwKn4v6wfe1/veV8aLAO/89FgXtIRWM7s+i+3PqBvbzLn1XTebLtW8QeF8FY1l5LLpy1Pdz6atAzMpm/iaHp73ytqwL1Kp9Da2MX2OCPgBsu6Hhf1Fr+Gn1tdAuqjycd/7/vs+sKotVlYxFLdeLwvqqmb6GHu88S9/37gvjiwYoz7/fd77z37NbOerbx7KB9X3LrPyWVYQWDew/TRmt+t08beXgNFcwVlXSN0DQB4AxusjWzkXdlfoC6aquGkPPaWjr7qLKQbfVfdXxl1VUFnV9WdT1az5EL6rYdM9t1TS4q64crArg8/eV6H7vIaF6VYVi6LKu/Vy15YoMeBb0AYCzO0uf7fMaerVi3V2phl59GNrqO/TYywZ3J4srN31dalZ1k+kL3VWna2lfNYNTt+IyNDgP2e9VWk57j5YLIOgDANtlU32zT9uNaN3HXXQ1Y9G2q/Y1XzkaWonpXqEYWtGbv2qxrOKy6AZ+fdvOX6lZVSEYek+MRRWXIVdRusuGW/NbWxz0a/Le5dvb22vHx8cbOTYAwEoHB8nRUbK/nxwebvpszm7+eySX951Oc9y+bd/0puT5569W+W+qLFeoqmdba3sPrBf0AQBgey0K+jc2cTIAAMDFEvQBAGCEBH0AABghQR8AAEZI0AcAgBES9AEAYIQEfQAAGCFBHwAARkjQBwCAERL0AQBghAR9AAAYIUEfAABGSNAHAIAREvQBAGCEBH0AABghQR8AAEZI0AcAgBES9AEAYIQEfQAAGCFBHwAARkjQBwCAERL0AQBghAR9AAAYIUEfAABGSNAHAIAREvQBAGCEBH0AABghQR8AAEZI0AcAgBES9AEAYIQEfQAAGKFBQb+q3lFVn6+qF6rqAz3vV1X9men7v1hVb1n/qcDLZiEAAAacSURBVAIAAEOtDPpVtZPkMMk7k7w5ybur6s2dzd6Z5I3T5akkH1zzeQIAAKcwpEX/bUleaK19sbX21SQfTfJkZ5snk/z5NvGzSV5dVa9b87kCAAADDQn6jyT58tzrO9N1p90GAAC4JLsDtqmede0M26Sqnsqka0+S/NOq+vyA41+k1yb5tQ2fw5goz/VSnuunTNdLea6X8lwv5bleynO91l2e/1rfyiFB/06S18+9fjTJV86wTVprH07y4QHHvBRVddxa29v0eYyF8lwv5bl+ynS9lOd6Kc/1Up7rpTzX67LKc0jXnc8keWNVPV5Vr0ryriTPdLZ5JskPTWff+Y4kv9Fa+9U1nysAADDQyhb91trdqnp/kk8l2Unykdba56rqvdP3P5Tkk0m+L8kLSf55kvdc3CkDAACrDOm6k9baJzMJ8/PrPjT3vCU5WO+pXYor041oJJTneinP9VOm66U810t5rpfyXC/luV6XUp41yegAAMCYDLozLgAAsF2ubdCvqndU1eer6oWq+sCmz2cbVNVHqurFqro9t+6hqvobVfV/TR+/Ze69H52W7+er6ns3c9ZXV1W9vqr+j6p6vqo+V1V/dLpemZ5BVf2mqvq5qvqFaXn+V9P1yvMcqmqnqv5uVf309LXyPKOq+pWqeq6qPltVx9N1yvOMqurVVfXxqvp7039Hf7fyPJuq+rbp7+Vs+SdV9ceU59lV1X86/b/odlX95PT/qMsvz9batVsyGVT895P81iSvSvILSd686fO66kuS70ryliS359b9d0k+MH3+gSR/avr8zdNy/dokj0/Le2fT3+EqLUlel+Qt0+fflOQL03JTpmcrz0ryjdPnX5Pk7yT5DuV57nL9z5L8pSQ/PX2tPM9elr+S5LWddcrz7OX555L8kenzVyV5tfJcS7nuJPmHmczLrjzPVoaPJPnlJF83ff2xJH9oE+V5XVv035bkhdbaF1trX03y0SRPbvicrrzW2qeTvNxZ/WQm/9hm+vgfza3/aGvtX7TWfjmTGZnediknuiVaa7/aWvv56fP/N8nzmfzjoEzPoE380+nLr5kuLcrzzKrq0ST/QZIfn1utPNdLeZ5BVf3mTBqf/qckaa19tbX2/0R5rsPbk/z91to/iPI8j90kX1dVu0m+PpP7S116eV7XoP9Iki/Pvb4zXcfpfWub3jNh+vivTNcr41OoqseS/M5MWqGV6RlNu5l8NsmLSf5Ga015ns+PJfnPk7wyt055nl1L8ter6tma3Ck+UZ5n9VuTvJTkf552LfvxqvqGKM91eFeSn5w+V55n0Fr7v5P890m+lORXM7m/1F/PBsrzugb96lln+qH1UsYDVdU3JvkrSf5Ya+2fLNu0Z50yndNau9da+x2Z3J37bVV1c8nmynOJqvoPk7zYWnt26Ed61inP+31na+0tSd6Z5KCqvmvJtspzud1MupJ+sLX2O5P8s0y6QiyiPAeoyY1Rvz/JX161ac865Tk17Xv/ZCbdcH5Lkm+oqj+w7CM969ZSntc16N9J8vq5149mckmF0/tHVfW6JJk+vjhdr4wHqKqvySTk/8XW2iemq5XpOU0v4f/NJO+I8jyr70zy/VX1K5l0b/x3q+ovRHmeWWvtK9PHF5P8VCaX5pXn2dxJcmd61S5JPp5J8Fee5/POJD/fWvtH09fK82z+vSS/3Fp7qbX2/yX5RJLfkw2U53UN+p9J8saqenxae31Xkmc2fE7b6pkkf3D6/A8m+atz699VVV9bVY8neWOSn9vA+V1ZVVWZ9C99vrX2P8y9pUzPoKoerqpXT59/XSb/0P69KM8zaa39aGvt0dbaY5n8G/m/t9b+QJTnmVTVN1TVN82eJ/n3k9yO8jyT1to/TPLlqvq26aq3J/mlKM/zendOuu0kyvOsvpTkO6rq66f/1789k3F4l16eg+6MOzattbtV9f4kn8pkdPlHWmuf2/BpXXlV9ZNJvjvJa6vqTpI/keS/TfKxqvrDmfxi/0CStNY+V1Ufy+Qf3rtJDlpr9zZy4lfXdyb5wSTPTfuVJ8kfjzI9q9cl+XNVtZNJI8bHWms/XVV/O8pznfx+ns23Jvmpyf/52U3yl1prP1NVn4nyPKsfTvIXpw12X0zynkz/9pXn6VXV1yf5niT7c6v9vZ9Ba+3vVNXHk/x8JuXzdzO5E+435pLL051xAQBghK5r1x0AABg1QR8AAEZI0AcAgBES9AEAYIQEfQAAGCFBHwAARkjQBwCAERL0AQBghP5/+409xUtHEQMAAAAASUVORK5CYII=\n",
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
      "text/plain": [
       "<Figure size 936x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_len = np.arange(len(y_acc))\n",
    "plt.figure(figsize=(13,6))\n",
    "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=2)\n",
    "plt.plot(x_len, y_acc, \"o\", c=\"blue\", markersize=2)\n",
    "plt.ylim(0.0, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 과제 2번"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 16,
=======
   "execution_count": 4,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
=======
   "execution_count": 4,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "X= digits.data\n",
    "Y= digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#random seed = 2020\n",
    "#model 구현할때 softmax\n",
    "#train_test_split - test_size= 0.2\n",
    "#model.fit 할때 validation_split = 0.2"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 17,
=======
   "execution_count": 5,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
=======
   "execution_count": 5,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2020\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 18,
=======
   "execution_count": 6,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
=======
   "execution_count": 6,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, \n",
    "                                                    random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 19,
=======
   "execution_count": 7,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
=======
   "execution_count": 7,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
<<<<<<< HEAD
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                170       \n",
=======
=======
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                170       \n",
<<<<<<< HEAD
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
=======
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
      "=================================================================\n",
      "Total params: 1,210\n",
      "Trainable params: 1,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(16, input_shape=(64,), activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 20,
=======
   "execution_count": 9,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
=======
   "execution_count": 9,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 27,
=======
   "execution_count": 10,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
=======
   "execution_count": 10,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1437 samples, validate on 360 samples\n",
<<<<<<< HEAD
<<<<<<< HEAD
      "Epoch 1/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 2.0822e-08 - accuracy: 1.0000 - val_loss: 1.1711 - val_accuracy: 0.9167\n",
      "Epoch 2/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 2.0324e-08 - accuracy: 1.0000 - val_loss: 1.1731 - val_accuracy: 0.9167\n",
      "Epoch 3/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 1.9163e-08 - accuracy: 1.0000 - val_loss: 1.1747 - val_accuracy: 0.9167\n",
      "Epoch 4/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.8831e-08 - accuracy: 1.0000 - val_loss: 1.1770 - val_accuracy: 0.9167\n",
      "Epoch 5/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.8748e-08 - accuracy: 1.0000 - val_loss: 1.1732 - val_accuracy: 0.9167\n",
      "Epoch 6/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.8168e-08 - accuracy: 1.0000 - val_loss: 1.1734 - val_accuracy: 0.9167\n",
      "Epoch 7/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.7836e-08 - accuracy: 1.0000 - val_loss: 1.1768 - val_accuracy: 0.9167\n",
      "Epoch 8/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 1.6508e-08 - accuracy: 1.0000 - val_loss: 1.1749 - val_accuracy: 0.9167\n",
      "Epoch 9/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.7255e-08 - accuracy: 1.0000 - val_loss: 1.1858 - val_accuracy: 0.9167\n",
      "Epoch 10/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 1.6923e-08 - accuracy: 1.0000 - val_loss: 1.1769 - val_accuracy: 0.9167\n",
      "Epoch 11/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 1.5430e-08 - accuracy: 1.0000 - val_loss: 1.1778 - val_accuracy: 0.9167\n",
      "Epoch 12/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 1.6591e-08 - accuracy: 1.0000 - val_loss: 1.1852 - val_accuracy: 0.9167\n",
      "Epoch 13/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 1.5596e-08 - accuracy: 1.0000 - val_loss: 1.1822 - val_accuracy: 0.9167\n",
      "Epoch 14/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 1.4186e-08 - accuracy: 1.0000 - val_loss: 1.1852 - val_accuracy: 0.9167\n",
      "Epoch 15/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 1.5015e-08 - accuracy: 1.0000 - val_loss: 1.1909 - val_accuracy: 0.9167\n",
      "Epoch 16/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.3605e-08 - accuracy: 1.0000 - val_loss: 1.1857 - val_accuracy: 0.9167\n",
      "Epoch 17/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 1.3439e-08 - accuracy: 1.0000 - val_loss: 1.1837 - val_accuracy: 0.9167\n",
      "Epoch 18/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 1.3605e-08 - accuracy: 1.0000 - val_loss: 1.1932 - val_accuracy: 0.9167\n",
      "Epoch 19/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.2692e-08 - accuracy: 1.0000 - val_loss: 1.1956 - val_accuracy: 0.9167\n",
      "Epoch 20/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 1.3522e-08 - accuracy: 1.0000 - val_loss: 1.1916 - val_accuracy: 0.9167\n",
      "Epoch 21/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 1.2527e-08 - accuracy: 1.0000 - val_loss: 1.2006 - val_accuracy: 0.9167\n",
      "Epoch 22/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 1.2278e-08 - accuracy: 1.0000 - val_loss: 1.1964 - val_accuracy: 0.9167\n",
      "Epoch 23/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 1.1780e-08 - accuracy: 1.0000 - val_loss: 1.1956 - val_accuracy: 0.9167\n",
      "Epoch 24/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.1282e-08 - accuracy: 1.0000 - val_loss: 1.1981 - val_accuracy: 0.9167\n",
      "Epoch 25/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.1365e-08 - accuracy: 1.0000 - val_loss: 1.1983 - val_accuracy: 0.9167\n",
      "Epoch 26/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 1.1863e-08 - accuracy: 1.0000 - val_loss: 1.1961 - val_accuracy: 0.9167\n",
      "Epoch 27/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 1.1448e-08 - accuracy: 1.0000 - val_loss: 1.1975 - val_accuracy: 0.9167\n",
      "Epoch 28/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.0287e-08 - accuracy: 1.0000 - val_loss: 1.2013 - val_accuracy: 0.9167\n",
      "Epoch 29/500\n",
      "1437/1437 [==============================] - 0s 31us/sample - loss: 1.0784e-08 - accuracy: 1.0000 - val_loss: 1.1985 - val_accuracy: 0.9167\n",
      "Epoch 30/500\n",
      "1437/1437 [==============================] - 0s 28us/sample - loss: 1.0701e-08 - accuracy: 1.0000 - val_loss: 1.2032 - val_accuracy: 0.9139\n",
      "Epoch 31/500\n",
      "1437/1437 [==============================] - 0s 31us/sample - loss: 1.0038e-08 - accuracy: 1.0000 - val_loss: 1.2007 - val_accuracy: 0.9167\n",
      "Epoch 32/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 9.7060e-09 - accuracy: 1.0000 - val_loss: 1.1981 - val_accuracy: 0.9167\n",
      "Epoch 33/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 9.2912e-09 - accuracy: 1.0000 - val_loss: 1.1966 - val_accuracy: 0.9167\n",
      "Epoch 34/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 8.6275e-09 - accuracy: 1.0000 - val_loss: 1.2070 - val_accuracy: 0.9139\n",
      "Epoch 35/500\n",
      "1437/1437 [==============================] - 0s 39us/sample - loss: 9.7889e-09 - accuracy: 1.0000 - val_loss: 1.2114 - val_accuracy: 0.9139\n",
      "Epoch 36/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 9.2082e-09 - accuracy: 1.0000 - val_loss: 1.2138 - val_accuracy: 0.9167\n",
      "Epoch 37/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 8.2127e-09 - accuracy: 1.0000 - val_loss: 1.2073 - val_accuracy: 0.9139\n",
      "Epoch 38/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 7.8809e-09 - accuracy: 1.0000 - val_loss: 1.2056 - val_accuracy: 0.9139\n",
      "Epoch 39/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 7.5491e-09 - accuracy: 1.0000 - val_loss: 1.2098 - val_accuracy: 0.9139\n",
      "Epoch 40/500\n",
      "1437/1437 [==============================] - 0s 33us/sample - loss: 8.0468e-09 - accuracy: 1.0000 - val_loss: 1.2034 - val_accuracy: 0.9139\n",
      "Epoch 41/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 8.4616e-09 - accuracy: 1.0000 - val_loss: 1.2165 - val_accuracy: 0.9139\n",
      "Epoch 42/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 8.1298e-09 - accuracy: 1.0000 - val_loss: 1.2095 - val_accuracy: 0.9139\n",
      "Epoch 43/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 7.5491e-09 - accuracy: 1.0000 - val_loss: 1.2116 - val_accuracy: 0.9139\n",
      "Epoch 44/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 7.5491e-09 - accuracy: 1.0000 - val_loss: 1.2109 - val_accuracy: 0.9139\n",
      "Epoch 45/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 6.9684e-09 - accuracy: 1.0000 - val_loss: 1.2060 - val_accuracy: 0.9139\n",
      "Epoch 46/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 7.3832e-09 - accuracy: 1.0000 - val_loss: 1.2171 - val_accuracy: 0.9139\n",
      "Epoch 47/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 7.1343e-09 - accuracy: 1.0000 - val_loss: 1.2150 - val_accuracy: 0.9139\n",
      "Epoch 48/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 6.8025e-09 - accuracy: 1.0000 - val_loss: 1.2222 - val_accuracy: 0.9139\n",
      "Epoch 49/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 6.8854e-09 - accuracy: 1.0000 - val_loss: 1.2123 - val_accuracy: 0.9139\n",
      "Epoch 50/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 6.7195e-09 - accuracy: 1.0000 - val_loss: 1.2209 - val_accuracy: 0.9139\n",
      "Epoch 51/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 7.0513e-09 - accuracy: 1.0000 - val_loss: 1.2237 - val_accuracy: 0.9139\n",
      "Epoch 52/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 5.8070e-09 - accuracy: 1.0000 - val_loss: 1.2225 - val_accuracy: 0.9139\n",
      "Epoch 53/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 6.8025e-09 - accuracy: 1.0000 - val_loss: 1.2293 - val_accuracy: 0.9139\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1437/1437 [==============================] - 0s 35us/sample - loss: 6.4706e-09 - accuracy: 1.0000 - val_loss: 1.2242 - val_accuracy: 0.9139\n",
      "Epoch 55/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 5.8070e-09 - accuracy: 1.0000 - val_loss: 1.2311 - val_accuracy: 0.9139\n",
      "Epoch 56/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 6.7195e-09 - accuracy: 1.0000 - val_loss: 1.2153 - val_accuracy: 0.9139\n",
      "Epoch 57/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 5.8070e-09 - accuracy: 1.0000 - val_loss: 1.2188 - val_accuracy: 0.9139\n",
      "Epoch 58/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 5.3922e-09 - accuracy: 1.0000 - val_loss: 1.2215 - val_accuracy: 0.9139\n",
      "Epoch 59/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 5.5581e-09 - accuracy: 1.0000 - val_loss: 1.2255 - val_accuracy: 0.9139\n",
      "Epoch 60/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 5.4752e-09 - accuracy: 1.0000 - val_loss: 1.2292 - val_accuracy: 0.9139\n",
      "Epoch 61/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 5.7240e-09 - accuracy: 1.0000 - val_loss: 1.2248 - val_accuracy: 0.9139\n",
      "Epoch 62/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 5.3093e-09 - accuracy: 1.0000 - val_loss: 1.2135 - val_accuracy: 0.9111\n",
      "Epoch 63/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 5.7240e-09 - accuracy: 1.0000 - val_loss: 1.2321 - val_accuracy: 0.9139\n",
      "Epoch 64/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 5.2263e-09 - accuracy: 1.0000 - val_loss: 1.2323 - val_accuracy: 0.9139\n",
      "Epoch 65/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 5.2263e-09 - accuracy: 1.0000 - val_loss: 1.2379 - val_accuracy: 0.9139\n",
      "Epoch 66/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 4.6456e-09 - accuracy: 1.0000 - val_loss: 1.2264 - val_accuracy: 0.9139\n",
      "Epoch 67/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 4.8115e-09 - accuracy: 1.0000 - val_loss: 1.2272 - val_accuracy: 0.9139\n",
      "Epoch 68/500\n",
      "1437/1437 [==============================] - 0s 39us/sample - loss: 5.2263e-09 - accuracy: 1.0000 - val_loss: 1.2364 - val_accuracy: 0.9111\n",
      "Epoch 69/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 4.5626e-09 - accuracy: 1.0000 - val_loss: 1.2180 - val_accuracy: 0.9139\n",
      "Epoch 70/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 4.7286e-09 - accuracy: 1.0000 - val_loss: 1.2319 - val_accuracy: 0.9139\n",
      "Epoch 71/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 4.8945e-09 - accuracy: 1.0000 - val_loss: 1.2316 - val_accuracy: 0.9111\n",
      "Epoch 72/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 4.5626e-09 - accuracy: 1.0000 - val_loss: 1.2444 - val_accuracy: 0.9111\n",
      "Epoch 73/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 4.6456e-09 - accuracy: 1.0000 - val_loss: 1.2377 - val_accuracy: 0.9139\n",
      "Epoch 74/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 4.5626e-09 - accuracy: 1.0000 - val_loss: 1.2263 - val_accuracy: 0.9139\n",
      "Epoch 75/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 4.6456e-09 - accuracy: 1.0000 - val_loss: 1.2353 - val_accuracy: 0.9111\n",
      "Epoch 76/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 4.7286e-09 - accuracy: 1.0000 - val_loss: 1.2358 - val_accuracy: 0.9111\n",
      "Epoch 77/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 4.2308e-09 - accuracy: 1.0000 - val_loss: 1.2384 - val_accuracy: 0.9139\n",
      "Epoch 78/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 4.3967e-09 - accuracy: 1.0000 - val_loss: 1.2396 - val_accuracy: 0.9111\n",
      "Epoch 79/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 4.4797e-09 - accuracy: 1.0000 - val_loss: 1.2324 - val_accuracy: 0.9139\n",
      "Epoch 80/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 4.6456e-09 - accuracy: 1.0000 - val_loss: 1.2472 - val_accuracy: 0.9111\n",
      "Epoch 81/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 3.6501e-09 - accuracy: 1.0000 - val_loss: 1.2423 - val_accuracy: 0.9111\n",
      "Epoch 82/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 4.3967e-09 - accuracy: 1.0000 - val_loss: 1.2458 - val_accuracy: 0.9111\n",
      "Epoch 83/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 4.0649e-09 - accuracy: 1.0000 - val_loss: 1.2283 - val_accuracy: 0.9139\n",
      "Epoch 84/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 5.2263e-09 - accuracy: 1.0000 - val_loss: 1.2368 - val_accuracy: 0.9139\n",
      "Epoch 85/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 3.7331e-09 - accuracy: 1.0000 - val_loss: 1.2422 - val_accuracy: 0.9139\n",
      "Epoch 86/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 4.3138e-09 - accuracy: 1.0000 - val_loss: 1.2535 - val_accuracy: 0.9111\n",
      "Epoch 87/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 3.7331e-09 - accuracy: 1.0000 - val_loss: 1.2465 - val_accuracy: 0.9139\n",
      "Epoch 88/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 3.5672e-09 - accuracy: 1.0000 - val_loss: 1.2457 - val_accuracy: 0.9139\n",
      "Epoch 89/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 3.8990e-09 - accuracy: 1.0000 - val_loss: 1.2448 - val_accuracy: 0.9139\n",
      "Epoch 90/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 3.7331e-09 - accuracy: 1.0000 - val_loss: 1.2433 - val_accuracy: 0.9139\n",
      "Epoch 91/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 3.9819e-09 - accuracy: 1.0000 - val_loss: 1.2518 - val_accuracy: 0.9139\n",
      "Epoch 92/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 3.6501e-09 - accuracy: 1.0000 - val_loss: 1.2459 - val_accuracy: 0.9139\n",
      "Epoch 93/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 3.5672e-09 - accuracy: 1.0000 - val_loss: 1.2546 - val_accuracy: 0.9139\n",
      "Epoch 94/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 4.3967e-09 - accuracy: 1.0000 - val_loss: 1.2596 - val_accuracy: 0.9139\n",
      "Epoch 95/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 3.4842e-09 - accuracy: 1.0000 - val_loss: 1.2616 - val_accuracy: 0.9167\n",
      "Epoch 96/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 3.8990e-09 - accuracy: 1.0000 - val_loss: 1.2452 - val_accuracy: 0.9139\n",
      "Epoch 97/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 3.5672e-09 - accuracy: 1.0000 - val_loss: 1.2511 - val_accuracy: 0.9139\n",
      "Epoch 98/500\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 3.7331e-09 - accuracy: 1.0000 - val_loss: 1.2552 - val_accuracy: 0.9139\n",
      "Epoch 99/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 3.4012e-09 - accuracy: 1.0000 - val_loss: 1.2445 - val_accuracy: 0.9139\n",
      "Epoch 100/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 4.0649e-09 - accuracy: 1.0000 - val_loss: 1.2466 - val_accuracy: 0.9139\n",
      "Epoch 101/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 3.4012e-09 - accuracy: 1.0000 - val_loss: 1.2588 - val_accuracy: 0.9167\n",
      "Epoch 102/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 3.1524e-09 - accuracy: 1.0000 - val_loss: 1.2550 - val_accuracy: 0.9139\n",
      "Epoch 103/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 3.4842e-09 - accuracy: 1.0000 - val_loss: 1.2566 - val_accuracy: 0.9167\n",
      "Epoch 104/500\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 3.2353e-09 - accuracy: 1.0000 - val_loss: 1.2526 - val_accuracy: 0.9139\n",
      "Epoch 105/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 3.5672e-09 - accuracy: 1.0000 - val_loss: 1.2600 - val_accuracy: 0.9167\n",
      "Epoch 106/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 3.4842e-09 - accuracy: 1.0000 - val_loss: 1.2488 - val_accuracy: 0.9139\n",
      "Epoch 107/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1437/1437 [==============================] - 0s 35us/sample - loss: 3.6501e-09 - accuracy: 1.0000 - val_loss: 1.2736 - val_accuracy: 0.9167\n",
      "Epoch 108/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 3.8990e-09 - accuracy: 1.0000 - val_loss: 1.2594 - val_accuracy: 0.9167\n",
      "Epoch 109/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 4.6456e-09 - accuracy: 1.0000 - val_loss: 1.2642 - val_accuracy: 0.9139\n",
      "Epoch 110/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 4.4797e-09 - accuracy: 1.0000 - val_loss: 1.2664 - val_accuracy: 0.9139\n",
      "Epoch 111/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 3.5672e-09 - accuracy: 1.0000 - val_loss: 1.2681 - val_accuracy: 0.9139\n",
      "Epoch 112/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 3.5672e-09 - accuracy: 1.0000 - val_loss: 1.2463 - val_accuracy: 0.9139\n",
      "Epoch 113/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 4.3967e-09 - accuracy: 1.0000 - val_loss: 1.2580 - val_accuracy: 0.9139\n",
      "Epoch 114/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 3.5672e-09 - accuracy: 1.0000 - val_loss: 1.2505 - val_accuracy: 0.9139\n",
      "Epoch 115/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 3.5672e-09 - accuracy: 1.0000 - val_loss: 1.2565 - val_accuracy: 0.9139\n",
      "Epoch 116/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 3.3183e-09 - accuracy: 1.0000 - val_loss: 1.2558 - val_accuracy: 0.9139\n",
      "Epoch 117/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 3.8160e-09 - accuracy: 1.0000 - val_loss: 1.2626 - val_accuracy: 0.9139\n",
      "Epoch 118/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 3.8160e-09 - accuracy: 1.0000 - val_loss: 1.2584 - val_accuracy: 0.9139\n",
      "Epoch 119/500\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0073 - accuracy: 0.9979 - val_loss: 1.2750 - val_accuracy: 0.9056\n",
      "Epoch 120/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 0.0423 - accuracy: 0.9937 - val_loss: 1.8642 - val_accuracy: 0.8750\n",
      "Epoch 121/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 0.0013 - accuracy: 0.9993 - val_loss: 1.4814 - val_accuracy: 0.9028\n",
      "Epoch 122/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 2.9463e-04 - accuracy: 1.0000 - val_loss: 1.5358 - val_accuracy: 0.9000\n",
      "Epoch 123/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 4.9884e-05 - accuracy: 1.0000 - val_loss: 1.5160 - val_accuracy: 0.9000\n",
      "Epoch 124/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 1.6990e-05 - accuracy: 1.0000 - val_loss: 1.5033 - val_accuracy: 0.9000\n",
      "Epoch 125/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.2862e-05 - accuracy: 1.0000 - val_loss: 1.4947 - val_accuracy: 0.9000\n",
      "Epoch 126/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.0205e-05 - accuracy: 1.0000 - val_loss: 1.4892 - val_accuracy: 0.9000\n",
      "Epoch 127/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 8.7318e-06 - accuracy: 1.0000 - val_loss: 1.4836 - val_accuracy: 0.9000\n",
      "Epoch 128/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 7.6304e-06 - accuracy: 1.0000 - val_loss: 1.4788 - val_accuracy: 0.9000\n",
      "Epoch 129/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 6.8252e-06 - accuracy: 1.0000 - val_loss: 1.4757 - val_accuracy: 0.9000\n",
      "Epoch 130/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 6.1738e-06 - accuracy: 1.0000 - val_loss: 1.4705 - val_accuracy: 0.9000\n",
      "Epoch 131/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 5.6144e-06 - accuracy: 1.0000 - val_loss: 1.4670 - val_accuracy: 0.9000\n",
      "Epoch 132/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 5.1275e-06 - accuracy: 1.0000 - val_loss: 1.4646 - val_accuracy: 0.9000\n",
      "Epoch 133/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 4.7338e-06 - accuracy: 1.0000 - val_loss: 1.4611 - val_accuracy: 0.9000\n",
      "Epoch 134/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 4.3777e-06 - accuracy: 1.0000 - val_loss: 1.4581 - val_accuracy: 0.9000\n",
      "Epoch 135/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 4.0651e-06 - accuracy: 1.0000 - val_loss: 1.4563 - val_accuracy: 0.9000\n",
      "Epoch 136/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 3.8041e-06 - accuracy: 1.0000 - val_loss: 1.4534 - val_accuracy: 0.9000\n",
      "Epoch 137/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 3.5786e-06 - accuracy: 1.0000 - val_loss: 1.4506 - val_accuracy: 0.9000\n",
      "Epoch 138/500\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 3.3652e-06 - accuracy: 1.0000 - val_loss: 1.4483 - val_accuracy: 0.9000\n",
      "Epoch 139/500\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 3.1681e-06 - accuracy: 1.0000 - val_loss: 1.4465 - val_accuracy: 0.9028\n",
      "Epoch 140/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 2.9966e-06 - accuracy: 1.0000 - val_loss: 1.4439 - val_accuracy: 0.9028\n",
      "Epoch 141/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 2.8303e-06 - accuracy: 1.0000 - val_loss: 1.4424 - val_accuracy: 0.9028\n",
      "Epoch 142/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 2.6995e-06 - accuracy: 1.0000 - val_loss: 1.4400 - val_accuracy: 0.9028\n",
      "Epoch 143/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 2.5493e-06 - accuracy: 1.0000 - val_loss: 1.4384 - val_accuracy: 0.9028\n",
      "Epoch 144/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 2.4314e-06 - accuracy: 1.0000 - val_loss: 1.4362 - val_accuracy: 0.9028\n",
      "Epoch 145/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 2.3432e-06 - accuracy: 1.0000 - val_loss: 1.4348 - val_accuracy: 0.9028\n",
      "Epoch 146/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 2.2000e-06 - accuracy: 1.0000 - val_loss: 1.4332 - val_accuracy: 0.9028\n",
      "Epoch 147/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 2.1115e-06 - accuracy: 1.0000 - val_loss: 1.4313 - val_accuracy: 0.9028\n",
      "Epoch 148/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 2.0132e-06 - accuracy: 1.0000 - val_loss: 1.4295 - val_accuracy: 0.9028\n",
      "Epoch 149/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 1.9302e-06 - accuracy: 1.0000 - val_loss: 1.4281 - val_accuracy: 0.9028\n",
      "Epoch 150/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 1.8436e-06 - accuracy: 1.0000 - val_loss: 1.4270 - val_accuracy: 0.9028\n",
      "Epoch 151/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 1.7966e-06 - accuracy: 1.0000 - val_loss: 1.4251 - val_accuracy: 0.9028\n",
      "Epoch 152/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 1.7059e-06 - accuracy: 1.0000 - val_loss: 1.4235 - val_accuracy: 0.9028\n",
      "Epoch 153/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 1.6312e-06 - accuracy: 1.0000 - val_loss: 1.4221 - val_accuracy: 0.9028\n",
      "Epoch 154/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 1.5710e-06 - accuracy: 1.0000 - val_loss: 1.4210 - val_accuracy: 0.9056\n",
      "Epoch 155/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 1.5077e-06 - accuracy: 1.0000 - val_loss: 1.4195 - val_accuracy: 0.9056\n",
      "Epoch 156/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 1.4558e-06 - accuracy: 1.0000 - val_loss: 1.4180 - val_accuracy: 0.9056\n",
      "Epoch 157/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 1.4026e-06 - accuracy: 1.0000 - val_loss: 1.4170 - val_accuracy: 0.9056\n",
      "Epoch 158/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.3538e-06 - accuracy: 1.0000 - val_loss: 1.4156 - val_accuracy: 0.9056\n",
      "Epoch 159/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 1.3078e-06 - accuracy: 1.0000 - val_loss: 1.4142 - val_accuracy: 0.9056\n",
      "Epoch 160/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1437/1437 [==============================] - 0s 36us/sample - loss: 1.2599e-06 - accuracy: 1.0000 - val_loss: 1.4131 - val_accuracy: 0.9056\n",
      "Epoch 161/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 1.2194e-06 - accuracy: 1.0000 - val_loss: 1.4118 - val_accuracy: 0.9083\n",
      "Epoch 162/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.1834e-06 - accuracy: 1.0000 - val_loss: 1.4108 - val_accuracy: 0.9083\n",
      "Epoch 163/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.1375e-06 - accuracy: 1.0000 - val_loss: 1.4093 - val_accuracy: 0.9083\n",
      "Epoch 164/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 1.0998e-06 - accuracy: 1.0000 - val_loss: 1.4083 - val_accuracy: 0.9111\n",
      "Epoch 165/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 1.0647e-06 - accuracy: 1.0000 - val_loss: 1.4072 - val_accuracy: 0.9111\n",
      "Epoch 166/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 1.0307e-06 - accuracy: 1.0000 - val_loss: 1.4061 - val_accuracy: 0.9111\n",
      "Epoch 167/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.0022e-06 - accuracy: 1.0000 - val_loss: 1.4050 - val_accuracy: 0.9111\n",
      "Epoch 168/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 9.6699e-07 - accuracy: 1.0000 - val_loss: 1.4040 - val_accuracy: 0.9111\n",
      "Epoch 169/500\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 9.3664e-07 - accuracy: 1.0000 - val_loss: 1.4025 - val_accuracy: 0.9111\n",
      "Epoch 170/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 9.0827e-07 - accuracy: 1.0000 - val_loss: 1.4014 - val_accuracy: 0.9111\n",
      "Epoch 171/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 8.8090e-07 - accuracy: 1.0000 - val_loss: 1.4005 - val_accuracy: 0.9111\n",
      "Epoch 172/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 8.5594e-07 - accuracy: 1.0000 - val_loss: 1.3994 - val_accuracy: 0.9111\n",
      "Epoch 173/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 8.2824e-07 - accuracy: 1.0000 - val_loss: 1.3988 - val_accuracy: 0.9111\n",
      "Epoch 174/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 8.0576e-07 - accuracy: 1.0000 - val_loss: 1.3975 - val_accuracy: 0.9111\n",
      "Epoch 175/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 7.8113e-07 - accuracy: 1.0000 - val_loss: 1.3966 - val_accuracy: 0.9111\n",
      "Epoch 176/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 7.5957e-07 - accuracy: 1.0000 - val_loss: 1.3956 - val_accuracy: 0.9111\n",
      "Epoch 177/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 7.3800e-07 - accuracy: 1.0000 - val_loss: 1.3948 - val_accuracy: 0.9111\n",
      "Epoch 178/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 7.1718e-07 - accuracy: 1.0000 - val_loss: 1.3937 - val_accuracy: 0.9111\n",
      "Epoch 179/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 6.9645e-07 - accuracy: 1.0000 - val_loss: 1.3930 - val_accuracy: 0.9111\n",
      "Epoch 180/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 6.7928e-07 - accuracy: 1.0000 - val_loss: 1.3919 - val_accuracy: 0.9139\n",
      "Epoch 181/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 6.5821e-07 - accuracy: 1.0000 - val_loss: 1.3910 - val_accuracy: 0.9139\n",
      "Epoch 182/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 6.4054e-07 - accuracy: 1.0000 - val_loss: 1.3903 - val_accuracy: 0.9139\n",
      "Epoch 183/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 6.2396e-07 - accuracy: 1.0000 - val_loss: 1.3890 - val_accuracy: 0.9139\n",
      "Epoch 184/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 6.0712e-07 - accuracy: 1.0000 - val_loss: 1.3883 - val_accuracy: 0.9139\n",
      "Epoch 185/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 5.9020e-07 - accuracy: 1.0000 - val_loss: 1.3874 - val_accuracy: 0.9139\n",
      "Epoch 186/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 5.7568e-07 - accuracy: 1.0000 - val_loss: 1.3863 - val_accuracy: 0.9139\n",
      "Epoch 187/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 5.5992e-07 - accuracy: 1.0000 - val_loss: 1.3855 - val_accuracy: 0.9139\n",
      "Epoch 188/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 5.4516e-07 - accuracy: 1.0000 - val_loss: 1.3847 - val_accuracy: 0.9139\n",
      "Epoch 189/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 5.3139e-07 - accuracy: 1.0000 - val_loss: 1.3837 - val_accuracy: 0.9139\n",
      "Epoch 190/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 5.1754e-07 - accuracy: 1.0000 - val_loss: 1.3828 - val_accuracy: 0.9139\n",
      "Epoch 191/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 5.0501e-07 - accuracy: 1.0000 - val_loss: 1.3822 - val_accuracy: 0.9139\n",
      "Epoch 192/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 4.9141e-07 - accuracy: 1.0000 - val_loss: 1.3816 - val_accuracy: 0.9139\n",
      "Epoch 193/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 4.8403e-07 - accuracy: 1.0000 - val_loss: 1.3807 - val_accuracy: 0.9139\n",
      "Epoch 194/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 4.6628e-07 - accuracy: 1.0000 - val_loss: 1.3793 - val_accuracy: 0.9139\n",
      "Epoch 195/500\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 4.5500e-07 - accuracy: 1.0000 - val_loss: 1.3785 - val_accuracy: 0.9139\n",
      "Epoch 196/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 4.4264e-07 - accuracy: 1.0000 - val_loss: 1.3783 - val_accuracy: 0.9139\n",
      "Epoch 197/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 4.3268e-07 - accuracy: 1.0000 - val_loss: 1.3773 - val_accuracy: 0.9167\n",
      "Epoch 198/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 4.2215e-07 - accuracy: 1.0000 - val_loss: 1.3765 - val_accuracy: 0.9167\n",
      "Epoch 199/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 4.1153e-07 - accuracy: 1.0000 - val_loss: 1.3758 - val_accuracy: 0.9167\n",
      "Epoch 200/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 4.0116e-07 - accuracy: 1.0000 - val_loss: 1.3749 - val_accuracy: 0.9167\n",
      "Epoch 201/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 3.9187e-07 - accuracy: 1.0000 - val_loss: 1.3743 - val_accuracy: 0.9167\n",
      "Epoch 202/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 3.8308e-07 - accuracy: 1.0000 - val_loss: 1.3736 - val_accuracy: 0.9167\n",
      "Epoch 203/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 3.7379e-07 - accuracy: 1.0000 - val_loss: 1.3732 - val_accuracy: 0.9167\n",
      "Epoch 204/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 3.6525e-07 - accuracy: 1.0000 - val_loss: 1.3722 - val_accuracy: 0.9167\n",
      "Epoch 205/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 3.5637e-07 - accuracy: 1.0000 - val_loss: 1.3713 - val_accuracy: 0.9167\n",
      "Epoch 206/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 3.4807e-07 - accuracy: 1.0000 - val_loss: 1.3708 - val_accuracy: 0.9167\n",
      "Epoch 207/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 3.3961e-07 - accuracy: 1.0000 - val_loss: 1.3700 - val_accuracy: 0.9167\n",
      "Epoch 208/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 3.3148e-07 - accuracy: 1.0000 - val_loss: 1.3695 - val_accuracy: 0.9167\n",
      "Epoch 209/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 3.2485e-07 - accuracy: 1.0000 - val_loss: 1.3688 - val_accuracy: 0.9167\n",
      "Epoch 210/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 3.1655e-07 - accuracy: 1.0000 - val_loss: 1.3679 - val_accuracy: 0.9167\n",
      "Epoch 211/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 3.0959e-07 - accuracy: 1.0000 - val_loss: 1.3673 - val_accuracy: 0.9167\n",
      "Epoch 212/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 3.0278e-07 - accuracy: 1.0000 - val_loss: 1.3666 - val_accuracy: 0.9167\n",
      "Epoch 213/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1437/1437 [==============================] - 0s 34us/sample - loss: 2.9582e-07 - accuracy: 1.0000 - val_loss: 1.3661 - val_accuracy: 0.9167\n",
      "Epoch 214/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 2.8918e-07 - accuracy: 1.0000 - val_loss: 1.3652 - val_accuracy: 0.9167\n",
      "Epoch 215/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 2.8221e-07 - accuracy: 1.0000 - val_loss: 1.3646 - val_accuracy: 0.9167\n",
      "Epoch 216/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 2.7649e-07 - accuracy: 1.0000 - val_loss: 1.3644 - val_accuracy: 0.9167\n",
      "Epoch 217/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 2.7027e-07 - accuracy: 1.0000 - val_loss: 1.3632 - val_accuracy: 0.9167\n",
      "Epoch 218/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 2.6429e-07 - accuracy: 1.0000 - val_loss: 1.3627 - val_accuracy: 0.9167\n",
      "Epoch 219/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 2.5899e-07 - accuracy: 1.0000 - val_loss: 1.3620 - val_accuracy: 0.9167\n",
      "Epoch 220/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 2.5310e-07 - accuracy: 1.0000 - val_loss: 1.3614 - val_accuracy: 0.9167\n",
      "Epoch 221/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 2.4762e-07 - accuracy: 1.0000 - val_loss: 1.3610 - val_accuracy: 0.9167\n",
      "Epoch 222/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 2.4256e-07 - accuracy: 1.0000 - val_loss: 1.3599 - val_accuracy: 0.9167\n",
      "Epoch 223/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 2.3700e-07 - accuracy: 1.0000 - val_loss: 1.3594 - val_accuracy: 0.9167\n",
      "Epoch 224/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 2.3178e-07 - accuracy: 1.0000 - val_loss: 1.3586 - val_accuracy: 0.9167\n",
      "Epoch 225/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 2.2680e-07 - accuracy: 1.0000 - val_loss: 1.3580 - val_accuracy: 0.9167\n",
      "Epoch 226/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 2.2191e-07 - accuracy: 1.0000 - val_loss: 1.3573 - val_accuracy: 0.9167\n",
      "Epoch 227/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 2.1751e-07 - accuracy: 1.0000 - val_loss: 1.3567 - val_accuracy: 0.9167\n",
      "Epoch 228/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 2.1220e-07 - accuracy: 1.0000 - val_loss: 1.3562 - val_accuracy: 0.9167\n",
      "Epoch 229/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 2.0805e-07 - accuracy: 1.0000 - val_loss: 1.3555 - val_accuracy: 0.9167\n",
      "Epoch 230/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 2.0366e-07 - accuracy: 1.0000 - val_loss: 1.3547 - val_accuracy: 0.9167\n",
      "Epoch 231/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 1.9943e-07 - accuracy: 1.0000 - val_loss: 1.3545 - val_accuracy: 0.9167\n",
      "Epoch 232/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.9528e-07 - accuracy: 1.0000 - val_loss: 1.3535 - val_accuracy: 0.9167\n",
      "Epoch 233/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 1.9163e-07 - accuracy: 1.0000 - val_loss: 1.3528 - val_accuracy: 0.9167\n",
      "Epoch 234/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.8731e-07 - accuracy: 1.0000 - val_loss: 1.3521 - val_accuracy: 0.9167\n",
      "Epoch 235/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 1.8308e-07 - accuracy: 1.0000 - val_loss: 1.3511 - val_accuracy: 0.9167\n",
      "Epoch 236/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.7968e-07 - accuracy: 1.0000 - val_loss: 1.3510 - val_accuracy: 0.9167\n",
      "Epoch 237/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 1.7612e-07 - accuracy: 1.0000 - val_loss: 1.3506 - val_accuracy: 0.9167\n",
      "Epoch 238/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 1.7222e-07 - accuracy: 1.0000 - val_loss: 1.3498 - val_accuracy: 0.9167\n",
      "Epoch 239/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.6940e-07 - accuracy: 1.0000 - val_loss: 1.3491 - val_accuracy: 0.9167\n",
      "Epoch 240/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 1.6550e-07 - accuracy: 1.0000 - val_loss: 1.3485 - val_accuracy: 0.9167\n",
      "Epoch 241/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.6210e-07 - accuracy: 1.0000 - val_loss: 1.3477 - val_accuracy: 0.9167\n",
      "Epoch 242/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.5903e-07 - accuracy: 1.0000 - val_loss: 1.3474 - val_accuracy: 0.9167\n",
      "Epoch 243/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 1.5604e-07 - accuracy: 1.0000 - val_loss: 1.3468 - val_accuracy: 0.9167\n",
      "Epoch 244/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 1.5289e-07 - accuracy: 1.0000 - val_loss: 1.3462 - val_accuracy: 0.9167\n",
      "Epoch 245/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 1.4974e-07 - accuracy: 1.0000 - val_loss: 1.3457 - val_accuracy: 0.9167\n",
      "Epoch 246/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 1.4708e-07 - accuracy: 1.0000 - val_loss: 1.3450 - val_accuracy: 0.9167\n",
      "Epoch 247/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.4443e-07 - accuracy: 1.0000 - val_loss: 1.3448 - val_accuracy: 0.9167\n",
      "Epoch 248/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 1.4161e-07 - accuracy: 1.0000 - val_loss: 1.3438 - val_accuracy: 0.9167\n",
      "Epoch 249/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 1.3862e-07 - accuracy: 1.0000 - val_loss: 1.3431 - val_accuracy: 0.9167\n",
      "Epoch 250/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 1.3655e-07 - accuracy: 1.0000 - val_loss: 1.3425 - val_accuracy: 0.9167\n",
      "Epoch 251/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.3348e-07 - accuracy: 1.0000 - val_loss: 1.3423 - val_accuracy: 0.9167\n",
      "Epoch 252/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 1.3074e-07 - accuracy: 1.0000 - val_loss: 1.3416 - val_accuracy: 0.9167\n",
      "Epoch 253/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.2817e-07 - accuracy: 1.0000 - val_loss: 1.3411 - val_accuracy: 0.9167\n",
      "Epoch 254/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.2576e-07 - accuracy: 1.0000 - val_loss: 1.3407 - val_accuracy: 0.9167\n",
      "Epoch 255/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.2319e-07 - accuracy: 1.0000 - val_loss: 1.3398 - val_accuracy: 0.9167\n",
      "Epoch 256/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 1.2103e-07 - accuracy: 1.0000 - val_loss: 1.3398 - val_accuracy: 0.9167\n",
      "Epoch 257/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.1871e-07 - accuracy: 1.0000 - val_loss: 1.3392 - val_accuracy: 0.9167\n",
      "Epoch 258/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.1672e-07 - accuracy: 1.0000 - val_loss: 1.3389 - val_accuracy: 0.9167\n",
      "Epoch 259/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.1456e-07 - accuracy: 1.0000 - val_loss: 1.3384 - val_accuracy: 0.9167\n",
      "Epoch 260/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.1257e-07 - accuracy: 1.0000 - val_loss: 1.3380 - val_accuracy: 0.9167\n",
      "Epoch 261/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 1.1033e-07 - accuracy: 1.0000 - val_loss: 1.3373 - val_accuracy: 0.9167\n",
      "Epoch 262/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 1.0851e-07 - accuracy: 1.0000 - val_loss: 1.3370 - val_accuracy: 0.9167\n",
      "Epoch 263/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.0660e-07 - accuracy: 1.0000 - val_loss: 1.3365 - val_accuracy: 0.9167\n",
      "Epoch 264/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.0419e-07 - accuracy: 1.0000 - val_loss: 1.3360 - val_accuracy: 0.9167\n",
      "Epoch 265/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.0253e-07 - accuracy: 1.0000 - val_loss: 1.3361 - val_accuracy: 0.9167\n",
      "Epoch 266/500\n"
=======
=======
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
      "Epoch 1/5000\n",
      "1437/1437 [==============================] - 0s 222us/sample - loss: 5.4885 - accuracy: 0.2109 - val_loss: 3.1083 - val_accuracy: 0.2806\n",
      "Epoch 2/5000\n",
      "1437/1437 [==============================] - 0s 39us/sample - loss: 2.2854 - accuracy: 0.2756 - val_loss: 2.1621 - val_accuracy: 0.2639\n",
      "Epoch 3/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 1.7680 - accuracy: 0.3660 - val_loss: 1.7946 - val_accuracy: 0.3639\n",
      "Epoch 4/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 1.4867 - accuracy: 0.4760 - val_loss: 1.5090 - val_accuracy: 0.4528\n",
      "Epoch 5/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 1.2382 - accuracy: 0.5908 - val_loss: 1.3314 - val_accuracy: 0.5556\n",
      "Epoch 6/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 1.0129 - accuracy: 0.6903 - val_loss: 1.1331 - val_accuracy: 0.6250\n",
      "Epoch 7/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.7885 - accuracy: 0.7474 - val_loss: 0.9412 - val_accuracy: 0.7000\n",
      "Epoch 8/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.5770 - accuracy: 0.8225 - val_loss: 0.7946 - val_accuracy: 0.7722\n",
      "Epoch 9/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.4514 - accuracy: 0.8650 - val_loss: 0.6704 - val_accuracy: 0.8056\n",
      "Epoch 10/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.3778 - accuracy: 0.8824 - val_loss: 0.6218 - val_accuracy: 0.8250\n",
      "Epoch 11/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.3330 - accuracy: 0.8970 - val_loss: 0.5681 - val_accuracy: 0.8361\n",
      "Epoch 12/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.2937 - accuracy: 0.9123 - val_loss: 0.5437 - val_accuracy: 0.8361\n",
      "Epoch 13/5000\n",
      "1437/1437 [==============================] - 0s 39us/sample - loss: 0.2626 - accuracy: 0.9186 - val_loss: 0.5062 - val_accuracy: 0.8583\n",
      "Epoch 14/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.2394 - accuracy: 0.9283 - val_loss: 0.4921 - val_accuracy: 0.8750\n",
      "Epoch 15/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.2202 - accuracy: 0.9311 - val_loss: 0.4600 - val_accuracy: 0.8639\n",
      "Epoch 16/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.1981 - accuracy: 0.9422 - val_loss: 0.4550 - val_accuracy: 0.8667\n",
      "Epoch 17/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.1859 - accuracy: 0.9422 - val_loss: 0.4509 - val_accuracy: 0.8722\n",
      "Epoch 18/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.1713 - accuracy: 0.9499 - val_loss: 0.4257 - val_accuracy: 0.8806\n",
      "Epoch 19/5000\n",
      "1437/1437 [==============================] - 0s 39us/sample - loss: 0.1592 - accuracy: 0.9527 - val_loss: 0.4272 - val_accuracy: 0.8778\n",
      "Epoch 20/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.1469 - accuracy: 0.9548 - val_loss: 0.4202 - val_accuracy: 0.8861\n",
      "Epoch 21/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.1365 - accuracy: 0.9617 - val_loss: 0.4118 - val_accuracy: 0.8861\n",
      "Epoch 22/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.1295 - accuracy: 0.9645 - val_loss: 0.4015 - val_accuracy: 0.8944\n",
      "Epoch 23/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.1222 - accuracy: 0.9687 - val_loss: 0.3956 - val_accuracy: 0.8917\n",
      "Epoch 24/5000\n",
      "1437/1437 [==============================] - 0s 39us/sample - loss: 0.1137 - accuracy: 0.9687 - val_loss: 0.3981 - val_accuracy: 0.9000\n",
      "Epoch 25/5000\n",
      "1437/1437 [==============================] - 0s 39us/sample - loss: 0.1062 - accuracy: 0.9701 - val_loss: 0.3857 - val_accuracy: 0.8889\n",
      "Epoch 26/5000\n",
      "1437/1437 [==============================] - 0s 39us/sample - loss: 0.1023 - accuracy: 0.9729 - val_loss: 0.3859 - val_accuracy: 0.8944\n",
      "Epoch 27/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0968 - accuracy: 0.9722 - val_loss: 0.3962 - val_accuracy: 0.8944\n",
      "Epoch 28/5000\n",
      "1437/1437 [==============================] - 0s 39us/sample - loss: 0.0904 - accuracy: 0.9743 - val_loss: 0.3872 - val_accuracy: 0.8917\n",
      "Epoch 29/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0868 - accuracy: 0.9749 - val_loss: 0.3813 - val_accuracy: 0.8944\n",
      "Epoch 30/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.0818 - accuracy: 0.9777 - val_loss: 0.3759 - val_accuracy: 0.8944\n",
      "Epoch 31/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.0781 - accuracy: 0.9791 - val_loss: 0.3667 - val_accuracy: 0.8972\n",
      "Epoch 32/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0729 - accuracy: 0.9812 - val_loss: 0.3791 - val_accuracy: 0.8944\n",
      "Epoch 33/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0704 - accuracy: 0.9819 - val_loss: 0.3664 - val_accuracy: 0.9056\n",
      "Epoch 34/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0668 - accuracy: 0.9826 - val_loss: 0.3758 - val_accuracy: 0.8972\n",
      "Epoch 35/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.0643 - accuracy: 0.9833 - val_loss: 0.3758 - val_accuracy: 0.9028\n",
      "Epoch 36/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.0615 - accuracy: 0.9847 - val_loss: 0.3746 - val_accuracy: 0.9056\n",
      "Epoch 37/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0590 - accuracy: 0.9861 - val_loss: 0.3806 - val_accuracy: 0.8944\n",
      "Epoch 38/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0558 - accuracy: 0.9875 - val_loss: 0.3814 - val_accuracy: 0.8972\n",
      "Epoch 39/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.0525 - accuracy: 0.9868 - val_loss: 0.3721 - val_accuracy: 0.8889\n",
      "Epoch 40/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0514 - accuracy: 0.9875 - val_loss: 0.3802 - val_accuracy: 0.8972\n",
      "Epoch 41/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0483 - accuracy: 0.9889 - val_loss: 0.3795 - val_accuracy: 0.8972\n",
      "Epoch 42/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0465 - accuracy: 0.9896 - val_loss: 0.3687 - val_accuracy: 0.9056\n",
      "Epoch 43/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.0455 - accuracy: 0.9882 - val_loss: 0.3737 - val_accuracy: 0.9000\n",
      "Epoch 44/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0411 - accuracy: 0.9903 - val_loss: 0.3831 - val_accuracy: 0.8972\n",
      "Epoch 45/5000\n",
      "1437/1437 [==============================] - 0s 39us/sample - loss: 0.0428 - accuracy: 0.9923 - val_loss: 0.3667 - val_accuracy: 0.9000\n",
      "Epoch 46/5000\n",
      "1437/1437 [==============================] - 0s 39us/sample - loss: 0.0385 - accuracy: 0.9910 - val_loss: 0.3872 - val_accuracy: 0.9083\n",
      "Epoch 47/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0367 - accuracy: 0.9910 - val_loss: 0.3886 - val_accuracy: 0.9056\n",
      "Epoch 48/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0340 - accuracy: 0.9937 - val_loss: 0.3744 - val_accuracy: 0.9056\n",
      "Epoch 49/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.0330 - accuracy: 0.9937 - val_loss: 0.3691 - val_accuracy: 0.9111\n",
      "Epoch 50/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0305 - accuracy: 0.9937 - val_loss: 0.3810 - val_accuracy: 0.9111\n",
      "Epoch 51/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0298 - accuracy: 0.9944 - val_loss: 0.3804 - val_accuracy: 0.9139\n",
      "Epoch 52/5000\n",
      "1437/1437 [==============================] - 0s 39us/sample - loss: 0.0281 - accuracy: 0.9958 - val_loss: 0.3735 - val_accuracy: 0.9028\n",
      "Epoch 53/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.0279 - accuracy: 0.9958 - val_loss: 0.3882 - val_accuracy: 0.9028\n",
      "Epoch 54/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0255 - accuracy: 0.9972 - val_loss: 0.3752 - val_accuracy: 0.9139\n",
      "Epoch 55/5000\n"
<<<<<<< HEAD
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
=======
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
<<<<<<< HEAD
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.0054e-07 - accuracy: 1.0000 - val_loss: 1.3356 - val_accuracy: 0.9167\n",
      "Epoch 267/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 9.8884e-08 - accuracy: 1.0000 - val_loss: 1.3352 - val_accuracy: 0.9167\n",
      "Epoch 268/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 9.7474e-08 - accuracy: 1.0000 - val_loss: 1.3344 - val_accuracy: 0.9167\n",
      "Epoch 269/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 9.5483e-08 - accuracy: 1.0000 - val_loss: 1.3338 - val_accuracy: 0.9167\n",
      "Epoch 270/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 9.4073e-08 - accuracy: 1.0000 - val_loss: 1.3333 - val_accuracy: 0.9167\n",
      "Epoch 271/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 9.1916e-08 - accuracy: 1.0000 - val_loss: 1.3329 - val_accuracy: 0.9167\n",
      "Epoch 272/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 9.0423e-08 - accuracy: 1.0000 - val_loss: 1.3324 - val_accuracy: 0.9167\n",
      "Epoch 273/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 8.8846e-08 - accuracy: 1.0000 - val_loss: 1.3322 - val_accuracy: 0.9167\n",
      "Epoch 274/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 8.7436e-08 - accuracy: 1.0000 - val_loss: 1.3316 - val_accuracy: 0.9167\n",
      "Epoch 275/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 8.6275e-08 - accuracy: 1.0000 - val_loss: 1.3307 - val_accuracy: 0.9167\n",
      "Epoch 276/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 8.4533e-08 - accuracy: 1.0000 - val_loss: 1.3302 - val_accuracy: 0.9167\n",
      "Epoch 277/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 8.2791e-08 - accuracy: 1.0000 - val_loss: 1.3309 - val_accuracy: 0.9167\n",
      "Epoch 278/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 8.2044e-08 - accuracy: 1.0000 - val_loss: 1.3293 - val_accuracy: 0.9167\n",
      "Epoch 279/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 8.0551e-08 - accuracy: 1.0000 - val_loss: 1.3291 - val_accuracy: 0.9167\n",
      "Epoch 280/500\n",
      "1437/1437 [==============================] - 0s 31us/sample - loss: 7.8975e-08 - accuracy: 1.0000 - val_loss: 1.3292 - val_accuracy: 0.9167\n",
      "Epoch 281/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 7.8145e-08 - accuracy: 1.0000 - val_loss: 1.3289 - val_accuracy: 0.9167\n",
      "Epoch 282/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 7.6154e-08 - accuracy: 1.0000 - val_loss: 1.3286 - val_accuracy: 0.9167\n",
      "Epoch 283/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 7.4910e-08 - accuracy: 1.0000 - val_loss: 1.3280 - val_accuracy: 0.9167\n",
      "Epoch 284/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 7.3666e-08 - accuracy: 1.0000 - val_loss: 1.3277 - val_accuracy: 0.9167\n",
      "Epoch 285/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 7.2504e-08 - accuracy: 1.0000 - val_loss: 1.3270 - val_accuracy: 0.9167\n",
      "Epoch 286/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 7.1758e-08 - accuracy: 1.0000 - val_loss: 1.3267 - val_accuracy: 0.9167\n",
      "Epoch 287/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 7.0181e-08 - accuracy: 1.0000 - val_loss: 1.3264 - val_accuracy: 0.9167\n",
      "Epoch 288/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 6.9352e-08 - accuracy: 1.0000 - val_loss: 1.3263 - val_accuracy: 0.9167\n",
      "Epoch 289/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 6.8190e-08 - accuracy: 1.0000 - val_loss: 1.3254 - val_accuracy: 0.9167\n",
      "Epoch 290/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 6.6946e-08 - accuracy: 1.0000 - val_loss: 1.3257 - val_accuracy: 0.9167\n",
      "Epoch 291/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 6.6034e-08 - accuracy: 1.0000 - val_loss: 1.3252 - val_accuracy: 0.9167\n",
      "Epoch 292/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 6.4789e-08 - accuracy: 1.0000 - val_loss: 1.3250 - val_accuracy: 0.9167\n",
      "Epoch 293/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 6.3711e-08 - accuracy: 1.0000 - val_loss: 1.3248 - val_accuracy: 0.9167\n",
      "Epoch 294/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 6.2798e-08 - accuracy: 1.0000 - val_loss: 1.3237 - val_accuracy: 0.9167\n",
      "Epoch 295/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 6.1803e-08 - accuracy: 1.0000 - val_loss: 1.3233 - val_accuracy: 0.9167\n",
      "Epoch 296/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 6.0641e-08 - accuracy: 1.0000 - val_loss: 1.3233 - val_accuracy: 0.9167\n",
      "Epoch 297/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 6.0144e-08 - accuracy: 1.0000 - val_loss: 1.3234 - val_accuracy: 0.9167\n",
      "Epoch 298/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 5.8733e-08 - accuracy: 1.0000 - val_loss: 1.3227 - val_accuracy: 0.9167\n",
      "Epoch 299/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 5.8070e-08 - accuracy: 1.0000 - val_loss: 1.3219 - val_accuracy: 0.9167\n",
      "Epoch 300/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 5.6908e-08 - accuracy: 1.0000 - val_loss: 1.3218 - val_accuracy: 0.9167\n",
      "Epoch 301/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 5.5913e-08 - accuracy: 1.0000 - val_loss: 1.3209 - val_accuracy: 0.9167\n",
      "Epoch 302/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 5.5415e-08 - accuracy: 1.0000 - val_loss: 1.3212 - val_accuracy: 0.9167\n",
      "Epoch 303/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 5.4503e-08 - accuracy: 1.0000 - val_loss: 1.3206 - val_accuracy: 0.9167\n",
      "Epoch 304/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 5.3673e-08 - accuracy: 1.0000 - val_loss: 1.3203 - val_accuracy: 0.9167\n",
      "Epoch 305/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 5.2761e-08 - accuracy: 1.0000 - val_loss: 1.3201 - val_accuracy: 0.9167\n",
      "Epoch 306/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 5.2014e-08 - accuracy: 1.0000 - val_loss: 1.3197 - val_accuracy: 0.9167\n",
      "Epoch 307/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 5.1433e-08 - accuracy: 1.0000 - val_loss: 1.3196 - val_accuracy: 0.9167\n",
      "Epoch 308/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 5.0355e-08 - accuracy: 1.0000 - val_loss: 1.3196 - val_accuracy: 0.9167\n",
      "Epoch 309/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 4.9857e-08 - accuracy: 1.0000 - val_loss: 1.3193 - val_accuracy: 0.9167\n",
      "Epoch 310/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 4.9193e-08 - accuracy: 1.0000 - val_loss: 1.3200 - val_accuracy: 0.9167\n",
      "Epoch 311/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 4.8115e-08 - accuracy: 1.0000 - val_loss: 1.3187 - val_accuracy: 0.9167\n",
      "Epoch 312/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 4.7617e-08 - accuracy: 1.0000 - val_loss: 1.3181 - val_accuracy: 0.9167\n",
      "Epoch 313/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 4.6539e-08 - accuracy: 1.0000 - val_loss: 1.3187 - val_accuracy: 0.9167\n",
      "Epoch 314/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 4.5875e-08 - accuracy: 1.0000 - val_loss: 1.3173 - val_accuracy: 0.9167\n",
      "Epoch 315/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 4.5046e-08 - accuracy: 1.0000 - val_loss: 1.3170 - val_accuracy: 0.9167\n",
      "Epoch 316/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 4.4548e-08 - accuracy: 1.0000 - val_loss: 1.3171 - val_accuracy: 0.9167\n",
      "Epoch 317/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 4.3801e-08 - accuracy: 1.0000 - val_loss: 1.3165 - val_accuracy: 0.9167\n",
      "Epoch 318/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 4.2889e-08 - accuracy: 1.0000 - val_loss: 1.3160 - val_accuracy: 0.9167\n",
      "Epoch 319/500\n"
=======
=======
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
      "1437/1437 [==============================] - 0s 39us/sample - loss: 0.0241 - accuracy: 0.9965 - val_loss: 0.3955 - val_accuracy: 0.9083\n",
      "Epoch 56/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.0240 - accuracy: 0.9965 - val_loss: 0.3867 - val_accuracy: 0.9056\n",
      "Epoch 57/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.0227 - accuracy: 0.9972 - val_loss: 0.3830 - val_accuracy: 0.9111\n",
      "Epoch 58/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0212 - accuracy: 0.9979 - val_loss: 0.3806 - val_accuracy: 0.9111\n",
      "Epoch 59/5000\n",
      "1437/1437 [==============================] - 0s 39us/sample - loss: 0.0210 - accuracy: 0.9979 - val_loss: 0.4036 - val_accuracy: 0.8972\n",
      "Epoch 60/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0198 - accuracy: 0.9972 - val_loss: 0.3921 - val_accuracy: 0.9028\n",
      "Epoch 61/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0202 - accuracy: 0.9972 - val_loss: 0.3913 - val_accuracy: 0.9056\n",
      "Epoch 62/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0184 - accuracy: 0.9986 - val_loss: 0.3785 - val_accuracy: 0.9111\n",
      "Epoch 63/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.0184 - accuracy: 0.9986 - val_loss: 0.3988 - val_accuracy: 0.9056\n",
      "Epoch 64/5000\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 0.0180 - accuracy: 0.9979 - val_loss: 0.3973 - val_accuracy: 0.9028\n",
      "Epoch 65/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0168 - accuracy: 0.9993 - val_loss: 0.3961 - val_accuracy: 0.9083\n",
      "Epoch 66/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0157 - accuracy: 0.9979 - val_loss: 0.3867 - val_accuracy: 0.9056\n",
      "Epoch 67/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.9986 - val_loss: 0.3931 - val_accuracy: 0.9056\n",
      "Epoch 68/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.0145 - accuracy: 0.9993 - val_loss: 0.3918 - val_accuracy: 0.9111\n",
      "Epoch 69/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.0141 - accuracy: 0.9993 - val_loss: 0.3962 - val_accuracy: 0.9056\n",
      "Epoch 70/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0134 - accuracy: 0.9986 - val_loss: 0.4071 - val_accuracy: 0.9083\n",
      "Epoch 71/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0130 - accuracy: 0.9986 - val_loss: 0.3975 - val_accuracy: 0.9139\n",
      "Epoch 72/5000\n",
      "1437/1437 [==============================] - 0s 39us/sample - loss: 0.0124 - accuracy: 0.9993 - val_loss: 0.4072 - val_accuracy: 0.9111\n",
      "Epoch 73/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0122 - accuracy: 0.9993 - val_loss: 0.4086 - val_accuracy: 0.9056\n",
      "Epoch 74/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.0119 - accuracy: 0.9993 - val_loss: 0.3990 - val_accuracy: 0.9028\n",
      "Epoch 75/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0119 - accuracy: 0.9986 - val_loss: 0.4110 - val_accuracy: 0.9111\n",
      "Epoch 76/5000\n",
      "1437/1437 [==============================] - 0s 39us/sample - loss: 0.0113 - accuracy: 0.9993 - val_loss: 0.3963 - val_accuracy: 0.9083\n",
      "Epoch 77/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0106 - accuracy: 0.9993 - val_loss: 0.4087 - val_accuracy: 0.9111\n",
      "Epoch 78/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.4084 - val_accuracy: 0.9083\n",
      "Epoch 79/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.4040 - val_accuracy: 0.9083\n",
      "Epoch 80/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.4227 - val_accuracy: 0.9028\n",
      "Epoch 81/5000\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.4107 - val_accuracy: 0.9139\n",
      "Epoch 82/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.4114 - val_accuracy: 0.9083\n",
      "Epoch 83/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.4065 - val_accuracy: 0.9056\n",
      "Epoch 84/5000\n",
      "1437/1437 [==============================] - 0s 39us/sample - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.4149 - val_accuracy: 0.9139\n",
      "Epoch 85/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.4177 - val_accuracy: 0.9111\n",
      "Epoch 86/5000\n",
      "1437/1437 [==============================] - 0s 39us/sample - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.4201 - val_accuracy: 0.9111\n",
      "Epoch 87/5000\n",
      "1437/1437 [==============================] - 0s 39us/sample - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.4232 - val_accuracy: 0.9083\n",
      "Epoch 88/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.4250 - val_accuracy: 0.9083\n",
      "Epoch 89/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.4346 - val_accuracy: 0.9139\n",
      "Epoch 90/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.4287 - val_accuracy: 0.9056\n",
      "Epoch 91/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.4309 - val_accuracy: 0.9167\n",
      "Epoch 92/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.4315 - val_accuracy: 0.9083\n",
      "Epoch 93/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.4444 - val_accuracy: 0.9111\n",
      "Epoch 94/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.4339 - val_accuracy: 0.9139\n",
      "Epoch 95/5000\n",
      "1437/1437 [==============================] - 0s 39us/sample - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.4340 - val_accuracy: 0.9083\n",
      "Epoch 96/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.4377 - val_accuracy: 0.9167\n",
      "Epoch 97/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.4378 - val_accuracy: 0.9111\n",
      "Epoch 98/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.4517 - val_accuracy: 0.9111\n",
      "Epoch 99/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.4459 - val_accuracy: 0.9111\n",
      "Epoch 100/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.4424 - val_accuracy: 0.9139\n",
      "Epoch 101/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.4460 - val_accuracy: 0.9194\n",
      "Epoch 102/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.4577 - val_accuracy: 0.9111\n",
      "Epoch 103/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.4516 - val_accuracy: 0.9167\n",
      "Epoch 104/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.4522 - val_accuracy: 0.9139\n",
      "Epoch 105/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.4596 - val_accuracy: 0.9167\n",
      "Epoch 106/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.4583 - val_accuracy: 0.9111\n",
      "Epoch 107/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.4604 - val_accuracy: 0.9111\n",
      "Epoch 108/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.4631 - val_accuracy: 0.9167\n",
      "Epoch 109/5000\n"
<<<<<<< HEAD
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "1437/1437 [==============================] - 0s 35us/sample - loss: 4.2474e-08 - accuracy: 1.0000 - val_loss: 1.3164 - val_accuracy: 0.9167\n",
      "Epoch 320/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 4.1644e-08 - accuracy: 1.0000 - val_loss: 1.3152 - val_accuracy: 0.9167\n",
      "Epoch 321/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 4.1147e-08 - accuracy: 1.0000 - val_loss: 1.3147 - val_accuracy: 0.9194\n",
      "Epoch 322/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 4.0566e-08 - accuracy: 1.0000 - val_loss: 1.3150 - val_accuracy: 0.9194\n",
      "Epoch 323/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 3.9985e-08 - accuracy: 1.0000 - val_loss: 1.3146 - val_accuracy: 0.9194\n",
      "Epoch 324/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 3.9073e-08 - accuracy: 1.0000 - val_loss: 1.3140 - val_accuracy: 0.9194\n",
      "Epoch 325/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 3.8741e-08 - accuracy: 1.0000 - val_loss: 1.3145 - val_accuracy: 0.9194\n",
      "Epoch 326/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 3.8160e-08 - accuracy: 1.0000 - val_loss: 1.3133 - val_accuracy: 0.9194\n",
      "Epoch 327/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 3.7662e-08 - accuracy: 1.0000 - val_loss: 1.3125 - val_accuracy: 0.9194\n",
      "Epoch 328/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 3.6750e-08 - accuracy: 1.0000 - val_loss: 1.3129 - val_accuracy: 0.9194\n",
      "Epoch 329/500\n",
      "1437/1437 [==============================] - 0s 32us/sample - loss: 3.6003e-08 - accuracy: 1.0000 - val_loss: 1.3122 - val_accuracy: 0.9194\n",
      "Epoch 330/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 3.5340e-08 - accuracy: 1.0000 - val_loss: 1.3121 - val_accuracy: 0.9194\n",
      "Epoch 331/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 3.4842e-08 - accuracy: 1.0000 - val_loss: 1.3122 - val_accuracy: 0.9194\n",
      "Epoch 332/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 3.4261e-08 - accuracy: 1.0000 - val_loss: 1.3123 - val_accuracy: 0.9194\n",
      "Epoch 333/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 3.4178e-08 - accuracy: 1.0000 - val_loss: 1.3124 - val_accuracy: 0.9194\n",
      "Epoch 334/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 3.3432e-08 - accuracy: 1.0000 - val_loss: 1.3124 - val_accuracy: 0.9194\n",
      "Epoch 335/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 3.2934e-08 - accuracy: 1.0000 - val_loss: 1.3124 - val_accuracy: 0.9194\n",
      "Epoch 336/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 3.2187e-08 - accuracy: 1.0000 - val_loss: 1.3119 - val_accuracy: 0.9194\n",
      "Epoch 337/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 3.1690e-08 - accuracy: 1.0000 - val_loss: 1.3119 - val_accuracy: 0.9194\n",
      "Epoch 338/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 3.1275e-08 - accuracy: 1.0000 - val_loss: 1.3120 - val_accuracy: 0.9194\n",
      "Epoch 339/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 3.0943e-08 - accuracy: 1.0000 - val_loss: 1.3116 - val_accuracy: 0.9194\n",
      "Epoch 340/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 3.0611e-08 - accuracy: 1.0000 - val_loss: 1.3111 - val_accuracy: 0.9194\n",
      "Epoch 341/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 3.0113e-08 - accuracy: 1.0000 - val_loss: 1.3110 - val_accuracy: 0.9194\n",
      "Epoch 342/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 2.9450e-08 - accuracy: 1.0000 - val_loss: 1.3108 - val_accuracy: 0.9194\n",
      "Epoch 343/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 2.9284e-08 - accuracy: 1.0000 - val_loss: 1.3107 - val_accuracy: 0.9194\n",
      "Epoch 344/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 2.8620e-08 - accuracy: 1.0000 - val_loss: 1.3105 - val_accuracy: 0.9194\n",
      "Epoch 345/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 2.8039e-08 - accuracy: 1.0000 - val_loss: 1.3104 - val_accuracy: 0.9194\n",
      "Epoch 346/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 2.7625e-08 - accuracy: 1.0000 - val_loss: 1.3107 - val_accuracy: 0.9194\n",
      "Epoch 347/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 2.7293e-08 - accuracy: 1.0000 - val_loss: 1.3106 - val_accuracy: 0.9194\n",
      "Epoch 348/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 2.7127e-08 - accuracy: 1.0000 - val_loss: 1.3100 - val_accuracy: 0.9194\n",
      "Epoch 349/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 2.6712e-08 - accuracy: 1.0000 - val_loss: 1.3100 - val_accuracy: 0.9194\n",
      "Epoch 350/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 2.6214e-08 - accuracy: 1.0000 - val_loss: 1.3094 - val_accuracy: 0.9194\n",
      "Epoch 351/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 2.5468e-08 - accuracy: 1.0000 - val_loss: 1.3090 - val_accuracy: 0.9194\n",
      "Epoch 352/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 2.5302e-08 - accuracy: 1.0000 - val_loss: 1.3089 - val_accuracy: 0.9194\n",
      "Epoch 353/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 2.5053e-08 - accuracy: 1.0000 - val_loss: 1.3091 - val_accuracy: 0.9194\n",
      "Epoch 354/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 2.4638e-08 - accuracy: 1.0000 - val_loss: 1.3083 - val_accuracy: 0.9194\n",
      "Epoch 355/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 2.4306e-08 - accuracy: 1.0000 - val_loss: 1.3083 - val_accuracy: 0.9194\n",
      "Epoch 356/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 2.3726e-08 - accuracy: 1.0000 - val_loss: 1.3081 - val_accuracy: 0.9194\n",
      "Epoch 357/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 2.3145e-08 - accuracy: 1.0000 - val_loss: 1.3077 - val_accuracy: 0.9194\n",
      "Epoch 358/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 2.2813e-08 - accuracy: 1.0000 - val_loss: 1.3073 - val_accuracy: 0.9194\n",
      "Epoch 359/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 2.2481e-08 - accuracy: 1.0000 - val_loss: 1.3073 - val_accuracy: 0.9194\n",
      "Epoch 360/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 2.2647e-08 - accuracy: 1.0000 - val_loss: 1.3067 - val_accuracy: 0.9194\n",
      "Epoch 361/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 2.1901e-08 - accuracy: 1.0000 - val_loss: 1.3070 - val_accuracy: 0.9194\n",
      "Epoch 362/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 2.1735e-08 - accuracy: 1.0000 - val_loss: 1.3068 - val_accuracy: 0.9194\n",
      "Epoch 363/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 2.1486e-08 - accuracy: 1.0000 - val_loss: 1.3067 - val_accuracy: 0.9194\n",
      "Epoch 364/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 2.1403e-08 - accuracy: 1.0000 - val_loss: 1.3061 - val_accuracy: 0.9194\n",
      "Epoch 365/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 2.0905e-08 - accuracy: 1.0000 - val_loss: 1.3057 - val_accuracy: 0.9194\n",
      "Epoch 366/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 2.0573e-08 - accuracy: 1.0000 - val_loss: 1.3055 - val_accuracy: 0.9194\n",
      "Epoch 367/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 2.0573e-08 - accuracy: 1.0000 - val_loss: 1.3059 - val_accuracy: 0.9194\n",
      "Epoch 368/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 2.0407e-08 - accuracy: 1.0000 - val_loss: 1.3047 - val_accuracy: 0.9194\n",
      "Epoch 369/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 2.0076e-08 - accuracy: 1.0000 - val_loss: 1.3053 - val_accuracy: 0.9194\n",
      "Epoch 370/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 1.9910e-08 - accuracy: 1.0000 - val_loss: 1.3048 - val_accuracy: 0.9194\n",
      "Epoch 371/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 1.9744e-08 - accuracy: 1.0000 - val_loss: 1.3044 - val_accuracy: 0.9194\n",
      "Epoch 372/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.9080e-08 - accuracy: 1.0000 - val_loss: 1.3038 - val_accuracy: 0.9194\n",
      "Epoch 373/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.8748e-08 - accuracy: 1.0000 - val_loss: 1.3044 - val_accuracy: 0.9194\n",
      "Epoch 374/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.8582e-08 - accuracy: 1.0000 - val_loss: 1.3037 - val_accuracy: 0.9194\n",
      "Epoch 375/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.8251e-08 - accuracy: 1.0000 - val_loss: 1.3031 - val_accuracy: 0.9194\n",
      "Epoch 376/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 1.7919e-08 - accuracy: 1.0000 - val_loss: 1.3034 - val_accuracy: 0.9194\n",
      "Epoch 377/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.7504e-08 - accuracy: 1.0000 - val_loss: 1.3020 - val_accuracy: 0.9194\n",
      "Epoch 378/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 1.7255e-08 - accuracy: 1.0000 - val_loss: 1.3020 - val_accuracy: 0.9194\n",
      "Epoch 379/500\n",
      "1437/1437 [==============================] - 0s 32us/sample - loss: 1.7006e-08 - accuracy: 1.0000 - val_loss: 1.3016 - val_accuracy: 0.9194\n",
      "Epoch 380/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 1.6591e-08 - accuracy: 1.0000 - val_loss: 1.3010 - val_accuracy: 0.9194\n",
      "Epoch 381/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.6674e-08 - accuracy: 1.0000 - val_loss: 1.3011 - val_accuracy: 0.9194\n",
      "Epoch 382/500\n",
      "1437/1437 [==============================] - 0s 32us/sample - loss: 1.6343e-08 - accuracy: 1.0000 - val_loss: 1.3009 - val_accuracy: 0.9194\n",
      "Epoch 383/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 1.5845e-08 - accuracy: 1.0000 - val_loss: 1.3014 - val_accuracy: 0.9194\n",
      "Epoch 384/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.6011e-08 - accuracy: 1.0000 - val_loss: 1.3003 - val_accuracy: 0.9194\n",
      "Epoch 385/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.5679e-08 - accuracy: 1.0000 - val_loss: 1.3012 - val_accuracy: 0.9194\n",
      "Epoch 386/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 1.5430e-08 - accuracy: 1.0000 - val_loss: 1.3001 - val_accuracy: 0.9194\n",
      "Epoch 387/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 1.5264e-08 - accuracy: 1.0000 - val_loss: 1.3001 - val_accuracy: 0.9194\n",
      "Epoch 388/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 1.4766e-08 - accuracy: 1.0000 - val_loss: 1.3006 - val_accuracy: 0.9194\n",
      "Epoch 389/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.5181e-08 - accuracy: 1.0000 - val_loss: 1.2999 - val_accuracy: 0.9194\n",
      "Epoch 390/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 1.4849e-08 - accuracy: 1.0000 - val_loss: 1.3009 - val_accuracy: 0.9194\n",
      "Epoch 391/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.4517e-08 - accuracy: 1.0000 - val_loss: 1.2991 - val_accuracy: 0.9194\n",
      "Epoch 392/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 1.4352e-08 - accuracy: 1.0000 - val_loss: 1.2993 - val_accuracy: 0.9194\n",
      "Epoch 393/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 1.4352e-08 - accuracy: 1.0000 - val_loss: 1.2990 - val_accuracy: 0.9194\n",
      "Epoch 394/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 1.3937e-08 - accuracy: 1.0000 - val_loss: 1.2990 - val_accuracy: 0.9194\n",
      "Epoch 395/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 1.3771e-08 - accuracy: 1.0000 - val_loss: 1.2986 - val_accuracy: 0.9194\n",
      "Epoch 396/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 1.3273e-08 - accuracy: 1.0000 - val_loss: 1.2998 - val_accuracy: 0.9194\n",
      "Epoch 397/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 1.2941e-08 - accuracy: 1.0000 - val_loss: 1.2984 - val_accuracy: 0.9194\n",
      "Epoch 398/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.2858e-08 - accuracy: 1.0000 - val_loss: 1.2980 - val_accuracy: 0.9194\n",
      "Epoch 399/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.2692e-08 - accuracy: 1.0000 - val_loss: 1.2985 - val_accuracy: 0.9194\n",
      "Epoch 400/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 1.2527e-08 - accuracy: 1.0000 - val_loss: 1.2992 - val_accuracy: 0.9194\n",
      "Epoch 401/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.2361e-08 - accuracy: 1.0000 - val_loss: 1.2992 - val_accuracy: 0.9194\n",
      "Epoch 402/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.2361e-08 - accuracy: 1.0000 - val_loss: 1.2978 - val_accuracy: 0.9194\n",
      "Epoch 403/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.2029e-08 - accuracy: 1.0000 - val_loss: 1.2984 - val_accuracy: 0.9194\n",
      "Epoch 404/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 1.2361e-08 - accuracy: 1.0000 - val_loss: 1.2968 - val_accuracy: 0.9194\n",
      "Epoch 405/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.1614e-08 - accuracy: 1.0000 - val_loss: 1.2963 - val_accuracy: 0.9194\n",
      "Epoch 406/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.1697e-08 - accuracy: 1.0000 - val_loss: 1.2976 - val_accuracy: 0.9194\n",
      "Epoch 407/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.1697e-08 - accuracy: 1.0000 - val_loss: 1.2969 - val_accuracy: 0.9194\n",
      "Epoch 408/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.1448e-08 - accuracy: 1.0000 - val_loss: 1.2970 - val_accuracy: 0.9194\n",
      "Epoch 409/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.1199e-08 - accuracy: 1.0000 - val_loss: 1.2956 - val_accuracy: 0.9194\n",
      "Epoch 410/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.1116e-08 - accuracy: 1.0000 - val_loss: 1.2959 - val_accuracy: 0.9194\n",
      "Epoch 411/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.0950e-08 - accuracy: 1.0000 - val_loss: 1.2960 - val_accuracy: 0.9194\n",
      "Epoch 412/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.0784e-08 - accuracy: 1.0000 - val_loss: 1.2959 - val_accuracy: 0.9194\n",
      "Epoch 413/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.0619e-08 - accuracy: 1.0000 - val_loss: 1.2964 - val_accuracy: 0.9194\n",
      "Epoch 414/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.0867e-08 - accuracy: 1.0000 - val_loss: 1.2950 - val_accuracy: 0.9194\n",
      "Epoch 415/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.0121e-08 - accuracy: 1.0000 - val_loss: 1.2958 - val_accuracy: 0.9194\n",
      "Epoch 416/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 1.0453e-08 - accuracy: 1.0000 - val_loss: 1.2944 - val_accuracy: 0.9194\n",
      "Epoch 417/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 1.0038e-08 - accuracy: 1.0000 - val_loss: 1.2947 - val_accuracy: 0.9194\n",
      "Epoch 418/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 9.6230e-09 - accuracy: 1.0000 - val_loss: 1.2952 - val_accuracy: 0.9194\n",
      "Epoch 419/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 9.4571e-09 - accuracy: 1.0000 - val_loss: 1.2941 - val_accuracy: 0.9194\n",
      "Epoch 420/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 9.2912e-09 - accuracy: 1.0000 - val_loss: 1.2945 - val_accuracy: 0.9194\n",
      "Epoch 421/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 9.0423e-09 - accuracy: 1.0000 - val_loss: 1.2926 - val_accuracy: 0.9194\n",
      "Epoch 422/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 9.0423e-09 - accuracy: 1.0000 - val_loss: 1.2915 - val_accuracy: 0.9194\n",
      "Epoch 423/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 8.7934e-09 - accuracy: 1.0000 - val_loss: 1.2920 - val_accuracy: 0.9194\n",
      "Epoch 424/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 8.8764e-09 - accuracy: 1.0000 - val_loss: 1.2910 - val_accuracy: 0.9194\n",
      "Epoch 425/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1437/1437 [==============================] - 0s 36us/sample - loss: 8.5446e-09 - accuracy: 1.0000 - val_loss: 1.2911 - val_accuracy: 0.9194\n",
      "Epoch 426/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 8.3787e-09 - accuracy: 1.0000 - val_loss: 1.2919 - val_accuracy: 0.9194\n",
      "Epoch 427/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 8.5446e-09 - accuracy: 1.0000 - val_loss: 1.2906 - val_accuracy: 0.9194\n",
      "Epoch 428/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 8.6275e-09 - accuracy: 1.0000 - val_loss: 1.2909 - val_accuracy: 0.9194\n",
      "Epoch 429/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 8.4616e-09 - accuracy: 1.0000 - val_loss: 1.2900 - val_accuracy: 0.9194\n",
      "Epoch 430/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 8.0468e-09 - accuracy: 1.0000 - val_loss: 1.2899 - val_accuracy: 0.9194\n",
      "Epoch 431/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 8.0468e-09 - accuracy: 1.0000 - val_loss: 1.2901 - val_accuracy: 0.9194\n",
      "Epoch 432/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 7.7980e-09 - accuracy: 1.0000 - val_loss: 1.2899 - val_accuracy: 0.9167\n",
      "Epoch 433/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 7.7980e-09 - accuracy: 1.0000 - val_loss: 1.2889 - val_accuracy: 0.9167\n",
      "Epoch 434/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 8.0468e-09 - accuracy: 1.0000 - val_loss: 1.2890 - val_accuracy: 0.9167\n",
      "Epoch 435/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 7.6320e-09 - accuracy: 1.0000 - val_loss: 1.2879 - val_accuracy: 0.9194\n",
      "Epoch 436/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 7.4661e-09 - accuracy: 1.0000 - val_loss: 1.2885 - val_accuracy: 0.9167\n",
      "Epoch 437/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 7.3002e-09 - accuracy: 1.0000 - val_loss: 1.2869 - val_accuracy: 0.9167\n",
      "Epoch 438/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 7.2173e-09 - accuracy: 1.0000 - val_loss: 1.2872 - val_accuracy: 0.9167\n",
      "Epoch 439/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 6.9684e-09 - accuracy: 1.0000 - val_loss: 1.2880 - val_accuracy: 0.9167\n",
      "Epoch 440/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 7.3832e-09 - accuracy: 1.0000 - val_loss: 1.2858 - val_accuracy: 0.9167\n",
      "Epoch 441/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 6.8025e-09 - accuracy: 1.0000 - val_loss: 1.2866 - val_accuracy: 0.9167\n",
      "Epoch 442/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 6.5536e-09 - accuracy: 1.0000 - val_loss: 1.2875 - val_accuracy: 0.9167\n",
      "Epoch 443/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 6.5536e-09 - accuracy: 1.0000 - val_loss: 1.2875 - val_accuracy: 0.9167\n",
      "Epoch 444/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 6.7195e-09 - accuracy: 1.0000 - val_loss: 1.2853 - val_accuracy: 0.9167\n",
      "Epoch 445/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 6.5536e-09 - accuracy: 1.0000 - val_loss: 1.2864 - val_accuracy: 0.9167\n",
      "Epoch 446/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 6.4706e-09 - accuracy: 1.0000 - val_loss: 1.2862 - val_accuracy: 0.9167\n",
      "Epoch 447/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 6.4706e-09 - accuracy: 1.0000 - val_loss: 1.2842 - val_accuracy: 0.9167\n",
      "Epoch 448/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 6.3047e-09 - accuracy: 1.0000 - val_loss: 1.2855 - val_accuracy: 0.9167\n",
      "Epoch 449/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 6.1388e-09 - accuracy: 1.0000 - val_loss: 1.2851 - val_accuracy: 0.9167\n",
      "Epoch 450/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 6.2218e-09 - accuracy: 1.0000 - val_loss: 1.2862 - val_accuracy: 0.9167\n",
      "Epoch 451/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 6.1388e-09 - accuracy: 1.0000 - val_loss: 1.2841 - val_accuracy: 0.9167\n",
      "Epoch 452/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 6.2218e-09 - accuracy: 1.0000 - val_loss: 1.2851 - val_accuracy: 0.9167\n",
      "Epoch 453/500\n",
      "1437/1437 [==============================] - 0s 33us/sample - loss: 5.8900e-09 - accuracy: 1.0000 - val_loss: 1.2852 - val_accuracy: 0.9167\n",
      "Epoch 454/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 6.0559e-09 - accuracy: 1.0000 - val_loss: 1.2854 - val_accuracy: 0.9167\n",
      "Epoch 455/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 5.7240e-09 - accuracy: 1.0000 - val_loss: 1.2843 - val_accuracy: 0.9167\n",
      "Epoch 456/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 5.7240e-09 - accuracy: 1.0000 - val_loss: 1.2846 - val_accuracy: 0.9167\n",
      "Epoch 457/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 5.8070e-09 - accuracy: 1.0000 - val_loss: 1.2847 - val_accuracy: 0.9167\n",
      "Epoch 458/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 5.8900e-09 - accuracy: 1.0000 - val_loss: 1.2853 - val_accuracy: 0.9167\n",
      "Epoch 459/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 5.6411e-09 - accuracy: 1.0000 - val_loss: 1.2831 - val_accuracy: 0.9167\n",
      "Epoch 460/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 6.0559e-09 - accuracy: 1.0000 - val_loss: 1.2836 - val_accuracy: 0.9167\n",
      "Epoch 461/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 5.8070e-09 - accuracy: 1.0000 - val_loss: 1.2841 - val_accuracy: 0.9167\n",
      "Epoch 462/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 5.6411e-09 - accuracy: 1.0000 - val_loss: 1.2840 - val_accuracy: 0.9167\n",
      "Epoch 463/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 5.3093e-09 - accuracy: 1.0000 - val_loss: 1.2843 - val_accuracy: 0.9167\n",
      "Epoch 464/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 5.7240e-09 - accuracy: 1.0000 - val_loss: 1.2830 - val_accuracy: 0.9167\n",
      "Epoch 465/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 5.4752e-09 - accuracy: 1.0000 - val_loss: 1.2832 - val_accuracy: 0.9167\n",
      "Epoch 466/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 5.3093e-09 - accuracy: 1.0000 - val_loss: 1.2835 - val_accuracy: 0.9167\n",
      "Epoch 467/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 5.6411e-09 - accuracy: 1.0000 - val_loss: 1.2836 - val_accuracy: 0.9167\n",
      "Epoch 468/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 5.3922e-09 - accuracy: 1.0000 - val_loss: 1.2810 - val_accuracy: 0.9167\n",
      "Epoch 469/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 5.2263e-09 - accuracy: 1.0000 - val_loss: 1.2815 - val_accuracy: 0.9167\n",
      "Epoch 470/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 5.4752e-09 - accuracy: 1.0000 - val_loss: 1.2817 - val_accuracy: 0.9167\n",
      "Epoch 471/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 5.1433e-09 - accuracy: 1.0000 - val_loss: 1.2819 - val_accuracy: 0.9167\n",
      "Epoch 472/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 5.2263e-09 - accuracy: 1.0000 - val_loss: 1.2802 - val_accuracy: 0.9167\n",
      "Epoch 473/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 5.0604e-09 - accuracy: 1.0000 - val_loss: 1.2803 - val_accuracy: 0.9167\n",
      "Epoch 474/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 4.8945e-09 - accuracy: 1.0000 - val_loss: 1.2808 - val_accuracy: 0.9167\n",
      "Epoch 475/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 4.9774e-09 - accuracy: 1.0000 - val_loss: 1.2798 - val_accuracy: 0.9167\n",
      "Epoch 476/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 4.8945e-09 - accuracy: 1.0000 - val_loss: 1.2812 - val_accuracy: 0.9167\n",
      "Epoch 477/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 4.8115e-09 - accuracy: 1.0000 - val_loss: 1.2823 - val_accuracy: 0.9167\n",
      "Epoch 478/500\n"
=======
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "1437/1437 [==============================] - 0s 35us/sample - loss: 4.7286e-09 - accuracy: 1.0000 - val_loss: 1.2816 - val_accuracy: 0.9167\n",
      "Epoch 479/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 4.5626e-09 - accuracy: 1.0000 - val_loss: 1.2801 - val_accuracy: 0.9167\n",
      "Epoch 480/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 4.6456e-09 - accuracy: 1.0000 - val_loss: 1.2805 - val_accuracy: 0.9167\n",
      "Epoch 481/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 4.5626e-09 - accuracy: 1.0000 - val_loss: 1.2800 - val_accuracy: 0.9167\n",
      "Epoch 482/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 4.3138e-09 - accuracy: 1.0000 - val_loss: 1.2795 - val_accuracy: 0.9167\n",
      "Epoch 483/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 4.3967e-09 - accuracy: 1.0000 - val_loss: 1.2792 - val_accuracy: 0.9167\n",
      "Epoch 484/500\n",
      "1437/1437 [==============================] - 0s 33us/sample - loss: 4.5626e-09 - accuracy: 1.0000 - val_loss: 1.2807 - val_accuracy: 0.9167\n",
      "Epoch 485/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 4.2308e-09 - accuracy: 1.0000 - val_loss: 1.2806 - val_accuracy: 0.9167\n",
      "Epoch 486/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 4.2308e-09 - accuracy: 1.0000 - val_loss: 1.2814 - val_accuracy: 0.9167\n",
      "Epoch 487/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 4.1479e-09 - accuracy: 1.0000 - val_loss: 1.2793 - val_accuracy: 0.9167\n",
      "Epoch 488/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 4.1479e-09 - accuracy: 1.0000 - val_loss: 1.2792 - val_accuracy: 0.9167\n",
      "Epoch 489/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 4.2308e-09 - accuracy: 1.0000 - val_loss: 1.2793 - val_accuracy: 0.9167\n",
      "Epoch 490/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 4.3967e-09 - accuracy: 1.0000 - val_loss: 1.2810 - val_accuracy: 0.9167\n",
      "Epoch 491/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 4.1479e-09 - accuracy: 1.0000 - val_loss: 1.2807 - val_accuracy: 0.9167\n",
      "Epoch 492/500\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 4.0649e-09 - accuracy: 1.0000 - val_loss: 1.2775 - val_accuracy: 0.9167\n",
      "Epoch 493/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 3.9819e-09 - accuracy: 1.0000 - val_loss: 1.2783 - val_accuracy: 0.9167\n",
      "Epoch 494/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 3.9819e-09 - accuracy: 1.0000 - val_loss: 1.2782 - val_accuracy: 0.9167\n",
      "Epoch 495/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 3.7331e-09 - accuracy: 1.0000 - val_loss: 1.2781 - val_accuracy: 0.9167\n",
      "Epoch 496/500\n",
      "1437/1437 [==============================] - 0s 36us/sample - loss: 3.8990e-09 - accuracy: 1.0000 - val_loss: 1.2798 - val_accuracy: 0.9167\n",
      "Epoch 497/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 3.8990e-09 - accuracy: 1.0000 - val_loss: 1.2794 - val_accuracy: 0.9167\n",
      "Epoch 498/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 3.8990e-09 - accuracy: 1.0000 - val_loss: 1.2787 - val_accuracy: 0.9167\n",
      "Epoch 499/500\n",
      "1437/1437 [==============================] - 0s 34us/sample - loss: 4.0649e-09 - accuracy: 1.0000 - val_loss: 1.2798 - val_accuracy: 0.9167\n",
      "Epoch 500/500\n",
      "1437/1437 [==============================] - 0s 35us/sample - loss: 3.8160e-09 - accuracy: 1.0000 - val_loss: 1.2764 - val_accuracy: 0.9167\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, Y,validation_split=0.2,epochs=500, batch_size=30)"
=======
=======
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.4614 - val_accuracy: 0.9167\n",
      "Epoch 110/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.4644 - val_accuracy: 0.9139\n",
      "Epoch 111/5000\n",
      "1437/1437 [==============================] - 0s 39us/sample - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.4746 - val_accuracy: 0.9139\n",
      "Epoch 112/5000\n",
      "1437/1437 [==============================] - 0s 39us/sample - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.4620 - val_accuracy: 0.9139\n",
      "Epoch 113/5000\n",
      "1437/1437 [==============================] - 0s 39us/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.4715 - val_accuracy: 0.9111\n",
      "Epoch 114/5000\n",
      "1437/1437 [==============================] - 0s 39us/sample - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.4733 - val_accuracy: 0.9167\n",
      "Epoch 115/5000\n",
      "1437/1437 [==============================] - 0s 39us/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.4715 - val_accuracy: 0.9139\n",
      "Epoch 116/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.4750 - val_accuracy: 0.9167\n",
      "Epoch 117/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.4773 - val_accuracy: 0.9139\n",
      "Epoch 118/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.4837 - val_accuracy: 0.9167\n",
      "Epoch 119/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.4844 - val_accuracy: 0.9111\n",
      "Epoch 120/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.4861 - val_accuracy: 0.9194\n",
      "Epoch 121/5000\n",
      "1437/1437 [==============================] - 0s 39us/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.4856 - val_accuracy: 0.9139\n",
      "Epoch 122/5000\n",
      "1437/1437 [==============================] - 0s 39us/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.4935 - val_accuracy: 0.9139\n",
      "Epoch 123/5000\n",
      "1437/1437 [==============================] - 0s 39us/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.4856 - val_accuracy: 0.9222\n",
      "Epoch 124/5000\n",
      "1437/1437 [==============================] - 0s 39us/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.4868 - val_accuracy: 0.9194\n",
      "Epoch 125/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.4994 - val_accuracy: 0.9139\n",
      "Epoch 126/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.4965 - val_accuracy: 0.9139\n",
      "Epoch 127/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.4940 - val_accuracy: 0.9167\n",
      "Epoch 128/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.4950 - val_accuracy: 0.9194\n",
      "Epoch 129/5000\n",
      "1437/1437 [==============================] - 0s 38us/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.5034 - val_accuracy: 0.9139\n",
      "Epoch 130/5000\n",
      "1437/1437 [==============================] - 0s 37us/sample - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4993 - val_accuracy: 0.9139\n",
      "Epoch 131/5000\n",
      "1437/1437 [==============================] - 0s 39us/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.5012 - val_accuracy: 0.9167\n",
      "Epoch 132/5000\n",
      "1437/1437 [==============================] - 0s 39us/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.5111 - val_accuracy: 0.9111\n",
      "Epoch 133/5000\n",
      "1437/1437 [==============================] - 0s 39us/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.5038 - val_accuracy: 0.9167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17759629608>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y,validation_split=0.2,epochs=5000, batch_size=30, callbacks=[early_stopping_callback])"
<<<<<<< HEAD
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
=======
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 28,
=======
   "execution_count": 11,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
=======
   "execution_count": 11,
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
<<<<<<< HEAD
      "1797/1797 - 0s - loss: 0.2557 - accuracy: 0.9833\n",
=======
      "1797/1797 - 0s - loss: 0.1021 - accuracy: 0.9833\n",
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
=======
      "1797/1797 - 0s - loss: 0.1021 - accuracy: 0.9833\n",
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
      "\n",
      " Accuracy: 0.9833\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Accuracy: %.4f\" % (model.evaluate(X, Y, verbose=2)[1]))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vloss=history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_acc=history.history['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_len = np.arange(len(y_acc))\n",
    "plt.figure(figsize=(13,6))\n",
    "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=2)\n",
    "plt.plot(x_len, y_acc, \"o\", c=\"blue\", markersize=2)\n",
    "plt.ylim(0.0, 1.1)\n",
    "plt.show()"
   ]
=======
=======
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
<<<<<<< HEAD
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
=======
>>>>>>> 21a835b30b4a384aa4d150a259378af058f4189b
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
